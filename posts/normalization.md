## 归一化（Normalization）的定义

**归一化（Normalization）**是将激活值转换到标准化分布的操作，核心目的是控制神经网络各层激活值的尺度，使梯度能够稳定传播。

统计学中的标准做法是 **Z-score 标准化**：

$$z = \frac{x - \mu}{\sigma}$$

其中 $x$ 是原始值，$\mu$ 是均值，$\sigma$ 是标准差。归一化后的值表示原始值偏离均值多少个标准差。

深度学习中的归一化做的是相同的事——对象从统计数据变成了神经网络中的激活值。

---

## 为什么神经网络需要归一化

神经网络每层的输出称为**激活值（Activation）**。没有归一化时，随着网络加深，激活值的分布会逐层偏移和放大：

```
第 1 层激活值：[-1, 0.5, 1.2, -0.3]     均值接近 0，范围小
第 5 层激活值：[-8, 0.1, 12, -6]         均值偏移，范围变大
第10层激活值：[-500, 2, 900, -300]       失控
```

这导致两个问题：

**梯度消失（Vanishing Gradients）**

以 Sigmoid 激活函数为例，当输入 $x$ 的绝对值很大时，函数曲线趋于水平，导数接近 0。梯度是神经网络学习的信号，导数接近 0 意味着学习停滞。

Sigmoid 导数的最大值是 0.25（在 $z=0$ 时）。对一个 10 层网络，梯度从最后一层传到第一层需乘以 10 个这样的系数：$0.25^{10} \approx 0.000001$，梯度缩小了 100 万倍。

**梯度爆炸（Exploding Gradients）**

激活值尺度不断放大时，梯度也随之增长到极大数值，参数更新步幅过大，损失值振荡，训练无法收敛。

**归一化的作用**：将每层激活值拉回合理范围，使梯度在各层间稳定传播。

```
归一化前：  [-500, 2, 900, -300]     → 范围混乱
归一化后：  [-1.3, 0.01, 1.8, -0.8]  → 范围稳定
```

---

## Batch Normalization

2015 年，Ioffe & Szegedy 提出 **Batch Normalization（批量归一化，BatchNorm）**：在每个批量内，对每个特征维度单独做归一化。

### 计算步骤

设一个批量内某个特征的值为 $\{x_1, x_2, \ldots, x_N\}$（$N$ 为批量大小）：

**批量均值**：

$$\mu_B = \frac{1}{N} \sum_{i=1}^{N} x_i$$

**批量方差**：

$$\sigma_B^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu_B)^2$$

**归一化**（$\epsilon$ 防止除以零，通常取 $10^{-5}$）：

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

**缩放和平移**：

$$y_i = \gamma \cdot \hat{x}_i + \beta$$

$\gamma$（缩放）和 $\beta$（偏移）是可学习参数。归一化强制将分布拉到均值 0、方差 1，但该分布未必是网络最优输出。$\gamma$ 和 $\beta$ 允许网络通过训练将分布调整到任意位置和尺度。初始化通常为 $\gamma = 1,\, \beta = 0$。

完整公式：

$$y = \gamma \cdot \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} + \beta$$

### 归一化维度

```
          特征1   特征2   特征3   特征4
样本1   [  8.2    0.1   -3.5    7.0  ]
样本2   [ -2.1    0.3    1.2    5.5  ]
样本3   [  5.5    0.0   -1.0    9.1  ]
样本4   [  1.4    0.2    2.3    6.2  ]
          ↑
    对这一列（同一特征维度的所有样本）
    计算均值和方差，然后归一化
```

BatchNorm 对每一列（每个特征维度）分别计算该批量内的均值和方差。

### 训练与推理的差异

- **训练时**：使用当前批量的均值和方差
- **推理时**：使用训练过程中累积的移动平均均值和方差

如果训练时的批量分布与实际数据分布不一致，模型表现会下降。

### BatchNorm 的局限

| 局限 | 原因 |
|------|------|
| 小批量时失效 | 样本数过少（如 4 个），批量统计量不稳定 |
| 序列模型难以使用 | 语言模型中序列长度不同，不同时间步的特征含义不同，跨样本求均值无意义 |

---

## Layer Normalization

**Layer Normalization（层归一化，LayerNorm）**（Ba et al., 2016）将归一化维度从跨样本旋转为样本内：

- **BatchNorm**：跨样本，对同一特征维度归一化（列方向）
- **LayerNorm**：单个样本内，对所有特征维度归一化（行方向）

### 维度对比

```
          特征1   特征2   特征3   特征4
样本1   [  8.2    0.1   -3.5    7.0  ]  ← LayerNorm 对这一行求均值/方差
样本2   [ -2.1    0.3    1.2    5.5  ]  ← LayerNorm 对这一行求均值/方差
样本3   [  5.5    0.0   -1.0    9.1  ]  ← LayerNorm 对这一行求均值/方差
样本4   [  1.4    0.2    2.3    6.2  ]  ← LayerNorm 对这一行求均值/方差
          ↑每列
      BatchNorm 对这一列求均值/方差
```

### 公式

设单个样本的特征向量为 $x = [x_1, x_2, \ldots, x_D]$（$D$ 为特征维度）：

$$\mu = \frac{1}{D} \sum_{i=1}^{D} x_i \qquad \sigma^2 = \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2$$

$$y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

形式与 BatchNorm 相同，区别在于 $\mu$ 和 $\sigma^2$ 的计算范围从跨样本变成单样本内所有特征。

### LayerNorm 适合 Transformer 的原因

| 特性 | 说明 |
|------|------|
| 不依赖批量大小 | 每个样本独立计算，批量大小为 1 或 1000 效果相同 |
| 训练推理行为一致 | 无需区分训练时统计和推理时移动平均 |
| 适合变长序列 | 不同长度序列用同样方式处理，互不干扰 |

原始 Transformer（Attention is All You Need, 2017）以及 BERT、GPT 系列均采用 LayerNorm。

---

## RMSNorm

Zhang & Sennrich（2019）在 *Root Mean Square Layer Normalization* 中提出：LayerNorm 中的均值减法可能是多余的。

### 分析

LayerNorm 的归一化包含两步操作：

1. **去中心化（Re-centering）**：减去均值 $\mu$，将分布中心移到 0
2. **重新缩放（Re-scaling）**：除以标准差 $\sigma$，统一尺度

作者假设去中心化是多余的——神经网络的偏置参数能自行调整分布中心，真正起稳定作用的只是重新缩放。

### RMS（均方根）

去掉均值后，分母简化为**均方根（Root Mean Square，RMS）**：

$$\text{RMS}(x) = \sqrt{\frac{1}{D} \sum_{i=1}^{D} x_i^2}$$

RMS 衡量向量的整体幅度，不关心分布中心。

### RMSNorm 公式

$$y_i = \frac{x_i}{\sqrt{\dfrac{1}{D}\displaystyle\sum_{j=1}^{D} x_j^2 + \epsilon}} \cdot \gamma_i$$

$\epsilon$（通常 $10^{-6}$）在根号内部，向量趋近全零时仍安全。

### 与 LayerNorm 对比

| | LayerNorm | RMSNorm |
|--|-----------|---------|
| **减去均值** | 是 | 否 |
| **除以** | 标准差 $\sigma$ | RMS |
| **可学习参数** | $\gamma$（缩放）+ $\beta$（偏移） | 只有 $\gamma$（缩放） |
| **扫描次数** | 两遍（先算均值，再算方差） | 一遍（直接算 $x^2$） |

### 计算量差异

LayerNorm 需要两遍扫描特征向量：第一遍计算均值 $\mu$，第二遍基于均值计算方差 $\sigma^2$。RMSNorm 只需一遍：直接计算 $\sum x_i^2$。

在大模型中这个差异显著。实测 RMSNorm 比 LayerNorm 快约 **7-15%**。

论文实验表明，去掉均值减法后模型的训练稳定性和最终效果与 LayerNorm 持平甚至略优，验证了去中心化确实多余。

---

## Pre-Norm 与 Post-Norm

归一化在 Transformer 层中的位置，对训练稳定性的影响不亚于归一化方法本身。

### Post-Norm（原始 Transformer, 2017）

```
输入 x
  ↓
┌──────────────────────┐
│   Attention 计算      │
└──────────────────────┘
  ↓                 ↓ 残差连接
  └────────────────→ + ← x（原始输入）
                    ↓
              LayerNorm     ← 归一化在残差加法之后
                    ↓
              输出
```

### Pre-Norm（GPT-2 及之后的现代模型）

```
输入 x
  ↓
LayerNorm              ← 归一化在 Attention 之前
  ↓
┌──────────────────────┐
│   Attention 计算      │
└──────────────────────┘
  ↓                 ↓ 残差连接
  └────────────────→ + ← x（原始输入）
                    ↓
              输出
```

### 区别：残差路径上的梯度流

Post-Norm 中，归一化作用在残差加法之后，同时管控了主路径和跳跃连接，梯度流经归一化层时被调整，深层网络中梯度尺度随深度变化，训练不稳定。

Pre-Norm 中，残差连接直接将梯度绕过归一化层传递，梯度主路径未经扰动：

```
Post-Norm：  输出 → LayerNorm → (Attention + 残差) → 上一层
                          ↑ 归一化层修改了梯度尺度

Pre-Norm：   输出 → (Attention + 残差) → 上一层
                                ↑ 残差路径直接透传，梯度尺度稳定
```

Pre-Norm 允许使用更大的学习率，不需要严格的预热（warmup），可训练更深的网络。从 GPT-2 开始几乎所有大模型都采用 Pre-Norm。

---

## 其他归一化变体

### Group Normalization（组归一化）

在 BatchNorm 因批量太小而失效的场景（如目标检测，批量大小 2-4），GroupNorm 将特征通道分组，在每组内独立归一化：

```
特征通道按组划分（4组，每组4个通道）：
[ch1, ch2, ch3, ch4 | ch5, ch6, ch7, ch8 | ...]
 ←── 第1组 ──────────→  ←── 第2组 ──────→
   在组内计算均值/方差         在组内计算均值/方差
```

适用场景：图像分割、目标检测等批量小但通道多的任务。

### Instance Normalization（实例归一化）

每个样本的每个通道单独归一化，常用于**风格迁移（Style Transfer）**——去掉图像的风格信息（亮度、对比度），只保留内容特征。

### 归一化方法对比

| 方法 | 批量大小依赖 | 适用模型 | 典型场景 |
|------|-------------|---------|---------|
| BatchNorm | 依赖大批量 | CNN | 图像分类、大批量训练 |
| LayerNorm | 无关 | Transformer | GPT-2/3, BERT |
| RMSNorm | 无关 | Transformer | LLaMA, DeepSeek, Qwen |
| GroupNorm | 无关 | CNN | 目标检测，小批量 |
| InstanceNorm | 无关 | CNN | 风格迁移 |

---

## 现代大模型的归一化选择

| 模型 | 归一化方法 | 放置方式 |
|------|-----------|---------|
| GPT-2 | LayerNorm | Pre-Norm |
| GPT-3 | LayerNorm | Pre-Norm |
| BERT | LayerNorm | Post-Norm |
| LLaMA 1 / 2 / 3 | **RMSNorm** | Pre-Norm |
| Mistral | **RMSNorm** | Pre-Norm |
| DeepSeek-V2 / V3 | **RMSNorm** | Pre-Norm |
| Qwen 系列 | **RMSNorm** | Pre-Norm |

2023 年起发布的主流大模型，几乎全部采用 **RMSNorm + Pre-Norm** 的组合。

以 LLaMA-3 70B 为例：80 层 Transformer，每层两次归一化调用。每次归一化节省 10% 的时间，积累到整个训练周期（数百亿 token），节省的算力可折合数十万美元。

---

## 核心公式对比

**BatchNorm**（批量维度归一化）：

$$y = \gamma \cdot \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} + \beta$$

$$\mu_B = \frac{1}{N}\sum_{i=1}^N x_i \quad \sigma_B^2 = \frac{1}{N}\sum_{i=1}^N(x_i - \mu_B)^2$$

**LayerNorm**（特征维度归一化）：

$$y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

$$\mu = \frac{1}{D}\sum_{i=1}^D x_i \quad \sigma^2 = \frac{1}{D}\sum_{i=1}^D(x_i - \mu)^2$$

**RMSNorm**（无去中心化，仅缩放）：

$$y = \gamma \cdot \frac{x}{\text{RMS}(x)}$$

$$\text{RMS}(x) = \sqrt{\frac{1}{D}\sum_{i=1}^D x_i^2 + \epsilon}$$

三者的核心差异在两个维度：

1. **归一化范围**：批量维度 vs 特征维度
2. **是否去中心化**：减均值 vs 不减均值

---

## 数值示例：RMSNorm 手动计算

输入向量 $x = [3, -1, 4, -2]$（$D = 4$）：

**计算各元素平方**：

$$x^2 = [9, 1, 16, 4]$$

**计算平均平方值**：

$$\frac{1}{D}\sum x_i^2 = \frac{9+1+16+4}{4} = 7.5$$

**计算 RMS**：

$$\text{RMS}(x) = \sqrt{7.5} \approx 2.739$$

**归一化**：

$$\hat{x} = \frac{x}{\text{RMS}(x)} = \frac{[3, -1, 4, -2]}{2.739} \approx [1.095, -0.365, 1.461, -0.730]$$

**乘以可学习的 $\gamma$**（初始化为全 1 时）：

$$y = [1.095, -0.365, 1.461, -0.730]$$

归一化后向量的 RMS 值：$\sqrt{(1.095^2 + 0.365^2 + 1.461^2 + 0.730^2)/4} \approx 1.0$。激活值的幅度被统一到 1 附近。

---

## 总结

归一化的本质是控制激活值尺度，使梯度稳定流动。不同方法的演进回答同一个问题：**最少做什么，就能达到这个目标。**

- **BatchNorm**：跨样本归一化，适合 CNN，依赖批量大小
- **LayerNorm**：样本内归一化，适合 Transformer，解耦批量
- **RMSNorm**：去掉均值计算，只做缩放，更快且效果相当

现代大模型的选择：**Pre-Norm + RMSNorm**。
