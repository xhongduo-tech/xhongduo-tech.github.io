## 从一个让人困惑的现象说起

2014 年前后，深度学习研究者们遇到了一个让人抓狂的问题：

把神经网络层数从 20 层增加到 56 层，训练误差反而变高了。

```
训练误差
  |
  |  56层 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  |
  |  20层 ──────────────────────────────
  |
  └──────────────────────────────→ 训练轮数
```

这很反常。直觉上，层数多的网络参数更多、表达能力更强，理应更容易拟合数据。但事实是：**深的网络比浅的网络更难训练，效果反而更差。**

这不是过拟合，因为就连**训练误差**（模型在训练集上的表现）也更差——过拟合的症状是训练误差低但测试误差高。这是另一种问题：**网络深了，根本就学不好。**

为什么？怎么解决？

2015 年，微软亚研院的何恺明团队给出了一个简单到令人意外的答案，并凭借这个答案赢得了当年 ImageNet 图像识别竞赛冠军。他们的模型有 **152 层**——而当时业界普遍认为超过 20 层就很难训练了。

这个答案叫做**残差连接（Residual Connection）**，或者**跳跃连接（Skip Connection）**。

---

## 先搞清楚：神经网络在学什么

### 一层神经网络的工作

每一层神经网络本质上是一个**数学变换**：接收一个输入，输出一个新的向量。

```
输入 x  ───→  [第n层的变换]  ───→  输出 H(x)
```

我们希望这一层能学到某种有用的变换 $H(x)$。比如从"猫的像素特征"变换到"猫的高级语义特征"。

### 很深的网络：变换叠加再叠加

把很多层堆叠起来，每层的输出作为下一层的输入：

```
输入 x
  ↓
[第1层]  → x₁
  ↓
[第2层]  → x₂
  ↓
[第3层]  → x₃
  ↓
  ⋮    （继续叠加很多层）
  ↓
[第N层]  → 最终输出
```

理论上，层越多，能学到的变换就越复杂、越强大。这个逻辑没错。

那问题究竟出在哪里？

---

## 问题的根源：梯度消失

神经网络通过**反向传播（Backpropagation）**来学习。训练过程可以简化为：

1. 给网络一个输入，得到预测结果
2. 计算预测结果和正确答案的差距（损失）
3. 从输出层往输入层"传递误差信号"，告诉每一层"你的参数应该怎么调整"
4. 每一层根据收到的信号更新自己的参数

这个"误差信号"就是**梯度**。

问题在于：误差信号从输出层向输入层传递时，每经过一层，都要和那一层的导数**相乘**。

```
误差信号传播方向（←← 反向传播 ←←）：

输出层 → 第N层 → ... → 第2层 → 第1层 → 输入层
          ×g_N    ×g_2   ×g_1
```

如果每一层的导数 $g$ 都小于 1（这在很多常见情况下都会发生），误差信号在传播过程中会**指数级衰减**：

$$\text{梯度} \propto g_N \times g_{N-1} \times \cdots \times g_1$$

20 层网络，每层导数是 0.9：$0.9^{20} \approx 0.12$，勉强能学习。

56 层网络，每层导数是 0.9：$0.9^{56} \approx 0.003$，梯度几乎消失。

```
误差信号强度（从输出层反向传播到第n层）：

100%  ████████████████████  ← 输出层附近（学得好）
 12%  ██░░░░░░░░░░░░░░░░░░  ← 中间层
  3%  █░░░░░░░░░░░░░░░░░░░  ← 前几层（几乎学不到东西）
  0%  ░░░░░░░░░░░░░░░░░░░░  ← 靠近输入的层（梯度消失）
```

靠近输入的层收不到有效的学习信号，参数基本不更新——**等于白加了那些层**，甚至还带来了噪声和额外的优化难度。

---

## 核心洞见：不学整体，只学"差值"

何恺明团队的洞见来自一个简单的观察：

如果一个很深的网络，理论上总能找到一个解——至少可以让新加的层什么都不做（做恒等变换 $H(x) = x$），效果至少不比浅网络差。

但实践中，让叠加层学会"什么都不做"对优化器来说很难。

于是他们换了一个角度提问：

> **如果我们直接把输入 $x$ 加到输出上，让每层只学"需要改变的部分"，会怎样？**

设原来这一层想学的变换是 $H(x)$。我们不让它直接学 $H(x)$，而是让它学：

$$F(x) = H(x) - x$$

也就是**输出和输入之间的差值，即"残差"（Residual）**。

然后在结构上，把输入 $x$ 直接加到这层的输出上：

$$\text{最终输出} = F(x) + x$$

这样 $F(x) + x = H(x)$，数学上等价。但训练时的行为完全不同。

---

## 残差连接的结构

用图表示：

```
没有残差连接（普通深层网络）：

x ─→ [权重层] ─→ [激活函数] ─→ [权重层] ─→ [激活函数] ─→ 输出
                                                            H(x)
```

```
有残差连接（残差块）：

        ┌──────── 跳跃连接（Skip Connection）────────┐
        │                                            │
x ─────┼─→ [权重层] ─→ [激活函数] ─→ [权重层] ─→ (+) ─→ 输出
        │                                            │
        │              F(x)（残差）                  │
        │                                            │
        └──────────────── x（原始输入）───────────────┘

最终输出 = F(x) + x
```

那条从 $x$ 直接跳过整个变换块、连到输出的路径，就叫**跳跃连接（Skip Connection）**或**残差连接（Residual Connection）**。

这个改动看起来微不足道——不就是加了一条线吗？但效果是革命性的。

---

## 为什么这么简单的改动就能解决问题？

### 原因一：恒等变换变得极其容易

有了残差连接，如果某一层真的不需要做任何变换，网络只需要让 $F(x) \approx 0$——把权重学到接近零就行了。把权重学到零，比学到一个精确的恒等变换容易得多。

```
没有残差连接，学恒等变换：
  需要让 H(x) = x
  → 权重矩阵需要非常精确地等于单位矩阵
  → 很难学到

有了残差连接，学恒等变换：
  需要让 F(x) = 0
  → 只需要让所有权重趋近于零
  → 用权重衰减等常规手段就能做到
```

所以，加了残差连接之后，"多余的层"会自然退化为恒等变换，而不是主动干扰训练。

### 原因二：梯度有了"高速公路"

这是更根本的原因。

用偏微分的链式法则描述梯度传播。对于普通深层网络：

$$\frac{\partial \text{损失}}{\partial x} = \frac{\partial \text{损失}}{\partial H} \cdot \frac{\partial H}{\partial x}$$

每过一层，都要乘以 $\frac{\partial H}{\partial x}$，层层累乘，梯度不断衰减。

对于有残差连接的网络，输出是 $H = F(x) + x$，求对 $x$ 的偏导：

$$\frac{\partial H}{\partial x} = \frac{\partial F(x)}{\partial x} + 1$$

注意那个 **$+1$**。

把它代入整个网络的梯度传播链，无论中间经过多少层，梯度表达式里始终存在一个值为 1 的项：

$$\frac{\partial \text{损失}}{\partial x} = \frac{\partial \text{损失}}{\partial H} \cdot \left(\frac{\partial F}{\partial x} + 1\right)$$

这意味着，即使 $\frac{\partial F}{\partial x}$ 非常小（梯度消失），整个括号里的值也至少是 1，梯度能**不衰减地直接传回去**。

可以把残差连接想象成一条**梯度高速公路**：

```
梯度传播（反向方向）：

输出
  ↓ 梯度通过 F(x) 传播（可能衰减）
  ↓ 梯度通过跳跃连接直接传播（不衰减，+1 保证了这一点）
  ↓
叠加后，靠近输入的层总能收到有效信号
  ↓
输入
```

**深度不再是阻碍，而是优势。**

---

## 一个具体的数字感受

假设每层的 $\frac{\partial F}{\partial x} = 0.1$（一个偏小的梯度）：

```
普通网络（10层）：  0.1^10 = 0.0000000001  → 梯度完全消失

残差网络（10层）：  (0.1 + 1)^10 = 1.1^10 ≈ 2.59  → 梯度健康传播
```

这就是残差连接的魔法：一个简单的加法，把梯度消失问题从根本上化解了。

---

## ResNet：实验验证

何恺明团队在 2015 年 ImageNet 竞赛上用了一个 152 层的残差网络（ResNet-152），轻松击败了所有竞争对手，Top-5 错误率从 3.57% 降到历史最低。

同年，他们还实验了 1000 层的残差网络，依然能正常训练。而普通网络超过 20 层就开始退化。

```
ImageNet Top-5 错误率演进：

2012 AlexNet (8层)          16.4%  ████████████████
2014 VGG (19层)              7.3%  ███████
2014 GoogLeNet (22层)        6.7%  ██████
2015 ResNet-152 (152层)      3.57% ████
     ↑ 残差连接让 152 层成为可能
```

一个加法操作，打开了深度的天花板。

---

## 残差连接在 Transformer 里

今天几乎所有的大语言模型（GPT、BERT、LLaMA、DeepSeek……）都基于 Transformer 架构。残差连接是 Transformer 的基础组件之一，每一个注意力层（Attention）和前馈网络层（FFN）都被残差连接包裹：

```
Transformer 单层结构（Pre-Norm 版本）：

输入 x
  │
  ├──────────────────────────────┐
  │                              │（跳跃连接）
  ↓                              │
RMSNorm(x)                       │
  ↓                              │
Attention 计算                    │
  ↓                              │
输出 A                           │
  │                              ↓
  └─────────────────────────→  (+)  → x + A（第一个残差块的输出）
                                │
                                ├──────────────────────────┐
                                │                          │（又一条跳跃连接）
                                ↓                          │
                           RMSNorm(...)                    │
                                ↓                          │
                          FFN（前馈网络）                   │
                                ↓                          ↓
                                └──────────────────────→ (+)  → 最终输出
```

两个残差连接、两次加法，把 Attention 和 FFN 的计算都包裹在里面。

这也是上一篇文章里谈到 **Pre-Norm 和 Post-Norm** 的背景：归一化（RMSNorm）是放在残差块**内部**的 Attention/FFN 之前，还是放在残差加法**之后**，这两种选择的效果差异很大。

```
Pre-Norm（现代模型的选择）：       Post-Norm（原始 Transformer）：

x ─┬──────────────────┐          x ─┬──────────────────┐
   │                  │             │                  │
   ↓                  │             ↓                  │
RMSNorm               │          Attention              │
   ↓                  │             ↓                  ↓
Attention             │           (+) ←────────────────┘
   ↓                  ↓             ↓
 (+) ←────────────────┘          LayerNorm
   ↓                                ↓
 输出                            （继续）
```

Pre-Norm 让梯度能更干净地通过残差路径传回去；Post-Norm 在梯度路径上多了一次归一化操作，影响梯度尺度，不如 Pre-Norm 稳定。

---

## 残差连接解决的深层问题：退化

值得多说一句：残差连接解决的问题，严格来说不完全是梯度消失，而是一个叫做**退化（Degradation）**的现象。

梯度消失是问题之一，但退化更广泛：随着深度增加，模型变得越来越难优化，哪怕用各种技巧缓解了梯度消失，训练误差仍然会随层数增加而上升。

退化的本质是**优化困难**：对于深层网络，损失函数的"地形"极其复杂，优化器很难找到好的极小值。

残差连接改变了损失函数的"地形"，让它更加平滑，更易于优化。

```
普通深层网络的损失地形（崎岖）：  残差网络的损失地形（平滑）：

    损失                              损失
     │  ↗↘↗↘↗↘↗↘                   │
     │      ↗↘↗↘↗↘                  │  ↘
     │          ↗↘                   │    ↘
     │                               │      ↘______
     └──────────→ 参数               └──────────→ 参数

     难以收敛                          容易找到好的极小值
```

---

## 一个有趣的视角：集成学习

研究者 Veit 等人在 2016 年提出了一个有趣的解释角度：

**带残差连接的深层网络，本质上是指数级数量的浅层网络的集成（Ensemble）。**

对于一个有 $n$ 个残差块的网络，每个块都可以选择"走跳跃连接（不做变换）"或"走变换路径"。两种选择，$n$ 个块，总共有 $2^n$ 条不同的路径。

```
3个残差块：

路径1：变换1 → 变换2 → 变换3      （最深路径）
路径2：变换1 → 变换2 → 跳过3
路径3：变换1 → 跳过2 → 变换3
路径4：变换1 → 跳过2 → 跳过3
路径5：跳过1 → 变换2 → 变换3
路径6：跳过1 → 变换2 → 跳过3
路径7：跳过1 → 跳过2 → 变换3      （最浅路径）
路径8：跳过1 → 跳过2 → 跳过3      （恒等变换）

共 2^3 = 8 条路径
```

前向传播时，所有路径同时被"激活"，最终输出是所有路径输出的叠加——就像把很多个不同深度的网络的预测结果取平均一样。

集成多个模型一般比单个模型效果更好、更鲁棒。残差网络在结构上天然实现了这一点。

---

## 总结

残差连接的思想用一句话说清楚：

> **不要让网络学"完整的变换 $H(x)$"，而是让它学"需要改变的部分 $F(x) = H(x) - x$"，然后把原始输入加回去。**

这个改动带来了三个关键好处：

**第一**：梯度高速公路。跳跃连接为梯度提供了不经过权重层的直接传播路径，从根本上缓解了梯度消失，让几百层的网络也能稳定训练。

**第二**：恒等变换变容易。不需要的层能自然退化为"什么都不做"，多余的层不再是负担。

**第三**：损失地形更平滑。网络更容易找到好的极小值，优化过程更稳定。

从 2015 年的 ResNet，到今天所有主流大语言模型（LLaMA、DeepSeek、GPT 系列），残差连接一直是每个 Transformer 层的基础骨架。理解了它，你就掌握了理解现代深度学习架构的另一块关键基石——配合上一篇的归一化，你已经看懂了 Transformer 单层结构里最核心的两个设计。
