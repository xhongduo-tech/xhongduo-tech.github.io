## 残差连接是什么

**残差连接（Residual Connection）**，也称**跳跃连接（Skip Connection）**，是一种网络结构设计：将某一层的输入直接加到该层的输出上，使网络只需学习输入与输出之间的差值（残差），而非完整的变换。

$$\text{输出} = F(x) + x$$

其中 $F(x)$ 是网络层学到的残差，$x$ 是原始输入。

残差连接由何恺明团队在 2015 年提出（ResNet 论文），解决了深层神经网络的退化问题，使上百层网络的有效训练成为可能。如今它是所有主流 Transformer 架构（GPT、LLaMA、DeepSeek 等）的基础组件。

---

## 问题背景：深层网络的退化

2014 年前后，研究者发现一个反常现象：将网络从 20 层增加到 56 层，训练误差反而升高。

```
训练误差
  |
  |  56层 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  |
  |  20层 ──────────────────────────────
  |
  └──────────────────────────────→ 训练轮数
```

这不是过拟合——过拟合表现为训练误差低但测试误差高。这里连训练误差都更高，说明网络本身更难优化。这个现象称为**退化（Degradation）**。

理论上，更深的网络参数更多、表达能力更强。即使新增的层不需要做任何有用的变换，只要学会恒等映射 $H(x) = x$，效果至少不会比浅网络差。但实践中，优化器很难让多余的层学会"什么都不做"。

---

## 退化的主要原因：梯度消失

### 前向传播与反向传播

神经网络的训练分两步：

1. **前向传播（Forward Pass）**：输入数据逐层通过网络，产生预测结果
2. **反向传播（Backpropagation）**：从输出层向输入层传递误差信号（**梯度**），指导每层参数如何调整

梯度从输出层向输入层传递时，每经过一层都要与该层的导数相乘：

$$\text{梯度} \propto g_N \times g_{N-1} \times \cdots \times g_1$$

如果每层导数 $g$ 小于 1，梯度在传播过程中指数级衰减。

### 数值直觉

假设每层导数为 0.9：

- 20 层网络：$0.9^{20} \approx 0.12$，勉强可训练
- 56 层网络：$0.9^{56} \approx 0.003$，梯度接近消失

```
误差信号强度（从输出层反向传播到第 n 层）：

100%  ████████████████████  ← 输出层附近
 12%  ██░░░░░░░░░░░░░░░░░░  ← 中间层
  3%  █░░░░░░░░░░░░░░░░░░░  ← 前几层
  0%  ░░░░░░░░░░░░░░░░░░░░  ← 靠近输入的层（梯度消失）
```

靠近输入的层收不到有效学习信号，参数基本不更新。新增的层不仅没有帮助，还引入了噪声和优化难度。

---

## 核心思想：学残差而非完整变换

何恺明团队的关键洞见：不让网络层直接学习目标变换 $H(x)$，而是学习输出与输入之间的差值。

设目标变换为 $H(x)$，将学习目标改为：

$$F(x) = H(x) - x$$

结构上将输入 $x$ 直接加到层的输出：

$$\text{输出} = F(x) + x$$

数学上与原始目标等价（$F(x) + x = H(x)$），但训练行为完全不同。

---

## 残差块的结构

```
普通深层网络：

x ─→ [权重层] ─→ [激活函数] ─→ [权重层] ─→ [激活函数] ─→ H(x)
```

```
残差块：

        ┌──────── 跳跃连接（Skip Connection）────────┐
        │                                            │
x ─────┼─→ [权重层] ─→ [激活函数] ─→ [权重层] ─→ (+) ─→ 输出
        │                                            │
        │              F(x)（残差）                  │
        │                                            │
        └──────────────── x（原始输入）───────────────┘

输出 = F(x) + x
```

从 $x$ 直接跳过变换块连到输出的路径，即跳跃连接。

代码实现一个残差块：

```python
def residual_block(x):
    residual = x              # 保存原始输入
    out = weight_layer_1(x)
    out = activation(out)
    out = weight_layer_2(out)
    out = out + residual      # F(x) + x
    return out
```

核心操作是一行加法 `out = out + residual`。

---

## 残差连接为什么有效

### 恒等变换变得容易

有残差连接时，如果某层不需要做任何变换，只需让 $F(x) \approx 0$（权重趋近零即可）。没有残差连接时，学习恒等变换需要权重矩阵精确等于单位矩阵，优化器很难做到。

```
无残差连接，学恒等变换：H(x) = x → 权重矩阵 ≈ 单位矩阵（难）
有残差连接，学恒等变换：F(x) = 0 → 权重趋近零（容易）
```

多余的层自然退化为恒等变换，不会干扰训练。

### 梯度高速公路

这是更根本的原因。

对于有残差连接的层，输出为 $H = F(x) + x$，对 $x$ 求偏导：

$$\frac{\partial H}{\partial x} = \frac{\partial F(x)}{\partial x} + 1$$

关键在于 **$+1$** 这一项。代入完整的梯度传播链：

$$\frac{\partial \text{损失}}{\partial x} = \frac{\partial \text{损失}}{\partial H} \cdot \left(\frac{\partial F}{\partial x} + 1\right)$$

即使 $\frac{\partial F}{\partial x}$ 非常小（趋向梯度消失），括号内的值至少为 1，梯度能不衰减地直接传回。跳跃连接为梯度提供了一条不经过权重层的直达路径。

### 数值对比

假设每层 $\frac{\partial F}{\partial x} = 0.1$：

```
普通网络（10 层）：  0.1^10 = 0.0000000001  → 梯度消失
残差网络（10 层）：  (0.1 + 1)^10 = 1.1^10 ≈ 2.59  → 梯度正常传播
```

### 损失地形更平滑

残差连接改变了损失函数的地形，使其更平滑、更易优化：

```
普通深层网络的损失地形：       残差网络的损失地形：

    损失                        损失
     │  ↗↘↗↘↗↘↗↘              │
     │      ↗↘↗↘↗↘             │  ↘
     │          ↗↘              │    ↘
     │                          │      ↘______
     └──────────→ 参数          └──────────→ 参数

     难以收敛                     容易找到好的极小值
```

---

## ResNet 的实验验证

何恺明团队在 2015 年 ImageNet 竞赛中使用 152 层残差网络（ResNet-152），Top-5 错误率降至 3.57%，达到当时的历史最低。同年实验了 1000 层残差网络，仍能正常训练。普通网络超过 20 层即开始退化。

```
ImageNet Top-5 错误率演进：

2012 AlexNet (8 层)          16.4%  ████████████████
2014 VGG (19 层)              7.3%  ███████
2014 GoogLeNet (22 层)        6.7%  ██████
2015 ResNet-152 (152 层)      3.57% ████
```

---

## 残差连接在 Transformer 中的应用

当前主流大语言模型（GPT、LLaMA、DeepSeek 等）均基于 Transformer 架构。残差连接是 Transformer 的基础组件，每个注意力层（Attention）和前馈网络层（FFN）都由残差连接包裹：

```
Transformer 单层结构（Pre-Norm 版本）：

输入 x
  │
  ├──────────────────────────────┐
  │                              │（跳跃连接）
  ↓                              │
RMSNorm(x)                       │
  ↓                              │
Attention 计算                    │
  ↓                              │
输出 A                           │
  │                              ↓
  └─────────────────────────→  (+)  → x + A（第一个残差块输出）
                                │
                                ├──────────────────────────┐
                                │                          │（跳跃连接）
                                ↓                          │
                           RMSNorm(...)                    │
                                ↓                          │
                          FFN（前馈网络）                   │
                                ↓                          ↓
                                └──────────────────────→ (+)  → 最终输出
```

每层包含两个残差连接，分别包裹 Attention 和 FFN。

### Pre-Norm 与 Post-Norm

归一化操作（RMSNorm/LayerNorm）放在残差块内部的变换之前（Pre-Norm）还是残差加法之后（Post-Norm），对训练稳定性影响显著。

```
Pre-Norm（现代模型的选择）：       Post-Norm（原始 Transformer）：

x ─┬──────────────────┐          x ─┬──────────────────┐
   │                  │             │                  │
   ↓                  │             ↓                  │
RMSNorm               │          Attention              │
   ↓                  │             ↓                  ↓
Attention             │           (+) ←────────────────┘
   ↓                  ↓             ↓
 (+) ←────────────────┘          LayerNorm
   ↓                                ↓
 输出                            （继续）
```

Pre-Norm 使梯度能更直接地通过残差路径传播，训练更稳定。Post-Norm 在梯度路径上增加了归一化操作，影响梯度尺度。

---

## 集成学习视角

Veit 等人在 2016 年提出的解释：带残差连接的深层网络本质上是指数级数量的浅层网络的**集成（Ensemble）**。

对于 $n$ 个残差块的网络，每个块可以选择走变换路径或跳跃连接，共有 $2^n$ 条不同路径。

```
3 个残差块的所有路径：

路径1：变换1 → 变换2 → 变换3    （最深路径）
路径2：变换1 → 变换2 → 跳过3
路径3：变换1 → 跳过2 → 变换3
路径4：变换1 → 跳过2 → 跳过3
路径5：跳过1 → 变换2 → 变换3
路径6：跳过1 → 变换2 → 跳过3
路径7：跳过1 → 跳过2 → 变换3    （最浅路径）
路径8：跳过1 → 跳过2 → 跳过3    （恒等变换）

共 2^3 = 8 条路径
```

前向传播时所有路径同时激活，最终输出是所有路径输出的叠加，等价于多个不同深度网络的预测结果取平均。集成多个模型通常比单一模型更鲁棒，残差网络在结构上天然实现了这一效果。

---

## 总结

残差连接的核心思想：不让网络学完整变换 $H(x)$，而是学残差 $F(x) = H(x) - x$，然后将原始输入加回。

三个关键效果：

- **梯度高速公路**：跳跃连接为梯度提供不经过权重层的直接传播路径，从根本上缓解梯度消失，使数百层网络能稳定训练
- **恒等变换容易学**：不需要的层自然退化为"什么都不做"，多余的层不成为负担
- **损失地形平滑**：网络更容易找到好的极小值，优化过程更稳定

残差连接从 2015 年的 ResNet 到当前所有主流大语言模型，一直是 Transformer 每层结构的基础骨架。
