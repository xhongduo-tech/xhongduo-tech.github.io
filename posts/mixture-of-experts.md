## 引言：一道无解的方程

大语言模型的核心规律只有一句话：**参数越多，能力越强**。

从 GPT-2 的 15 亿参数到 GPT-3 的 1750 亿，这条规律被反复验证。但随之而来的是一道几乎无解的方程：

> **计算成本 ≈ 参数量**

一个 1750 亿参数的密集模型，每生成一个词，就需要激活并计算全部 1750 亿个参数。这意味着：推理成本极高、速度难以压缩、扩展受限——再大一倍的模型，算力成本就翻一倍，没有上限。

**混合专家模型（Mixture of Experts，MoE）**是目前已知最有效的破解方案：**参数量可以不断扩大，但每次计算只激活其中一小部分**。

这不是妥协，是一种更聪明的设计哲学。

---

## 从一个问题开始：当诊所变成了医院

在进入技术细节之前，先用一个演进故事建立直觉。

### 第一阶段：全科诊所（对应密集模型）

小镇上有一家诊所，只有一位全科医生王医生。他一个人负责所有病人——感冒发烧、骨折外伤、心脏病、皮肤病……每来一位病人，他都要从头到尾亲自检查，事无巨细，全部处理。

这在规模小的时候行得通。但随着小镇人口增长，病人越来越多，王医生开始力不从心。他需要处理的问题太庞杂，每位病人都消耗了他的全部精力。**更糟糕的是：无论你只是来量个血压，还是来做复杂手术，他花的力气都一样多。**

这就是**密集模型（Dense Model）**的困境：每一个 token 进来，所有参数都平等参与计算，无论这个词简单还是复杂。

### 第二阶段：大型综合医院（对应 MoE 模型）

城市里新建了一家大型医院，请来了 8 位顶尖专科医生：心脏科、神经科、骨科、肿瘤科、皮肤科、消化科、儿科、急诊科。

但 8 位医生同时出诊，如果每位病人都要依次经过 8 位专家，效率反而更低。

**解决方案：在医院大门口设一个分诊台。**

由一位经验丰富的分诊护士（**Router，路由器**）来把关。每位病人进门后，分诊护士快速评估症状，然后：

- 心脏病患者 → 心脏科 + 急诊科（激活 2 位专家）
- 骨折患者 → 骨科 + 急诊科（激活 2 位专家）
- 皮肤病患者 → 皮肤科（激活 1 位专家）

其余 6 位医生继续在各自诊室待命，**完全不参与这位病人的诊断**。

---

这个演进揭示了 MoE 的三个核心角色：

1. **专家（Experts）**：并行存在的多个子神经网络，各自处理不同类型的信息
2. **路由器（Router / Gating Network）**：决定"这个 token 该由哪几个专家处理"的轻量级网络
3. **稀疏激活（Sparse Activation）**：每次只调用 K 个专家，其余专家完全不参与计算，不消耗任何算力

**直观的数据对比：**

| 架构 | 总参数 | 每次推理激活 | 算力消耗 |
|------|--------|-------------|---------|
| 密集模型（LLaMA 2 13B）| 13B | 13B（100%）| 高 |
| MoE 模型（Mixtral 8x7B）| 46.7B | 12.9B（28%）| 中低 |

Mixtral 8x7B 拥有约 47B 总参数，但每次推理只用 13B——以密集 13B 模型的算力成本，撬动了 47B 参数量级的知识储备，在多个基准测试上超越了 LLaMA 2 70B。

---

## MoE 在 Transformer 里的位置

了解了直觉，来看 MoE 具体在模型里"住"在哪里。

标准 Transformer 的每一层由两类子结构交替组成：**Attention 层**负责让词与词之间"交流"、理解上下文关系；**FFN 层**（前馈网络）负责对每个词的表示进行深度变换：

```text
输入序列
 ↓
[Multi-Head Attention]  ← 所有词互相"交流"，理解关系
 ↓
[FFN Layer]             ← 对每个词的表示单独深度加工
 ↓
重复 N 层
 ↓
输出
```

MoE 做的事情只有一件：**把 FFN 层换成一组并行的小型 FFN（专家）+ 一个路由器**。

```text
输入 token x
 ↓
[Multi-Head Attention]         ← 不变，所有 token 共享
 ↓
[Router] 给每个专家打分，选 Top-K
 ↓           ↓              ↓
[Expert 1] [Expert 2]  ... [Expert N]   ← 每个都是一个独立的小 FFN
             ↓
   只有被选中的 K 个专家参与计算，加权求和后输出
```

有一个关键细节：**Attention 层完全不受影响，所有 token 共享同一套 Attention 参数**。稀疏化只发生在 FFN 层。这个设计决定了 MoE 在工程实现上的基本结构。

---

## 数学深挖：稀疏门控，逐步拆解

"稀疏"这个词听起来高级，说白了就是**"万中无一，只挑几个"**。

为了实现"只有少数专家被激活，其他专家休息"的效果，路由器（Router）在后台悄悄进行了一场极其严密的"选秀打分"。

为了让大家彻底搞懂，我们先讲通俗逻辑，再带入具体的数字跑一遍公式。

---

### 第一步：基础打分 + 引入"运气成分"（添加噪声）

当一个词（数据输入 $x$）进来时，路由器会通过权重矩阵 $W_g$ 给每个专家打一个基础分。但为了防止每次都只选那几个"老戏骨"（训练早期容易形成路径依赖，导致大部分专家永远不被选中），系统会引入标准正态分布的噪声，相当于给专家们加一点"随机运气分"。

**[数学拆解]** 公式如下：

$$H(x) = x \cdot W_g + \text{StandardNormal}() \cdot \text{Softplus}(x \cdot W_{noise})$$

- $x \cdot W_g$：基础评分，路由器通过权重矩阵 $W_g$ 对当前输入 $x$ 打分
- $\text{Softplus}(x \cdot W_{noise})$：噪声的幅度上限，由另一个可学习矩阵 $W_{noise}$ 控制
- $\text{StandardNormal}()$：每次独立采样的标准正态噪声，让分数带上随机性

**[推演示例]** 假设我们一共只有 4 个专家（$n=4$），输入了一个词汇。经过上述公式计算，4 个专家包含"运气分"的总得分 $H(x)$ 如下：

| 专家 | 基础分 | 加噪后得分 |
|------|--------|-----------|
| 专家 1 | 7.6 | **8.0** |
| 专家 2 | 2.1 | **2.0** |
| 专家 3 | 1.3 | **1.0** |
| 专家 4 | 6.8 | **7.0** |

此时的得分数组为：$H(x) = [8.0,\ 2.0,\ 1.0,\ 7.0]$

---

### 第二步：残酷的淘汰赛（Top-K 机制）

接下来是无情的淘汰环节。假设我们设定 $K=2$，也就是每次只激活排名前 2 的专家。

**[数学拆解]** 路由器使用 KeepTopK 函数，保留前 $K$ 名的分数，把落选者的分数直接变成负无穷大（$-\infty$），意思是彻底出局：

$$\text{KeepTopK}(v, k)_i = \begin{cases} v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v \\ -\infty & \text{otherwise} \end{cases}$$

**[推演示例]** 在我们的数组 $[8.0,\ 2.0,\ 1.0,\ 7.0]$ 中，排名前 2 的是专家 1（8.0）和专家 4（7.0）。经过 KeepTopK 处理后，数组变成了：

$$[8.0,\ 2.0,\ 1.0,\ 7.0] \longrightarrow [8.0,\ -\infty,\ -\infty,\ 7.0]$$

专家 2 和专家 3 已经出局——分数被打入"冷宫"。

---

### 第三步：把分数变成"切蛋糕"的比例（Softmax 函数）

最后一步，需要把原始分数变成加起来等于 100%（即 1.0）的权重比例，这样才能作为乘数去混合专家的输出。这就是 **Softmax 函数**的作用。

**[数学拆解]** Softmax 的核心原理是将自然常数 $e$ 作为底数、得分为指数，再求各自占比。它有一个神奇的性质：$e^{-\infty} = 0$，所以被淘汰的专家权重精确地等于 0：

$$G(x) = \text{Softmax}(\text{KeepTopK}(H(x),\ k))$$

**[推演示例]** 将上一步的数组 $[8.0,\ -\infty,\ -\infty,\ 7.0]$ 代入 Softmax：

$$\text{专家 1 的权重} = \frac{e^{8.0}}{e^{8.0} + e^{-\infty} + e^{-\infty} + e^{7.0}} = \frac{e^{8.0}}{e^{8.0} + 0 + 0 + e^{7.0}}$$

简单计算（$e^{8.0} \approx 2981$，$e^{7.0} \approx 1097$，总和 $\approx 4078$）：

| 专家 | 权重计算 | 最终权重 |
|------|---------|---------|
| 专家 1 | $2981\ /\ 4078$ | **≈ 0.73（73%）** |
| 专家 2 | $0\ /\ 4078$ | **= 0** |
| 专家 3 | $0\ /\ 4078$ | **= 0** |
| 专家 4 | $1097\ /\ 4078$ | **≈ 0.27（27%）** |

---

### 最终的完美闭环

最后，回到 MoE 的总输出公式：

$$y = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)$$

代入我们的数字，这行公式实际执行的是：

$$y = 0.73 \cdot E_1(x) + 0 \cdot E_2(x) + 0 \cdot E_3(x) + 0.27 \cdot E_4(x)$$

**因为中间两项是 0，计算机在底层运算时，根本不需要去计算 $E_2(x)$ 和 $E_3(x)$！**

算力就这样被完美地省下来了，而公式在数学逻辑上依然保持了严密的连贯性。这就是 MoE 稀疏激活最迷人的数学魔法。

---

## 负载均衡：训练时最容易忽视的暗礁

理解了 MoE 的运作原理后，一个问题浮出水面：**路由器可以自由选择专家，它会不会总是偏爱同几个？**

答案是：**如果不加干预，几乎一定会。**

### 路由器崩溃（Router Collapse）

想象一下第一步打分时的场景：训练刚开始，专家 1 偶然被多选了几次，获得了更多梯度更新，能力稍有提升。路由器注意到它效果不错，下次更倾向于选它。它被选得越多 → 更新越多 → 越强 → 被选得更多……

这个**马太效应式的正反馈**会导致：最终只有 1-2 个专家在工作，其余专家几乎"失业"，大量参数浪费。MoE 退化成了一个事实上的密集模型。

### 解决方案一：辅助损失（Auxiliary Loss）

Shazeer et al.（2017）引入了一个额外的**均衡惩罚项**，专门惩罚"不均衡"的分配：

$$\mathcal{L}_{aux} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i$$

- $f_i$：专家 $i$ 实际处理的 token 比例（真实分配量）
- $P_i$：路由器对专家 $i$ 的平均倾向（路由器"心里"想选 $i$ 的概率）
- 当每个 $f_i$ 和 $P_i$ 都趋近 $1/N$ 时（所有专家均等被选），这个损失最小

训练时，总损失 = 主任务损失 + 均衡惩罚：

$$\mathcal{L} = \mathcal{L}_{main} + \mathcal{L}_{aux}$$

超参数 $\alpha$（通常为 $10^{-2}$ 到 $10^{-3}$）控制均衡的力度：$\alpha$ 太小则路由崩溃，$\alpha$ 太大则均衡任务压制主任务的学习。这个值的调整是训练大型 MoE 的核心"手艺活"之一。

### 解决方案二：容量因子（Capacity Factor）

除了损失函数，还可以给每个专家设置**接诊上限**（就像医院规定每位医生每天最多接诊 30 位病人）：

$$C = \left\lfloor \frac{T \cdot k}{N} \right\rfloor \times \text{capacity\_factor}$$

$T$ 为 batch 内 token 总数，capacity_factor 通常设为 1.25，留出一定缓冲。超出上限 $C$ 的 token 会被"溢出"——跳过这个专家，直接通过残差连接传递，放弃这次专家处理。

这是工程上的妥协：防止个别专家成为瓶颈，代价是溢出的 token 处理质量略有下降。

---

## 关键论文演进：三十年的接力

### 1991 — 概念的起源

Jacobs et al. 在 1991 年提出 MoE 的原始框架：不同的数据由不同的子模型处理，每个子模型专注于数据空间的某个子区域。这时还是统计机器学习时代，MoE 只是一个学术概念，等待硬件和规模的到来。

### 2017 — 第一次大规模神经网络验证

Shazeer et al.（Google Brain）发表 [*Outrageously Large Neural Networks*](https://arxiv.org/abs/1701.06538)，将 MoE 层嵌入 LSTM 语言模型，实现约 **137B 参数**的模型（当时密集模型普遍在数十亿以内）。

最重要的贡献：提出 **Noisy Top-K Gating**（也就是我们在"第一步"讲到的加噪声机制），正式解决了路由崩溃问题。这是 MoE 从理论走向工程实用的第一块基石。

### 2020 — GShard：MoE 进入千亿规模

Google 发表 *GShard*，将 MoE 用于神经机器翻译，实现 **600B 总参数**（每次激活约 48B）。

最关键的工程贡献：提出 **Expert Parallelism（专家并行）**——把不同专家分配到不同的 TPU 核心，让参数内存分散在多个计算节点上。这是让大规模 MoE 真正能够在工程上落地的基础设计，后来所有大型 MoE 系统都继承了这一范式。

### 2021 — Switch Transformer：极简主义的万亿参数

Google 的 Switch Transformer 提出了一个大胆简化：**K=1，每个 token 只激活一个专家**。

> "你只需要一个专家，但要有很多个可以选。"

K=1 相比 K=2 的工程优势：路由梯度只流向一个专家（更简单），通信量减半，训练稳定性更好。最终，Switch Transformer 达到 **1.6 万亿参数**，在等计算预算下比 T5-XXL 快约 7 倍，正式证明了"参数量可以远超激活量，模型依然能有效学习"。

### 2023 — Mixtral 8x7B：开源 MoE 的破局

Mistral AI 发布 Mixtral 8x7B，第一次让开源社区拿到了工业级 MoE 模型，将 MoE 的价值主张变成了每个人都可以复现的现实：

| 指标 | 数值 |
|------|------|
| 总参数 | ~46.7B |
| 每次激活参数 | ~12.9B（约 28%）|
| 专家数 | 8 |
| 每次激活专家数 | 2（Top-2）|
| 与 LLaMA 2 70B 的对比 | 多个基准上超越，算力不到 1/3 |

### 2024 — DeepSeek-MoE：把专家切得更细

DeepSeek 团队提出两项架构创新，将 MoE 的参数效率推向新高度。

**创新 1：细粒度专家分割**

传统 MoE 中每个专家是一个完整的 FFN。DeepSeek-MoE 将每个专家的中间维度缩小为原来的 $1/m$，同时把专家数增加 $m$ 倍，激活的专家数也增加 $m$ 倍——总计算量不变，但路由器拥有了更细的组合粒度。64 个小专家的任意 6 种组合，远比 8 个大专家的任意 2 种组合更灵活、知识冗余更少。

**创新 2：共享专家隔离**

保留若干个"**始终激活的共享专家**"，专门处理所有 token 都需要的通用知识（基础语法、常见实体等）。路由专家因此可以完全专注于特化知识：

$$y = \underbrace{\sum_{i=1}^{K_s} E_i^{shared}(x)}_{\text{共享专家（始终激活）}} + \underbrace{\sum_{j \in \text{TopK}} G_j(x) \cdot E_j^{routed}(x)}_{\text{路由专家（稀疏激活）}}$$

DeepSeek-V3（2024 年底）将这一架构推至 **671B 总参数、每次激活仅 37B**，在众多基准测试上追平或超越 GPT-4o，训练成本据报道不到同类模型的 1/10。

---

## 工程挑战：为什么 MoE 不是银弹

理论上 MoE 完美。现实中，工程师们要额外解决一个密集模型没有的问题：**不同专家在不同 GPU 上，token 怎么找到自己的专家？**

### Expert Parallelism + All-to-All 通信

MoE 的天然结构允许把不同专家分放在不同 GPU：

```text
GPU 0  |  GPU 1  |  GPU 2  |  GPU 3
-------+---------+---------+--------
Expert1  Expert2   Expert3   Expert4
Expert5  Expert6   Expert7   Expert8
```

但问题随之而来：一个 batch 里每个 token 被路由到不同专家，而不同专家在不同 GPU 上。这需要以下三个阶段：

```text
阶段 1 [分发]  每个 GPU 把"不属于自己专家"的 token 发送给对应 GPU
               → All-to-All 通信（每个 GPU 与其他所有 GPU 交换数据）

阶段 2 [计算]  每个 GPU 在本地运行自己负责的专家，处理收到的 token

阶段 3 [收集]  把计算结果发回原来的 GPU
               → 再一次 All-to-All 通信
```

**All-to-All** 是代价最高的集合通信操作。前向传播需要 2 次（分发 + 收集），反向传播还需要 2 次，共 4 次 All-to-All。这就是为什么：

- MoE 在**大 batch 高吞吐**场景表现出色（通信固定开销被摊薄）
- MoE 在**单请求低延迟**场景反而不如同等激活参数量的密集模型（通信无法摊薄）

### 内存：另一道墙

**所有专家的参数必须常驻显存**，即使每次只用其中 2 个。

Mixtral 8x7B 的 46.7B 参数以 fp16 存储，需要约 **93 GB 显存**，远超单张 H100 的 80 GB，至少需要 2 卡部署。DeepSeek-V3 的 671B 参数则需要 40 块 H100 以上才能完整加载。

llama.cpp 等推理框架正在探索 **Expert Offloading**——将暂时不使用的专家卸载到 CPU 内存或 NVMe SSD，按需调回显存，目标是让消费级硬件也能运行大型 MoE 模型。

---

## 现代 MoE 模型一览

| 模型 | 机构 | 总参数 | 激活参数 | 专家数 | K |
|------|------|--------|---------|--------|---|
| Switch-C | Google | 1571B | ~7B/层 | 2048 | 1 |
| GLaM | Google | 1200B | 96B | 64 | 2 |
| Mixtral 8x7B | Mistral AI | 46.7B | 12.9B | 8 | 2 |
| Mixtral 8x22B | Mistral AI | 141B | 39B | 8 | 2 |
| Grok-1 | xAI | 314B | ~86B | 8 | 2 |
| DeepSeek-V2 | DeepSeek | 236B | 21B | 160 | 6 |
| DeepSeek-V3 | DeepSeek | 671B | 37B | 256 | 8 |

**一个有趣的规律**：从 Switch-C 到 DeepSeek-V3，专家数从 2048 降到 256，但每个专家更小、组合粒度更细。这印证了 DeepSeek-MoE"细粒度分割"方向的正确性——不需要最多的专家，需要最灵活的组合。

GPT-4 和 Gemini 1.5 Pro 被业界广泛认为采用了 MoE 架构，但 OpenAI 和 Google 官方均未正式确认细节。

---

## MoE 的局限与未来

### 三道真实的墙

**显存是第一道墙。** 所有专家常驻显存的设计，使得 MoE 的部署门槛远高于同等激活参数量的密集模型。这是 MoE 在边缘部署、消费级硬件上最大的障碍。

**训练稳定性是第二道墙。** 辅助损失的权重 $\alpha$、容量因子的设置、噪声幅度的调校……这些超参数对训练结果极为敏感。DeepSeek 等团队在论文里花大量篇幅描述调参策略，正说明这是一门"手艺活"。

**低延迟场景是第三道墙。** All-to-All 通信的延迟是刚性成本，无法通过软件优化完全消除。在追求极低延迟的在线应用中，MoE 需要额外手段（专家预取、投机路由）才能与密集模型竞争。

### 三个值得期待的方向

**更细的粒度。** DeepSeek-MoE 的细粒度专家方向还有空间。极端情况可能是"特征维度级路由"——同一 token 的不同隐层维度路由到不同专家，实现更彻底的知识分工。

**自适应深度（MoE + Early Exit）。** 简单的 token 不需要经过所有层。未来可能出现"简单词提前退出，复杂词继续深入"的自适应深度 MoE，让算力分配更精准——不只是在层内稀疏激活专家，而是在层间也动态分配计算深度。

**可解释性研究。** 研究者已经发现，训练完成的 MoE 中，专家会自发展现语义偏好：某些专家倾向于处理数字运算，某些偏向代码，某些偏向多语言。这为"可设计的专家分工"方向打开了大门——也许未来我们可以主动规定专家学什么，而不是等待自发涌现。

---

## 总结

MoE 的核心思想只有一句话：

> **输入内容决定计算路径——而不是所有参数平等参与每次计算。**

从 1991 年 Jacobs 的学术构想，到 2024 年 DeepSeek-V3、GPT-4、Gemini 等最强 AI 系统背后的核心架构，MoE 走过了三十年。

它不是银弹：显存压力、All-to-All 通信、训练不稳定性，都是真实的工程代价。但在"以最小的每次推理 FLOP 撬动最大的模型参数容量"这个维度上，目前没有更好的答案。

如果说 Scaling Law 是 AI 的"油门"，那 MoE 就是让这辆车**油耗减半的变速箱设计**——参数规模可以继续增长，算力成本不必同步增长。这两个轮子一起转，才驱动了我们今天看到的 AI 能力边界。

---

*参考文献*

- Jacobs et al., 1991. *Adaptive Mixtures of Local Experts.*
- Shazeer et al., 2017. *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.* [arxiv:1701.06538](https://arxiv.org/abs/1701.06538)
- Lepikhin et al., 2020. *GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.* [arxiv:2006.16668](https://arxiv.org/abs/2006.16668)
- Fedus et al., 2021. *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.* [arxiv:2101.03961](https://arxiv.org/abs/2101.03961)
- Jiang et al., 2024. *Mixtral of Experts.* [arxiv:2401.04088](https://arxiv.org/abs/2401.04088)
- DeepSeek-AI, 2024. *DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models.* [arxiv:2401.06066](https://arxiv.org/abs/2401.06066)
- DeepSeek-AI, 2024. *DeepSeek-V3 Technical Report.* [arxiv:2412.19437](https://arxiv.org/abs/2412.19437)
