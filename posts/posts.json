[
  {
    "title":   "混合专家模型 MoE：从直觉到数学",
    "slug":    "mixture-of-experts",
    "date":    "2026-02-26",
    "tags":    ["LLM", "MoE", "架构", "深度学习"],
    "summary": "从分诊台比喻到稀疏门控数学推导，从 1991 年的起源到 DeepSeek-V3 的 671B 实践，系统梳理混合专家模型的原理、关键论文与工程挑战。"
  }
]
