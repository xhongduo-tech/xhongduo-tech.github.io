## Embedding 是什么

Embedding 的核心思想：**把离散的语言符号映射为连续向量空间中的点**。语义相近的词在空间中距离近，语义关系体现为向量运算。

2013 年，Word2Vec 从大规模文本中自动学出的向量空间中，涌现出了如下关系：

$$\vec{v}(\text{国王}) - \vec{v}(\text{男人}) + \vec{v}(\text{女人}) \approx \vec{v}(\text{王后})$$

性别、职位等语义关系被编码进了向量的方向和距离。这不是手工设计的规则，而是模型从词的上下文分布中自动习得的几何结构。

从 2013 年的 Word2Vec 到今天能理解跨语言语义的大模型 Embedding，这条进化路线串联了过去十年 NLP 最关键的几次突破。

---

## 为什么需要 Embedding

### 词袋模型的根本缺陷

Embedding 出现之前，NLP 主要靠**词袋模型（Bag of Words）**和 **TF-IDF**：每个词用一个 one-hot 向量表示。词汇表有 10 万个词，"猫"就是第 1000 维为 1、其余全为 0 的十万维向量。

两个根本问题：

- **稀疏**。向量中 99.999% 的维度是 0，存储和计算极度浪费。
- **无语义结构**。"猫"和"狗"的余弦相似度是 0，"猫"和"核武器"的余弦相似度也是 0——模型无法区分这两对词之间的语义差距。

这意味着搜索引擎如果用 TF-IDF，查"笔记本电脑推荐"时无法匹配到"laptop 选购攻略"。这不是分词问题，是表示方法的根本缺陷。

Embedding 的目标：让语义相近的词在向量空间中彼此相近。

---

## Word2Vec：第一次让词语住进向量空间

### 核心思想：用上下文定义词义

Word2Vec（2013，Mikolov 等）的出发点是一个语言学假设：

> 一个词的含义由它的上下文决定。

Word2Vec 把这个假设转化为训练任务，有两种变体：

**Skip-gram**：给定中心词，预测上下文词。给定"银行"，预测附近出现的"储蓄""贷款""利率"。

**CBOW（Continuous Bag of Words）**：给定上下文词，预测中心词。给定"储蓄""贷款""利率"，预测"银行"。

### 模型结构

```
输入层（one-hot）→ 隐藏层（Embedding 矩阵 W）→ 输出层（softmax）
```

本质是一个两层神经网络。训练完成后，**Embedding 矩阵 W** 就是目标词向量——隐藏层的权重记录了每个词的语义信息。

### 负采样：让训练可行

原始 softmax 需要对整个词汇表（10 万词）归一化，计算量极大。Word2Vec 引入了**负采样（Negative Sampling）**：

对于每个正样本对（"银行"-"储蓄"），随机采样 $K$ 个负样本（"银行"-"苹果"、"银行"-"海豚"等）。训练目标变成：

$$\mathcal{L} = \log \sigma(\vec{w}_c \cdot \vec{w}_t) + \sum_{k=1}^{K} \log \sigma(-\vec{w}_c \cdot \vec{w}_{n_k})$$

其中 $\sigma$ 是 sigmoid 函数，$\vec{w}_c$ 是中心词向量，$\vec{w}_t$ 是正样本上下文词向量，$\vec{w}_{n_k}$ 是第 $k$ 个负样本向量。

目标：让正样本对的点积大（相似），负样本对的点积小（不相似）。共享上下文的词被推到向量空间中相近的位置，类比关系（如国王-王后）是这个几何结构的自发涌现。

同期的 **GloVe**（2014，Pennington 等）不用局部窗口预测，而是直接对全局词共现矩阵做分解，得到的向量在类比任务上略优于 Word2Vec，核心思想一脉相承。

---

## 静态 Embedding 的天花板：一词一向量

Word2Vec 训练完成后，每个词只有**一个固定向量**，不随上下文变化。

"bank"在英文中至少有两个意思：金融机构和河岸。Word2Vec 给它的是同一个向量——两种语境下向量的某种平均。这就是**多义词问题（Polysemy）**。

更深层地说：静态 Embedding 没有语境感知。"我很好"和"这很好吃"中的"好"映射到完全相同的向量，但下游任务需要的是"这个词在这个语境下的意思"，而非所有语境的平均。

---

## BERT：让每个词的向量因语境而变

### ELMo 的过渡

BERT 之前，ELMo（2018，Peters 等）率先提出**上下文化词向量（Contextualized Word Embeddings）**：用双向 LSTM 处理整句话，每个词的 Embedding 基于整句上下文动态生成。

"bank"在"river bank"和"bank account"中会得到不同的向量。但 LSTM 有两个限制：计算无法并行（必须按顺序处理），且长距离依赖建模能力有限。

### BERT 的架构

BERT（2018，Devlin 等）用 Transformer 的多头自注意力机制替代 LSTM：

- **双向注意力**：每个词直接关注整句中所有其他词，没有从左到右的顺序限制
- **掩码语言模型（MLM）**：随机遮掩 15% 的 token，让模型预测被遮掩的词
- **大规模无标签预训练**：在 Wikipedia + BookCorpus 上预训练，再针对下游任务微调

BERT 的输出是每个词的**上下文化向量**：

```
输入："他去了银行取钱"
输出："银行"的向量 → 金融机构语义方向

输入："坐在银行边上钓鱼"
输出："银行"的向量 → 河岸语义方向
```

同一个词，不同语境，不同向量。多义词问题从根本上解决。

### BERT 做句子相似度的局限

BERT 理解单句能力强，但不擅长直接比较两句话的语义相似度。

用 BERT 做句子相似度的两种朴素方案都有问题：

- 用 `[CLS]` 向量代表句子——但 `[CLS]` 的训练目标（下一句预测 NSP）不是语义相似度，效果差
- 把两句话拼接后送入 BERT——$N$ 个句子两两比较需要 $O(N^2)$ 次推理，不实用

解决这个问题需要专门为**句子级语义相似度**设计的训练方法。

---

## Sentence-BERT：双编码器架构

### 架构设计

Sentence-BERT（SBERT，2019，Reimers & Gurevych）的核心是**双编码器（Bi-Encoder）**结构：

```
句子 A → BERT → 池化 → 向量 a ──┐
                                  → 余弦相似度
句子 B → BERT → 池化 → 向量 b ──┘
```

两个句子分别独立编码，得到固定维度的向量，相似度用余弦距离计算。

$N$ 个文档只需 $N$ 次前向传播，预先存好所有向量。查询时直接计算余弦相似度，结合 ANN（近似最近邻）索引，检索复杂度从 $O(N^2)$ 降到接近 $O(\log N)$。千万级文档可实现毫秒级检索。

### 双编码器 vs 交叉编码器

| | 双编码器（Bi-Encoder） | 交叉编码器（Cross-Encoder）|
|--|--|--|
| **架构** | 两句独立编码 | 两句拼接后联合编码 |
| **精度** | 较低（词级别信息交互受限）| 高（词级别交互充分）|
| **速度** | 极快（可预计算向量）| 慢（每对句子都要推理）|
| **适用场景** | 大规模检索、RAG 召回 | 精排、小规模重排序 |

实际系统中通常先用双编码器从百万文档中召回 Top-100，再用交叉编码器对这 100 个结果精排。两者配合，在精度和效率之间取得最优平衡。

---

## 对比学习的训练机制

现代 Embedding 模型的核心训练范式是**对比学习（Contrastive Learning）**。

### 基本设置

给定一个 batch，包含 $N$ 个训练样本。每个样本包含：

- $x_i$：锚点（anchor）
- $x_i^+$：正样本（语义相近，如同一段话的改写、问题与对应答案）
- $x_i^-$：负样本（语义不相关）

### InfoNCE 损失

$$\mathcal{L}_i = -\log \frac{e^{\,\text{sim}(h_i,\, h_i^+)\, /\, \tau}}{\displaystyle\sum_{j=1}^{N} e^{\,\text{sim}(h_i,\, h_j^+)\, /\, \tau}}$$

其中：

- $h_i = f(x_i)$ 是锚点经过编码器的输出向量
- $h_i^+$ 是正样本的向量
- $\text{sim}(\cdot, \cdot)$ 是余弦相似度
- $\tau$ 是温度参数（temperature）
- 分母中其他 $N-1$ 个正样本自动成为当前样本的 **in-batch 负样本**

本质上是一个 $N$ 分类问题：模型从 batch 中的 $N$ 个候选中识别出正样本。

### 温度参数 $\tau$ 的作用

| $\tau$ 大 | $\tau$ 小 |
|-----------|-----------|
| 相似度分布平滑，对负样本惩罚温和 | 相似度分布尖锐，强迫模型区分细微差异 |
| 训练稳定，但表示可分性差 | 表示判别力强，但训练容易不稳定 |

温度参数控制模型分辨的粒度。典型取值：$\tau \in [0.05,\ 0.1]$。

### Hard Negative：负样本质量决定上限

随机负样本（"猫"和"量子纠缠"）太容易区分，模型几乎无需学习就能分对。**Hard Negative（难负样本）**是语义上接近但实际不相关的样本：

- "iPhone 15 购买渠道"和"iPhone 15 使用评测"——同一产品，用户意图不同
- "Python 列表的 append 方法"和"Python 列表的 extend 方法"——相关但不等价

Hard Negative 的质量是现代 Embedding 模型训练的核心竞争力。BGE、E5 等主流模型都有专门的难负样本挖掘策略：先用弱模型检索出 Top-K 结果，把排名高但实际错误的结果作为难负样本。

### SimCSE：无监督对比学习

SimCSE（2021，Gao 等）提出了一个无监督方案：将**同一句话过两次 dropout** 作为正样本对。

Transformer 中的 dropout 每次随机屏蔽不同的神经元，让同一句话产生两个略有不同的向量表示。这两个表示互为正样本，batch 中的其他句子是负样本。

无监督 SimCSE 超越了很多依赖人工标注的方法，在 STS 基准上达到 SOTA。其成功揭示了一个关键发现：**正样本的多样性（Augmentation）比标注数量更重要**。

---

## 现代 Embedding 的两大创新

### Matryoshka Representation Learning（MRL）

传统 Embedding 的维度是固定的（如 1536 维）。**MRL（2022，Kusupati 等）**的思想：训练时同时优化多个前缀维度，让前 $d$ 维（$d \in \{32, 64, 128, 256, 512, 1536\}$）本身就是一个有效的 Embedding。

```
完整向量（1536 维）：[0.21, -0.54, 0.83, ..., 0.31]
                      └── 前 64 维  ─── 粗粒度语义
                      └── 前 256 维 ─── 中粒度语义
                      └── 全 1536 维 ── 完整语义
```

每一层截断都是完整的、可用的表示。

实际意义：

- 召回阶段用 64 维粗筛（速度快、存储省），精排阶段用 1536 维（精度高）
- 按需截断，在精度和成本之间灵活权衡，不需要为不同需求训练多个模型

OpenAI 的 `text-embedding-3` 系列已原生支持 MRL——API 调用时直接指定输出维度，截断后的向量仍然有效。

### 指令微调 Embedding

E5（2022，Wang 等）发现：**在查询前加任务描述能显著提升检索效果**。

```
检索模式：
  查询：Instruct: 找与用户问题语义相关的段落。\nQuery: 怎么治疗感冒？
  文档：感冒的常见治疗方法包括……

分类模式：
  查询：Instruct: 判断句子的情感倾向。\nQuery: 这部电影太棒了！
```

模型通过任务描述区分"检索任务"和"分类任务"，相应调整向量空间的组织方式。

后续的 E5-mistral-7B（Microsoft）、GTE-Qwen2-7B（阿里）等将这个思路推向极致：用大参数量 LLM 作为编码器底座，配合精细的任务指令和多阶段训练，在 MTEB 多任务基准上取得全面提升。

---

## 评测与模型选择

### MTEB：统一评测基准

**Massive Text Embedding Benchmark（MTEB，2022）**包含 56 个数据集、8 类任务，是目前 Embedding 模型最权威的排行榜。

8 类任务覆盖：检索（Retrieval）、重排序（Reranking）、分类（Classification）、聚类（Clustering）、文本相似度（STS）、摘要（Summarization）、双语挖掘（Bitext Mining）、配对分类（PairClassification）。

MTEB 要求模型在多种任务上都表现良好，单靠一个数据集刷榜无意义。

### 主要模型格局（截至 2025 年底）

| 模型 | 机构 | 维度 | 多语言 | 亮点 |
|------|------|------|--------|------|
| `text-embedding-3-large` | OpenAI | 3072 | 是 | MRL 支持，API 直接可用 |
| `BGE-M3` | BAAI | 1024 | 是（100+ 语言）| 稠密 + 稀疏 + 多向量三模式 |
| `E5-mistral-7b-instruct` | Microsoft | 4096 | 是 | Mistral 7B 底座，指令微调 |
| `GTE-Qwen2-7B-instruct` | 阿里 | 3584 | 是 | Qwen2 底座，中文表现强 |
| `voyage-3` | Voyage AI | 1024 | 是 | 支持 32k 长上下文 |
| `Nomic Embed v2` | Nomic | 768 | 是 | 完全开源，支持 MRL |

**中文场景**：BGE 系列（BAAI）和 GTE 系列（阿里）在中文 MTEB 上持续领先，是中文 RAG 的首选。

**代码检索**：`voyage-code-3` 和 `text-embedding-3-large` 在代码语义搜索上表现更好。

### BGE-M3 的三模式设计

BGE-M3 同时支持三种检索模式：

```
BGE-M3 输出
├── 稠密向量（Dense）── 传统语义向量，余弦相似度检索
├── 稀疏向量（Sparse）─ 类 BM25 的词频权重，关键词精确匹配
└── 多向量（ColBERT）─ 每个 token 都有向量，查询时做细粒度交互
```

三种模式可融合使用：专业术语需要精确匹配（稀疏），语义模糊查询需要语义理解（稠密），复杂长文档需要精细交互（ColBERT）。在专业领域知识库检索中，融合方案比任何单一模式都强。

---

## 实际使用指南

### 维度的权衡

| 维度 | 相对存储 | 典型适用场景 |
|------|---------|--------------|
| 128 ~ 256 | 1x | 亿级别向量库的粗筛层 |
| 512 ~ 768 | 3~4x | 中等规模，平衡精度与成本 |
| 1024 ~ 1536 | 6~8x | 高精度检索（百万级别）|
| 3072 ~ 4096 | 16~20x | 极高精度，小规模场景 |

支持 MRL 的模型可以用低维向量粗筛、高维向量精排。

### 场景选择

| 场景 | 推荐方案 |
|------|---------|
| 中文 RAG / 知识库检索 | BGE-M3 或 GTE-Qwen2-7B |
| 多语言全球化产品 | text-embedding-3-large 或 BGE-M3 |
| 代码语义搜索 | voyage-code-3 |
| 成本敏感，精度要求适中 | text-embedding-3-small |
| 完全本地化部署 | BGE-M3 或 Nomic Embed v2 |

### Embedding 在 RAG 中的位置

```
用户问题
  ↓ Embedding 编码
查询向量 → ANN 索引 → Top-K 候选文档
                           ↓（可选：Cross-Encoder 重排）
                       精排结果 → LLM 生成回答
```

Embedding 模型是 RAG 系统的**召回地基**。召回质量的上限由 Embedding 决定——LLM 再强，也无法从未被召回的文档中生成正确答案。选好 Embedding 模型比调整 Prompt 模板对 RAG 效果的影响更根本。

---

## 总结

| 阶段 | 代表技术 | 核心突破 |
|------|---------|---------|
| 静态词向量 | Word2Vec、GloVe | 用上下文分布学出语义几何 |
| 上下文化 | ELMo、BERT | 同词在不同语境下有不同向量 |
| 句子表示 | SBERT、SimCSE | 对比学习训练出可直接比较的句子向量 |
| 大模型底座 | E5-mistral、GTE-Qwen2 | 指令微调 + 大参数量，多任务泛化 |
| 工程优化 | MRL、BGE-M3 | 可变维度截断、多路检索模式融合 |

Embedding 的进化本质上在回答同一个问题：**如何把语言压缩成向量，同时保留尽可能多的语义信息**。每一次突破——从静态到上下文化，从词级到句子级，从单一稠密向量到多模式混合检索——都是在这个"压缩与保留"的权衡上向前推进一步。

---

*参考资料*

- Word2Vec：[Mikolov et al., 2013](https://arxiv.org/abs/1301.3781)
- GloVe：[Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf)
- ELMo：[Peters et al., 2018](https://arxiv.org/abs/1802.05365)
- BERT：[Devlin et al., 2018](https://arxiv.org/abs/1810.04805)
- SBERT：[Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084)
- SimCSE：[Gao et al., 2021](https://arxiv.org/abs/2104.08821)
- MRL：[Kusupati et al., 2022](https://arxiv.org/abs/2205.13147)
- MTEB：[Muennighoff et al., 2022](https://arxiv.org/abs/2210.07316)
- BGE-M3：[Chen et al., 2024](https://arxiv.org/abs/2402.03216)
