## 一个向量，代表什么？

2013 年，Google 研究员发现了一件奇怪的事：

$$\vec{v}(\text{国王}) - \vec{v}(\text{男人}) + \vec{v}(\text{女人}) \approx \vec{v}(\text{王后})$$

这不是手工设计的规则。这是 Word2Vec 从大规模文本中自动学出来的向量空间，自发涌现出了性别、职位等语义关系。词与词之间的语义关系，被编码进了向量的方向和距离里。

这就是 Embedding 的核心思想：**把离散的语言符号，映射成连续向量空间中的点**。语义相近的词靠在一起，语义关系体现为向量运算。

从 2013 年的 Word2Vec，到今天能理解跨语言语义的现代大模型 Embedding，这条进化路线串联起了过去十年 NLP 最关键的几次突破。

---

## 为什么需要 Embedding？

### 词袋模型的困境

Embedding 出现之前，NLP 主要靠**词袋模型（Bag of Words）**和 **TF-IDF**：每个词用一个 one-hot 向量表示。词汇表有 10 万个词，「猫」就是第 1000 维是 1、其余全是 0 的十万维向量。

两个根本问题：

**问题一：稀疏**。向量中 99.999% 的维度是 0，存储和计算极度浪费。

**问题二：无语义结构**。「猫」和「狗」的余弦相似度是 0，「猫」和「核武器」的余弦相似度也是 0——模型完全不知道这两对词之间的语义差距。

这不是小问题。搜索引擎如果用 TF-IDF，查「笔记本电脑推荐」时，因为文档里写的是「laptop 选购攻略」，就会检索不到。这不是分词问题，是表示方法的根本缺陷。

Embedding 的目标：让语义相近的词，在向量空间里也彼此相近。

---

## Word2Vec：第一次让语言住进向量空间

### 核心思想：用上下文定义词义

Word2Vec（2013，Mikolov 等）的出发点是一个语言学直觉：

> **一个词的含义，由它的上下文决定。**

「银行」在「去银行取钱」和「坐在河银行边上」里是完全不同的意思。但不管哪个意思，「银行」周围的词（取钱、河、边）都在给它「定义」。

Word2Vec 把这个直觉转化为训练任务，有两种变体：

**Skip-gram**：给定中心词，预测上下文词。给定「银行」，预测附近出现的「储蓄」「贷款」「利率」。

**CBOW（Continuous Bag of Words）**：给定上下文词，预测中心词。给定「储蓄」「贷款」「利率」，预测「银行」。

### 模型结构

```
输入层（one-hot）→ 隐藏层（Embedding 矩阵 W）→ 输出层（softmax）
```

本质是一个两层神经网络。训练完成后，**Embedding 矩阵 W 本身**就是我们要的词向量——隐藏层的权重记录了每个词的语义信息。

### 负采样：让训练变得可行

原始 softmax 要对整个词汇表（10 万词）归一化，计算量极大。Word2Vec 引入了**负采样（Negative Sampling）**：

对于每个正样本对（「银行」-「储蓄」），随机采样 $K$ 个负样本（「银行」-「苹果」、「银行」-「海豚」……）。训练目标变成：

$$\mathcal{L} = \log \sigma(\vec{w}_c \cdot \vec{w}_t) + \sum_{k=1}^{K} \log \sigma(-\vec{w}_c \cdot \vec{w}_{n_k})$$

其中 $\sigma$ 是 sigmoid 函数，$\vec{w}_c$ 是中心词向量，$\vec{w}_t$ 是正样本上下文词向量，$\vec{w}_{n_k}$ 是第 $k$ 个负样本向量。

目标很直接：让正样本对的点积大（相似），负样本对的点积小（不相似）。

相同上下文出现的词被推到向量空间里相近的位置，而 $\vec{v}(\text{国王}) - \vec{v}(\text{男人}) + \vec{v}(\text{女人}) \approx \vec{v}(\text{王后})$ 这类类比关系，就是这个几何结构自发涌现的结果。

同期还有 **GloVe**（2014，Pennington 等）：不用局部窗口预测，而是直接对全局词共现矩阵做分解，得到的向量在类比任务上略优于 Word2Vec，但核心思想一脉相承。

---

## 静态 Embedding 的天花板：一词一向量

Word2Vec 训练完之后，每个词只有**一个固定向量**，不管上下文如何变化。

「bank」在英文里至少有两个意思：金融机构和河岸。Word2Vec 给它的是同一个向量——是两种语境下向量的某种平均，两头都不讨好。

这就是多义词问题（Polysemy）。更深层地：**没有语境感知**。「我很好」和「这很好吃」里的「好」，Word2Vec 会映射到完全相同的向量——但下游任务往往需要的是「这个词在这个语境下的意思」，而非「这个词所有语境的平均意思」。

这个问题，直到 2018 年才被根本解决。

---

## BERT：让每个词的向量因语境而变

### ELMo 的过渡

BERT 之前，ELMo（2018，Peters 等）率先提出**上下文化词向量**：用双向 LSTM 处理整句话，每个词的 Embedding 基于整句上下文动态生成。

「bank」在「river bank」和「bank account」里会得到不同的向量。但 LSTM 有两个问题：计算无法并行（必须按顺序处理），且长距离依赖建模能力有限。

### BERT 的架构

BERT（2018，Devlin 等）用 Transformer 的多头自注意力机制替代 LSTM：

- **双向注意力**：每个词直接看整句话里所有其他词，没有「从左到右」的顺序限制
- **掩码语言模型（MLM）**：随机遮掩 15% 的 token，让模型预测被遮掩的词
- **大规模无标签预训练**：在 Wikipedia + BookCorpus 上预训练，再针对下游任务微调

BERT 的输出是每个词的**上下文化向量**：

```
输入：「他去了银行取钱」
输出：「银行」的向量 → 金融机构语义方向

输入：「坐在银行边上钓鱼」
输出：「银行」的向量 → 河岸语义方向
```

同一个词，不同语境，不同向量。多义词问题从根本上解决。

### BERT 的盲点

BERT 理解单句很强，但有一个重要局限：**它不擅长直接比较两句话的语义相似度**。

用 BERT 做句子相似度有两种朴素方案：

- 用 `[CLS]` 的向量代表句子——但 `[CLS]` 的训练目标（下一句预测 NSP）不是语义相似度，效果差
- 把两句话拼接后送入 BERT——推理时每对句子都需要一次前向传播，$N$ 个句子两两比较需要 $O(N^2)$ 次推理，完全不实用

解决这个问题，需要专门为**句子级语义相似度**设计的训练方法。

---

## Sentence-BERT：把 BERT 改造成语义搜索引擎

### 双编码器架构

Sentence-BERT（SBERT，2019，Reimers & Gurevych）的核心是**双编码器（Bi-Encoder）**结构：

```
句子 A → BERT → 池化 → 向量 a ──┐
                                  → 余弦相似度
句子 B → BERT → 池化 → 向量 b ──┘
```

两个句子分别**独立编码**，得到固定维度的向量，相似度用余弦距离计算。

这样，$N$ 个文档只需做 $N$ 次前向传播，预先存好所有向量。查询时直接计算余弦相似度，结合 ANN（近似最近邻）索引，检索复杂度从 $O(N^2)$ 降到接近 $O(\log N)$。千万级文档，毫秒级检索。

### 双编码器 vs 交叉编码器

| | 双编码器（Bi-Encoder） | 交叉编码器（Cross-Encoder）|
|--|--|--|
| **架构** | 两句独立编码 | 两句拼接后联合编码 |
| **精度** | 较低（词级别信息交互受限）| 高（词级别交互充分）|
| **速度** | 极快（可预计算向量）| 慢（每对句子都要推理）|
| **适用场景** | 大规模检索、RAG 召回 | 精排、小规模重排序 |

实际系统里，通常**先用双编码器**从百万文档里召回 Top-100，**再用交叉编码器**对这 100 个结果精排。两者配合，在精度和效率之间取得最优平衡。

---

## 数学深挖：对比学习怎么训练的？

现代 Embedding 模型的核心训练范式是**对比学习（Contrastive Learning）**。

### 基本设置

给定一个 batch，包含 $N$ 个训练样本。每个样本包含：

- $x_i$：锚点（anchor）
- $x_i^+$：正样本（语义相近，如同一段话的改写、问题与对应答案）
- $x_i^-$：负样本（语义不相关）

### InfoNCE 损失

$$\mathcal{L}_i = -\log \frac{e^{\,\text{sim}(h_i,\, h_i^+)\, /\, \tau}}{\displaystyle\sum_{j=1}^{N} e^{\,\text{sim}(h_i,\, h_j^+)\, /\, \tau}}$$

其中：

- $h_i = f(x_i)$ 是锚点经过编码器的输出向量
- $h_i^+$ 是正样本的向量
- $\text{sim}(\cdot, \cdot)$ 是余弦相似度
- $\tau$ 是温度参数（temperature）
- 分母里，其他 $N-1$ 个正样本自动成为当前样本的 **in-batch 负样本**

直觉上：这是一个 $N$ 分类问题，模型要从 batch 里的 $N$ 个候选中挑出「哪个是正样本」。

### 温度参数 $\tau$ 的作用

| $\tau$ 大 | $\tau$ 小 |
|-----------|-----------|
| 相似度分布平滑，对负样本惩罚温和 | 相似度分布尖锐，强迫模型区分细微差异 |
| 训练稳定，但表示可分性差 | 表示判别力强，但训练容易不稳定 |

温度参数本质上控制的是「我要求模型分辨的粒度」。典型取值：$\tau \in [0.05,\ 0.1]$。

### Hard Negative：负样本质量决定上限

随机负样本（「猫」和「量子纠缠」）太容易区分，模型几乎不用学什么就能分对。**Hard Negative（难负样本）** 是语义上看起来相近、但实际不相关的样本。

举例：
- 「iPhone 15 购买渠道」和「iPhone 15 使用评测」——涉及同一产品，但用户意图完全不同
- 「Python 列表的 append 方法」和「Python 列表的 extend 方法」——相关但不等价

Hard Negative 的质量是现代 Embedding 模型训练的核心竞争力。BGE、E5 等主流模型都有专门的难负样本挖掘策略：先用弱模型检索出 Top-K 结果，把排名高但实际错误的结果作为难负样本。

### SimCSE：不需要标注数据的对比学习

SimCSE（2021，Gao 等）提出了一个优雅的无监督方案：把**同一句话过两次 dropout** 作为正样本对。

Transformer 中的 dropout 每次会随机屏蔽不同的神经元，让同一句话产生两个略有不同的向量表示——这两个表示互为正样本，batch 里的其他句子是负样本。

结果出人意料的好：无监督 SimCSE 超越了很多依赖人工标注的方法，在 STS 基准上达到 SOTA。它的成功揭示了一个关键洞察：**正样本的多样性（Augmentation）比标注数量更重要**。

---

## 现代 Embedding 的两大创新

### Matryoshka Representation Learning（MRL）

传统 Embedding 的维度是固定的（如 1536 维）。**MRL（2022，Kusupati 等）** 的思想是：训练时同时优化多个前缀维度，让前 $d$ 维（$d \in \{32, 64, 128, 256, 512, 1536\}$）本身就是一个有效的 Embedding。

```
完整向量（1536 维）：[0.21, -0.54, 0.83, ..., 0.31]
                      └── 前 64 维  ─── 粗粒度语义
                      └── 前 256 维 ─── 中粒度语义
                      └── 全 1536 维 ── 完整语义
```

类比俄罗斯套娃（Matryoshka）：每一层截断都是完整的、可用的表示，不是残缺品。

**实际意义**：

- 召回阶段用 64 维粗筛（速度快、存储省），精排阶段用 1536 维（精度高）
- 按需截断，在精度和成本之间灵活权衡，不需要为不同需求训练多个模型

OpenAI 的 `text-embedding-3` 系列已原生支持 MRL——API 调用时直接指定输出维度，截断后的向量仍然有效。

### 指令微调 Embedding

E5（2022，Wang 等）发现：**在查询前加一句任务描述，能显著提升检索效果**。

```
检索模式：
  查询：Instruct: 找与用户问题语义相关的段落。\nQuery: 怎么治疗感冒？
  文档：感冒的常见治疗方法包括……

分类模式：
  查询：Instruct: 判断句子的情感倾向。\nQuery: 这部电影太棒了！
```

模型知道「这是检索任务」还是「这是分类任务」，会相应调整向量空间的组织方式。

后续的 E5-mistral-7B（Microsoft）、GTE-Qwen2-7B（阿里）等把这个思路推向极致：用大参数量 LLM 作为编码器底座，配合精细的任务指令和多阶段训练，在 MTEB 多任务基准上取得全面提升。

---

## 现在最好的模型在哪里？

### MTEB：统一评测基准

Massive Text Embedding Benchmark（MTEB，2022）包含 56 个数据集、8 类任务，是目前 Embedding 模型最权威的排行榜。

8 类任务覆盖：检索（Retrieval）、重排序（Reranking）、分类（Classification）、聚类（Clustering）、文本相似度（STS）、摘要（Summarization）、双语挖掘（Bitext Mining）、配对分类（PairClassification）。

单靠一个数据集刷榜没有意义——MTEB 要求模型在多种任务上都表现良好，才能得高分。

### 主要模型格局（截至 2025 年底）

| 模型 | 机构 | 维度 | 多语言 | 亮点 |
|------|------|------|--------|------|
| `text-embedding-3-large` | OpenAI | 3072 | 是 | MRL 支持，API 直接可用 |
| `BGE-M3` | BAAI | 1024 | 是（100+ 语言）| 稠密 + 稀疏 + 多向量三模式 |
| `E5-mistral-7b-instruct` | Microsoft | 4096 | 是 | Mistral 7B 底座，指令微调 |
| `GTE-Qwen2-7B-instruct` | 阿里 | 3584 | 是 | Qwen2 底座，中文表现强 |
| `voyage-3` | Voyage AI | 1024 | 是 | 支持 32k 长上下文 |
| `Nomic Embed v2` | Nomic | 768 | 是 | 完全开源，支持 MRL |

**中文场景**：BGE 系列（BAAI）和 GTE 系列（阿里）在中文 MTEB 上持续领先，是中文 RAG 的首选。

**代码检索**：`voyage-code-3` 和 `text-embedding-3-large` 在代码语义搜索上表现更好。

### BGE-M3 的三模式设计

BGE-M3 值得单独说一下。它同时支持三种检索模式：

```
BGE-M3 输出
├── 稠密向量（Dense）── 传统语义向量，余弦相似度检索
├── 稀疏向量（Sparse）─ 类 BM25 的词频权重，关键词精确匹配
└── 多向量（ColBERT）─ 每个 token 都有向量，查询时做细粒度交互
```

三种模式可融合使用：对专业术语需要精确匹配（稀疏），对语义模糊查询需要语义理解（稠密），对复杂长文档需要精细交互（ColBERT）。在专业领域知识库检索中，融合方案比任何单一模式都强。

---

## 实际使用时怎么选？

### 维度的权衡

| 维度 | 相对存储 | 典型适用场景 |
|------|---------|--------------|
| 128 ~ 256 | 1x | 亿级别向量库的粗筛层 |
| 512 ~ 768 | 3~4x | 中等规模，平衡精度与成本 |
| 1024 ~ 1536 | 6~8x | 高精度检索（百万级别）|
| 3072 ~ 4096 | 16~20x | 极高精度，小规模场景 |

如果用支持 MRL 的模型，可以用低维向量粗筛、高维向量精排，两全其美。

### 场景选择

| 场景 | 推荐方案 |
|------|---------|
| 中文 RAG / 知识库检索 | BGE-M3 或 GTE-Qwen2-7B |
| 多语言全球化产品 | text-embedding-3-large 或 BGE-M3 |
| 代码语义搜索 | voyage-code-3 |
| 成本敏感，精度要求适中 | text-embedding-3-small |
| 完全本地化部署 | BGE-M3 或 Nomic Embed v2 |

### Embedding 在 RAG 里的位置

最后梳理一下 Embedding 在 RAG（检索增强生成）系统里的角色：

```
用户问题
  ↓ Embedding 编码
查询向量 → ANN 索引 → Top-K 候选文档
                           ↓（可选：Cross-Encoder 重排）
                       精排结果 → LLM 生成回答
```

Embedding 模型是整个 RAG 系统的**召回地基**。召回质量的上限，由 Embedding 决定——LLM 再强，也无法从「根本没被召回的文档」里生成正确答案。选好 Embedding 模型，比调整 Prompt 模板对 RAG 效果的影响更根本。

---

## 小结

| 阶段 | 代表技术 | 核心突破 |
|------|---------|---------|
| 静态词向量 | Word2Vec、GloVe | 用上下文分布学出语义几何 |
| 上下文化 | ELMo、BERT | 同词在不同语境下有不同向量 |
| 句子表示 | SBERT、SimCSE | 对比学习训练出可直接比较的句子向量 |
| 大模型底座 | E5-mistral、GTE-Qwen2 | 指令微调 + 大参数量，多任务泛化 |
| 工程优化 | MRL、BGE-M3 | 可变维度截断、多路检索模式融合 |

Embedding 的进化，本质上是在回答同一个问题：**怎么把语言压缩成一个向量，同时保留尽可能多的语义信息**。

每一次突破——从静态到上下文化，从词级到句子级，从单一稠密向量到多模式混合检索——都是在这个「压缩—保留」的权衡上再往前推进一步。

---

*参考资料*

- Word2Vec：[Mikolov et al., 2013](https://arxiv.org/abs/1301.3781)
- GloVe：[Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf)
- ELMo：[Peters et al., 2018](https://arxiv.org/abs/1802.05365)
- BERT：[Devlin et al., 2018](https://arxiv.org/abs/1810.04805)
- SBERT：[Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084)
- SimCSE：[Gao et al., 2021](https://arxiv.org/abs/2104.08821)
- MRL：[Kusupati et al., 2022](https://arxiv.org/abs/2205.13147)
- MTEB：[Muennighoff et al., 2022](https://arxiv.org/abs/2210.07316)
- BGE-M3：[Chen et al., 2024](https://arxiv.org/abs/2402.03216)
