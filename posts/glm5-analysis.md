## GLM-5 概览

2026 年 2 月 11 日，智谱 AI 发布 GLM-5，核心事实：

- 744B 参数 MoE 架构，**完全在华为昇腾芯片上训练**，不依赖英伟达 GPU
- MIT 协议开源，权重可直接下载
- SWE-bench Verified 达到 77.8%，超过 Gemini 3.0 Pro
- 幻觉率比上一代降低 56%，通过新的强化学习基础设施实现

本文拆解 GLM-5 的关键技术：GLM 架构设计、Slime 异步 RL 框架、三阶段 Agentic RL 训练流水线，以及这些技术如何具体驱动性能提升。

---

## GLM 架构：不同于 GPT 的第三条路线

### 两种主流设计

目前主流大模型有两种底层路线：

**GPT 路线（纯自回归）**：从左到右逐词预测下一个词。训练时模型只能往左看。

```
输入："今天天气
预测：很" → "好" → "，" → "适合" → "出门"
```

优点：生成流畅，逻辑连贯。局限：单向理解，对需要双向语境的任务（如歧义词消解）不够强。

**BERT 路线（双向理解）**：看到整句话，随机遮掩部分词让模型预测。

```
输入："今天天气[MASK]好，适合出门"
预测：[MASK] = "很"
```

优点：双向理解，语言理解任务强。局限：只能做填词，不擅长生成长文本。

### GLM 的自回归空白填充

GLM（General Language Model，2021）将两种范式合并。训练任务称为 **Autoregressive Blank Infilling**（自回归空白填充）：

1. 从文本中随机抠掉几段连续的词（可以是 1 个词，也可以是一大段）
2. 被抠掉的位置打上 `[MASK]` 标记
3. 模型以自回归方式逐词重建被遮掩的内容

```
原句："今天天气很好，适合出门散步"

抠掉两段 → "今天[MASK1]，[MASK2]散步"

重建 MASK1："天气很好"（逐词生成：天→气→很→好）
重建 MASK2："适合出门"（逐词生成：适→合→出→门）
```

关键细节：重建时模型同时能看到左边的上下文（"今天"）和右边的上下文（"，散步"），但生成被遮掩内容时是从左到右自回归的。

GLM 用生成的方式做理解，用理解的方式辅助生成。一个模型在理解、条件生成、无条件生成三类任务上都表现良好，不需要针对不同任务训练不同模型。

---

## GLM-5 技术规格

### 模型规模

| 指标 | GLM-5 | GLM-4.5（上一代）|
|------|-------|---------|
| 总参数量 | **744B** | 355B |
| 推理时激活参数 | **40B** | 32B |
| 上下文窗口 | **205K tokens** | 128K tokens |
| 最大输出长度 | **128K tokens** | — |
| 预训练数据量 | **28.5T tokens** | 23T tokens |

GLM-5 采用 **MoE（混合专家模型）**架构：共 256 个专家，每次推理只激活其中 8 个（3.1%）。总参数 744B，但推理时实际运算的只有 40B——在保持大模型知识广度的同时把计算成本控制在中等规模模型的水平。MoE 架构的详细解释见本博客的 [MoE 专题文章](/post?slug=mixture-of-experts)。

### 关键数字的含义

**205K context window**：相当于一次对话能放入约 15 万个中文字。一部完整的中篇小说或一个大型代码库的核心文件可以全部送进上下文一起处理。

**28.5T tokens 预训练数据**：GPT-3 用了约 0.3T tokens，GLM-5 的训练数据是 GPT-3 的近 100 倍，覆盖中文、英文及另外 24 种语言。

---

## 三个核心技术突破

GLM-5 的提升不只是参数量增长，有三个具体的技术创新驱动了这次升级。

### 突破一：Slime 异步强化学习框架

Slime 是 GLM-5 最关键的工程创新，也是幻觉率降低 56% 的直接原因。

**传统 RL 训练 LLM 的瓶颈**：

强化学习训练流程为：模型生成回答 → 评判回答质量 → 根据反馈更新参数 → 循环。三步必须串行：

```
生成回答（慢，占总时间约 90%）
      ↓ 等待
评估质量
      ↓ 等待
更新参数
      ↓ 等待
再次生成……
```

生成阶段非常慢，GPU 利用率低。

**Slime 的解法：解耦 + 异步**

Slime 把三个步骤完全解耦：

```
生成工作组（多个）──→ 轨迹缓冲区 ──→ 训练工作组
   持续生成             持续写入          持续消费
   不等训练             不等任何人        不等生成
```

- 多个生成工作组同时生成，将训练样本放入共享缓冲区
- 训练工作组从缓冲区取数据，持续更新参数
- 两边互不等待，GPU 始终在运算

Slime 还集成了 **APRIL（Active Partial Rollouts）**技术：不等完整回答生成完毕才开始评估，在生成过程中就开始处理已完成的部分。

结果：GPU 利用率接近 100%，同等算力下能完成更多 RL 训练步骤。

### 突破二：三阶段 Agentic RL

GLM-5 的后训练分三个阶段的强化学习流水线：

```
阶段一：Reasoning RL   → 数学推理、逻辑链条
      ↓ 跨阶段蒸馏（防止遗忘）
阶段二：Agentic RL     → 自主任务规划、工具使用
      ↓ 跨阶段蒸馏
阶段三：General RL     → 综合能力打磨
```

Agentic RL 阶段是关键：智谱 AI 构建了超过 **10,000 个可验证的训练场景**，包括 GitHub issue 修复、终端操作、搜索与推理等。

"可验证"是重点。与人类主观评分不同，这些场景有客观的对错标准——例如"这段代码能否通过测试"，要么通过要么不通过，强化学习信号更干净。

**跨阶段蒸馏（Cross-Stage Distillation）**解决了一个经典难题：先训好的阶段一能力在训练阶段二时容易退化。智谱 AI 用在线蒸馏机制，让前一阶段的模型持续给当前训练提供软标签，防止已学会的能力被遗忘。

### 突破三：稀疏注意力（DSA）

GLM-5 采用 **DeepSeek Sparse Attention（DSA）**机制处理超长序列。

标准自注意力的计算复杂度为 $O(n^2)$，序列从 128K 增长到 205K 时计算量增长 2.5 倍。

DSA 的核心思想：不是每个 token 都需要与所有其他 token 交互。大多数情况下一个 token 只需要关注邻近的词和全局的关键词，远处的大多数词可以省略。

```
标准注意力：每个词看所有词（205K × 205K = 420亿次交互）
稀疏注意力：每个词只看局部窗口 + 少量全局锚点（大幅减少）
```

实际效果：205K 长序列的计算成本降低约 1.5 到 2 倍，性能基本保持不变。这让 205K 的超长上下文在实际部署中可行。

---

## 性能基准数据

### 核心基准

| 基准测试 | GLM-5 | 说明 |
|---------|-------|------|
| **MATH-500** | 98.0% | 数学竞赛题，500 道 |
| **AIME 2025** | 84.0% | 美国数学邀请赛，高难度 |
| **MMLU** | 92.0% | 57 学科综合知识 |
| **GPQA** | — | 博士级科学问题，表现突出 |
| **HumanEval** | 90.0% | 代码生成正确率 |
| **SWE-bench Verified** | **77.8%** | 真实 GitHub issue 修复 |
| **Terminal Bench 2.0** | 56.2% | 终端操作任务 |

### SWE-bench 的意义

SWE-bench Verified 是目前最贴近真实软件工程能力的代码基准：从 GitHub 上真实的 Python 项目中抽取 bug 报告，让模型自主修复，用原始测试套件验证是否通过。

GLM-5 的 77.8% 超过 Gemini 3.0 Pro（76.2%），处于所有公开测试模型的领先位置。

大多数代码基准（如 HumanEval）考的是写一个独立函数。SWE-bench 考的是在真实代码库中定位 bug 并修复——需要理解上下文、跨文件分析、修改后不破坏其他测试。难度差距显著。

### 竞品横向对比

| 模型 | MATH-500 | SWE-bench | 开源 | 参数量 |
|------|---------|-----------|------|--------|
| **GLM-5** | 98.0% | 77.8% | MIT | 744B（40B 激活）|
| Gemini 3.0 Pro | — | 76.2% | 否 | 未公开 |
| DeepSeek-R1 | 97.3% | — | Apache 2.0 | 671B（37B 激活）|
| GPT-4o | — | — | 否 | 未公开 |

---

## 华为昇腾芯片训练的技术意义

GLM-5 的**整个训练过程完全在华为昇腾（Ascend）芯片上完成**，框架使用昇思 MindSpore（华为自研）。未使用英伟达 A100 或 H100，未使用 PyTorch。

技术层面的含义：

- 昇腾芯片的软件生态与 CUDA 完全不兼容，工程团队需要从零实现所有底层算子
- GLM-5 的 RL 训练基础设施（Slime）也需要在昇腾上重新适配

这一点在技术报告中有明确记录，不是营销宣传。

部署方面，GLM-5 支持英伟达 GPU、华为昇腾、沐曦、寒武纪等多种芯片，并针对各芯片做了量化和算子优化。

---

## GLM-Z1：推理专用模型

GLM 家族中还有一个专为复杂推理设计的模型：**GLM-Z1-32B**（2024 年 4 月发布）。

规格：32B 参数，训练方式为冷启动（Cold-start）+ 延伸强化学习，引入**成对评判奖励（Pairwise Critic Rewards）**。

| 基准 | GLM-Z1-32B | DeepSeek-R1（671B，37B 激活）|
|------|-----------|--------------------------|
| AIME 24/25 | 接近 | — |
| LiveCodeBench | 接近 | — |
| GPQA | 接近 | — |
| **推理成本** | **最多便宜 5 倍** | 基准 |

32B 模型的推理效果接近 671B 的 DeepSeek-R1，推理成本降低 5 倍。

背后的逻辑：**模型做推理时真正需要的参数量远少于存储知识所需的参数量**。GLM-Z1 通过专门的推理强化学习，把小模型的推理能力推到大模型级别。

---

## 使用方式

### 网页使用

[z.ai](https://www.z.ai/) 注册即可使用，支持标准聊天和 Agent 模式（自主任务执行）。国内用户可访问 [zhipuai.cn](https://www.zhipuai.cn/)。

### API 调用

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-zhipu-api-key",
    base_url="https://open.bigmodel.cn/api/paas/v4/"
)

response = client.chat.completions.create(
    model="glm-5",
    messages=[
        {"role": "user", "content": "分析一下快速排序在最坏情况下的时间复杂度"}
    ]
)
print(response.choices[0].message.content)
```

定价（截至 2026 年 2 月）：

| | GLM-5 | Claude Opus 4.6（参考）|
|--|--|--|
| 输入 | $1.00 / 百万 tokens | $5.00 / 百万 tokens |
| 输出 | $3.20 / 百万 tokens | $25.00 / 百万 tokens |

GLM-5 约为 Claude Opus 级别的 1/5 到 1/8 价格。

### 本地部署

GLM-5 以 MIT 协议开源，可从 Hugging Face 下载权重：

```bash
# 需要约 1.5TB 存储空间（744B fp16 权重）
# 推荐使用 vLLM 或 SGLang 做推理
pip install vllm
```

> 本地部署门槛较高（744B 模型需要大量 GPU 显存），更适合有基础设施的团队。个人用户建议直接用 API。

---

## 总结

GLM-5 的技术要点：

**规模**：744B MoE 参数，40B 激活，205K 上下文，28.5T 训练数据。

**训练**：Slime 异步 RL 框架使 GPU 利用率接近 100%；三阶段 Agentic RL 加 10,000+ 可验证场景使幻觉率降低 56%。

**独立性**：全程在华为昇腾芯片上训练，MIT 开源。

对开发者而言，GLM-5 目前最值得关注的场景是**代码与软件工程任务**——SWE-bench 77.8% 是可验证的数字，API 定价明显低于同档位的闭源模型。

---

*参考资料*

- GLM-5 技术报告：[arXiv 2602.15763](https://arxiv.org/abs/2602.15763)
- GLM 原始论文（Blank Infilling）：[arXiv 2103.10360](https://arxiv.org/abs/2103.10360)
- Slime RL 框架：[github.com/THUDM/slime](https://github.com/THUDM/slime)
- 模型权重（MIT 开源）：[huggingface.co/zai-org/GLM-5](https://huggingface.co/zai-org/GLM-5)
- 智谱 AI 开发者平台：[bigmodel.cn](https://bigmodel.cn/)
