## GLM-5 为什么值得关注？

2026 年 2 月 11 日，智谱 AI 发布了 GLM-5。这次发布有几件事同时发生，放在一起看格外耐人寻味：

- **完全用华为昇腾芯片训练**，不依赖任何英伟达 GPU
- **MIT 开源**，744B 参数的权重可以直接下载
- **SWE-bench Verified 达到 77.8%**，超过 Gemini 3.0 Pro，代码能力进入全球第一梯队
- **幻觉率比上一代降低 56%**，靠一套新的强化学习基础设施实现

这不是一篇「某国产大模型又宣称超 GPT-4」的新闻。这篇文章想做的是：拆开 GLM-5 的技术盖子，看看里面究竟是什么让它变好了。

---

## 先搞懂 GLM 架构：为什么不是 GPT？

在讲 GLM-5 的技术之前，需要先理解 GLM 这个架构和 GPT 的根本区别。

### 两种主流路线

目前主流大模型有两种底层设计思路：

**GPT 路线（纯自回归）**：从左到右，一个词一个词预测下一个词。训练时，模型只能「往左看」——用已经生成的内容预测接下来的词。

```
输入：「今天天气
预测：很」→「好」→「，」→「适合」→「出门」
```

优点：生成流畅，逻辑连贯。缺点：只能单向理解，对需要双向语境的任务（如判断歧义词）不够强。

**BERT 路线（双向理解）**：看到整句话，理解每个词在上下文里的含义。训练时随机遮掩部分词，让模型预测被遮掩的词——也就是「完形填空」。

```
输入：「今天天气[MASK]好，适合出门」
预测：[MASK] = 「很」
```

优点：双向理解，对语言理解任务很强。缺点：只能做「填词」，不擅长生成长文本。

### GLM 的第三条路：自回归完形填空

GLM（General Language Model，2021 年提出）的核心创新是把两种范式合并了。

训练任务叫做 **Autoregressive Blank Infilling**（自回归空白填充）：

1. 从文本里随机抠掉几段连续的词（可以是 1 个词，也可以是一大段）
2. 被抠掉的位置打上 `[MASK]` 标记
3. 让模型**自回归地**重建被抠掉的内容——也就是一个词一个词按顺序生成被遮掩的部分

```
原句：「今天天气很好，适合出门散步」

抠掉两段 → 「今天[MASK1]，[MASK2]散步」

重建 MASK1：「天气很好」（逐词生成：天→气→很→好）
重建 MASK2：「适合出门」（逐词生成：适→合→出→门）
```

关键细节：重建时，模型**同时能看到左边的上下文（「今天」）和右边的上下文（「，散步」）**，但生成被遮掩的内容时是从左到右自回归的。

一句话总结：**GLM 用生成的方式做理解，用理解的方式辅助生成**。既不像 GPT 只往左看，也不像 BERT 不能生成长文本。这让一个模型在理解、条件生成、无条件生成三类任务上都表现良好，不需要针对不同任务训练不同模型。

---

## GLM-5 的技术规格

理解了 GLM 架构，再看 GLM-5 的具体参数：

### 规模

| 指标 | GLM-5 | GLM-4.5（上一代）|
|------|-------|---------|
| 总参数量 | **744B** | 355B |
| 推理时激活参数 | **40B** | 32B |
| 上下文窗口 | **205K tokens** | 128K tokens |
| 最大输出长度 | **128K tokens** | — |
| 预训练数据量 | **28.5T tokens** | 23T tokens |

GLM-5 是 **MoE（混合专家模型）** 架构：共 256 个专家，每次推理只激活其中 8 个（3.1%）。总参数 744B 听起来很大，但推理时实际运算的只有 40B——在保持大模型知识广度的同时，把计算成本控制在中等规模模型的水平。如果你对 MoE 不熟悉，可以参考本博客的 [MoE 专题文章](/post?slug=mixture-of-experts)。

### 数字背后的意义

**205K context window**：相当于一次对话能放进去约 15 万个中文字——一部完整的中篇小说，或者一个大型代码库的核心文件，可以全部送进上下文一起处理。

**28.5T tokens 预训练数据**：GPT-3 当年用了约 0.3T tokens。GLM-5 的训练数据是 GPT-3 的近 100 倍，覆盖中文、英文及另外 24 种语言。

---

## 为什么更好？三个核心突破

GLM-5 比上一代好，不只是「参数更多了」。有三个具体的技术创新驱动了这次提升。

### 突破一：Slime——异步强化学习框架

这是 GLM-5 最值得关注的工程创新，也是幻觉率降低 56% 的直接原因。

**传统强化学习训练 LLM 的瓶颈**：

强化学习（RL）训练的流程是：模型生成回答 → 评判回答好不好 → 根据反馈更新参数 → 循环。

问题在于：这三步**必须串行**。

```
生成回答（慢，占总时间约 90%）
      ↓ 等待
评估质量
      ↓ 等待
更新参数
      ↓ 等待
再次生成……
```

模型生成文本（Inference）非常慢，每次生成时 GPU 在等，更新参数时又在等生成——GPU 利用率很低。

**Slime 的解法：解耦 + 异步**

Slime（智谱 AI 开源的 RL 训练框架）把三个步骤**完全解耦**：

```
生成工作组（多个）──→ 轨迹缓冲区 ──→ 训练工作组
   持续生成             持续写入          持续消费
   不等训练             不等任何人        不等生成
```

- 多个「生成工作组」同时生成，把生成的训练样本放进共享缓冲区
- 「训练工作组」从缓冲区里取数据，持续更新参数
- 两边互不等待，GPU 始终在干活

Slime 还集成了 **APRIL（Active Partial Rollouts）** 技术：不等一个完整回答生成完毕才开始评估，而是在生成过程中就开始处理已完成的部分。

结果：GPU 利用率接近 100%，同等算力下能完成更多 RL 训练步骤，最终把幻觉率降低了 56%。

### 突破二：三阶段 Agentic RL

GLM-5 的后训练分三个阶段的强化学习流水线：

```
阶段一：Reasoning RL   → 数学推理、逻辑链条
      ↓ 跨阶段蒸馏（防止遗忘）
阶段二：Agentic RL     → 自主任务规划、工具使用
      ↓ 跨阶段蒸馏
阶段三：General RL     → 综合能力打磨
```

**Agentic RL 是关键**：为了让模型学会像「代理」一样自主完成任务，智谱 AI 构建了超过 **10,000 个可验证的训练场景**，包括 GitHub issue 修复、终端操作、搜索与推理等。

可验证是重点。和「人类给个评分」不同，这些场景有客观的对错标准——比如「这段代码能不能通过测试」，要么通过要么不通过，没有主观模糊性，强化学习信号更干净。

**跨阶段蒸馏（Cross-Stage Distillation）**：解决了一个经典难题——先训好的「阶段一能力」在训练阶段二时容易退化。智谱 AI 用一个在线蒸馏机制，让前一阶段的模型持续给当前训练提供「软标签」，防止已学会的能力被遗忘。

### 突破三：稀疏注意力（DSA）

GLM-5 采用了 **DeepSeek Sparse Attention（DSA）** 机制处理超长序列。

标准自注意力的计算复杂度是 $O(n^2)$，其中 $n$ 是序列长度。序列从 128K 增长到 205K，计算量增长了 2.5 倍。

DSA 的核心思想：**不是每个 token 都需要跟所有其他 token 交互**。大多数时候，一个 token 只需要关注「邻近的词」和「全局的关键词」，远处的大多数词可以省略。

```
标准注意力：每个词看所有词（205K × 205K = 420亿次交互）
稀疏注意力：每个词只看局部窗口 + 少量全局锚点（大幅减少）
```

实际效果：205K 长序列的计算成本降低了约 **1.5 到 2 倍**，同时性能基本保持不变。这让 205K 的超长上下文在实际部署中变得可行，而不只是技术指标。

---

## 性能实测

### 核心基准数据

| 基准测试 | GLM-5 | 说明 |
|---------|-------|------|
| **MATH-500** | 98.0% | 数学竞赛题，500 道 |
| **AIME 2025** | 84.0% | 美国数学邀请赛，高难度 |
| **MMLU** | 92.0% | 57 学科综合知识 |
| **GPQA** | — | 博士级科学问题，表现突出 |
| **HumanEval** | 90.0% | 代码生成正确率 |
| **SWE-bench Verified** | **77.8%** | 真实 GitHub issue 修复 |
| **Terminal Bench 2.0** | 56.2% | 终端操作任务 |

### SWE-bench 值得单独说

SWE-bench Verified 是目前最贴近「真实软件工程能力」的代码基准：从 GitHub 上真实的 Python 项目中抽取 bug 报告，让模型自主修复，用原始测试套件验证是否通过。

GLM-5 的 77.8% 超过了 Gemini 3.0 Pro（76.2%），在所有公开测试的模型中处于领先位置。

**为什么这个数字有意义**：大多数代码基准（如 HumanEval）考的是「写一个独立函数」，而 SWE-bench 考的是「在真实代码库里定位 bug 并修复」——需要理解上下文、跨文件分析、修改后不破坏其他测试。这比写单个函数难得多。

### 与竞品横向对比

| 模型 | MATH-500 | SWE-bench | 开源 | 参数量 |
|------|---------|-----------|------|--------|
| **GLM-5** | 98.0% | 77.8% | MIT | 744B（40B 激活）|
| Gemini 3.0 Pro | — | 76.2% | 否 | 未公开 |
| DeepSeek-R1 | 97.3% | — | Apache 2.0 | 671B（37B 激活）|
| GPT-4o | — | — | 否 | 未公开 |

---

## 用华为芯片训练：战略意义

GLM-5 的另一个重要事实：**整个训练过程完全在华为昇腾（Ascend）芯片上完成**，框架使用昇思 MindSpore（华为自研）。

没有用英伟达 A100 或 H100，没有用 PyTorch。

这在技术上的意义：

- 昇腾芯片的软件生态和 CUDA 完全不兼容，工程团队需要从零实现所有底层算子
- GLM-5 的大量 RL 训练基础设施（Slime）也需要在昇腾上重新适配

智谱 AI 在论文中明确记录了这一点，不是营销说法，是技术报告里写明的训练基础设施细节。

GLM-5 发布后，**部署**支持英伟达 GPU、华为昇腾、沐曦、寒武纪等多种芯片，并针对各芯片做了量化和算子优化。

---

## GLM-Z1：推理专家

GLM 家族里还有一个专门为复杂推理设计的模型：**GLM-Z1-32B**（2024 年 4 月发布）。

规格：
- **32B 参数**（精密而不庞大）
- 训练方式：冷启动（Cold-start）+ 延伸强化学习，引入「成对评判奖励」（Pairwise Critic Rewards）

成绩：

| 基准 | GLM-Z1-32B | DeepSeek-R1（671B，37B 激活）|
|------|-----------|--------------------------|
| AIME 24/25 | 接近 | — |
| LiveCodeBench | 接近 | — |
| GPQA | 接近 | — |
| **推理成本** | **最多便宜 5 倍** | 基准 |

32B 的模型，推理效果接近 671B 的 DeepSeek-R1，推理成本降低 5 倍。

这背后的逻辑：**模型做推理时真正需要的参数量，远少于「模型知道多少知识」所需要的参数量**。GLM-Z1 通过专门的推理强化学习，把小模型的推理能力推到了大模型级别。

---

## 怎么用 GLM-5

### 网页直接使用

打开 [z.ai](https://www.z.ai/)，注册账号即可开始对话，支持标准聊天模式和 Agent 模式（自主任务执行）。

国内用户可以访问 [zhipuai.cn](https://www.zhipuai.cn/)，界面和功能基本相同，使用 Doubao-1.5-Pro 等国内备案模型。

### API 调用

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-zhipu-api-key",
    base_url="https://open.bigmodel.cn/api/paas/v4/"
)

response = client.chat.completions.create(
    model="glm-5",
    messages=[
        {"role": "user", "content": "分析一下快速排序在最坏情况下的时间复杂度"}
    ]
)
print(response.choices[0].message.content)
```

**定价**（截至 2026 年 2 月）：
- 输入：$1.00 / 百万 tokens
- 输出：$3.20 / 百万 tokens

相比之下 Claude Opus 4.6 的输入 $5 / 百万、输出 $25 / 百万——GLM-5 大约是 Claude Opus 级别能力的 1/5 到 1/8 价格。

### 本地部署

GLM-5 以 MIT 协议开源，可以从 Hugging Face 下载权重：

```bash
# 需要约 1.5TB 存储空间（744B fp16 权重）
# 推荐使用 vLLM 或 SGLang 做推理
pip install vllm
```

本地部署的门槛较高（744B 模型需要大量 GPU 显存），更适合有基础设施的团队。个人用户建议直接用 API。

---

## 小结

GLM-5 的「为什么好」，可以用三句话概括：

**规模**：744B MoE 参数，40B 激活，205K 上下文，28.5T 训练数据——把量堆到了目前开源模型的前列。

**训练**：Slime 异步 RL 框架让 GPU 利用率接近 100%，三阶段 Agentic RL 加 10,000+ 可验证场景，幻觉率比上一代降低 56%——量变之外还有质变。

**独立性**：全程在华为昇腾芯片上训练完成，MIT 开源。

对于开发者，GLM-5 目前最值得关注的场景是**代码与软件工程任务**——SWE-bench 77.8% 是实打实的数字，API 定价又明显低于同档位的闭源模型。

---

*参考资料*

- GLM-5 技术报告：[arXiv 2602.15763](https://arxiv.org/abs/2602.15763)
- GLM 原始论文（Blank Infilling）：[arXiv 2103.10360](https://arxiv.org/abs/2103.10360)
- Slime RL 框架：[github.com/THUDM/slime](https://github.com/THUDM/slime)
- 模型权重（MIT 开源）：[huggingface.co/zai-org/GLM-5](https://huggingface.co/zai-org/GLM-5)
- 智谱 AI 开发者平台：[bigmodel.cn](https://bigmodel.cn/)
