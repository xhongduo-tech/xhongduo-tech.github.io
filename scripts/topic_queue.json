[
  {"id":1,"title":"向量与矩阵：神经网络计算的数学基础","slug":"vectors-and-matrices-ml","tags":["底层原理","数学基础","线性代数"],"brief":"神经网络的每一层都是矩阵乘法加非线性变换。讲清向量的内积、L2 范数、余弦相似度；矩阵乘法的维度规则（m×n 乘 n×p 得 m×p）；转置、逆矩阵的计算意义；以及为什么 GPU 擅长矩阵运算（SIMD 并行）。","depth_hint":"包含维度变换的具体数值示例，用 NumPy 代码验证每个操作","status":"pending"},
  {"id":2,"title":"梯度与链式法则：反向传播的数学基础","slug":"gradient-chain-rule","tags":["底层原理","数学基础","微积分"],"brief":"从单变量导数推广到多变量偏导数和梯度向量。链式法则 dz/dx = dz/dy × dy/dx 是反向传播的核心。讲清方向导数、梯度下降的几何意义（梯度指向最陡上升方向），以及计算图中每条边对应一次链式法则应用。","depth_hint":"手推一个三层网络的链式求导过程，包含具体数值","status":"pending"},
  {"id":3,"title":"概率论基础：从随机变量到最大似然估计","slug":"probability-basics-mle","tags":["底层原理","数学基础","概率论"],"brief":"随机变量、概率分布（伯努利、高斯、多项分布）、期望与方差的定义。贝叶斯定理 P(θ|X) ∝ P(X|θ)P(θ)。最大似然估计（MLE）：为什么交叉熵损失等价于对数似然最大化——这是分类任务损失函数选择的理论基础。","depth_hint":"推导高斯分布的 MLE 得到样本均值和方差，推导分类交叉熵的 MLE 来源","status":"pending"},
  {"id":4,"title":"信息熵与交叉熵：损失函数的信息论基础","slug":"entropy-cross-entropy","tags":["底层原理","数学基础","信息论"],"brief":"香农熵 H(p) = -Σ p(x) log p(x) 衡量分布的不确定性。交叉熵 H(p,q) = -Σ p(x) log q(x) 衡量用 q 编码真实分布 p 的额外代价。KL 散度 KL(p||q) = H(p,q) - H(p)，非对称性及其含义。为什么分类任务用交叉熵而非 MSE：梯度行为的对比分析。","depth_hint":"包含二分类交叉熵和 MSE 在 sigmoid 输出上的梯度对比计算","status":"pending"},
  {"id":5,"title":"Softmax 函数：多分类输出的概率化","slug":"softmax-function","tags":["底层原理","数学基础","激活函数"],"brief":"Softmax(z_i) = exp(z_i) / Σ exp(z_j)，将任意实数向量映射为概率分布。数值稳定性问题：exp 溢出与 log-sum-exp 技巧。温度参数 τ：Softmax(z/τ) 在 τ→0 时趋向 argmax，τ→∞ 时趋向均匀分布。在注意力机制中 τ=√d_k 的作用。","depth_hint":"展示 exp 溢出的数值示例，实现数值稳定版 Softmax，对比不同温度下的输出分布","status":"pending"},
  {"id":6,"title":"感知机到多层感知机：深度学习的起点","slug":"perceptron-to-mlp","tags":["底层原理","神经网络","MLP"],"brief":"1958 年 Rosenblatt 感知机：y = sign(w·x + b)，线性可分的几何意义。XOR 问题证明单层感知机的局限。MLP 通过隐藏层引入非线性，Universal Approximation Theorem：两层 MLP 可以以任意精度逼近任意连续函数（但不说明需要多少神经元）。前向传播的矩阵形式。","depth_hint":"手工实现一个解决 XOR 问题的两层 MLP，展示权重矩阵维度","status":"pending"},

  {"id":7,"title":"激活函数全景：Sigmoid、ReLU、GELU 的数学性质与选择","slug":"activation-functions","tags":["底层原理","神经网络","激活函数"],"brief":"Sigmoid σ(x) = 1/(1+e^{-x})：饱和区梯度 <0.25，10 层后梯度缩小 100 万倍。ReLU max(0,x)：梯度直通，稀疏激活，但 Dead ReLU 问题（负区梯度恒零）。GELU x·Φ(x)（Φ 为标准正态 CDF）：平滑近似 ReLU，BERT/GPT 的默认选择。SwiGLU = Swish(xW) ⊙ (xV)：LLaMA 系列的 FFN 激活。","depth_hint":"绘制每种激活函数的函数值与导数曲线，分析梯度消失的具体数值","status":"pending"},
  {"id":8,"title":"批归一化（BatchNorm）：内部协变量偏移的定义与解法","slug":"batch-normalization","tags":["底层原理","训练优化","归一化"],"brief":"内部协变量偏移（Internal Covariate Shift）：每层输入分布随参数更新而变化，导致后层需要不断适应。BatchNorm 对 mini-batch 内每个特征维度做 z-score 归一化，再用可学习的 γ、β 缩放偏移。推理时用训练集的运行均值/方差（不是 batch 统计量）。局限：小 batch size 时不稳定，RNN/Transformer 中不适用。","depth_hint":"展示 BN 的前向与反向传播公式，分析为什么 batch size=1 时 BN 退化","status":"pending"},
  {"id":9,"title":"梯度下降三形态：SGD、Mini-batch、Full-batch 的收敛分析","slug":"gradient-descent-variants","tags":["底层原理","训练优化","优化器"],"brief":"Full-batch GD：精确梯度，但内存无法放下全部数据，且鞍点难以逃脱。SGD：高方差梯度带来随机性，有助于逃脱局部极小，但收敛震荡。Mini-batch：权衡两者，batch size 影响梯度噪声与计算效率。线性缩放规则：batch size ×k 时学习率 ×k（适用范围与限制）。","depth_hint":"对比三种形态在凸/非凸函数上的收敛轨迹可视化描述，给出线性缩放规则的证明","status":"pending"},
  {"id":10,"title":"Adam 优化器：一阶矩与二阶矩的联合自适应","slug":"adam-optimizer","tags":["底层原理","训练优化","优化器"],"brief":"Adam = Momentum + RMSProp。m_t = β_1 m_{t-1} + (1-β_1)g_t（一阶矩，方向）；v_t = β_2 v_{t-1} + (1-β_2)g_t²（二阶矩，幅度）；偏差修正 m̂_t = m_t/(1-β_1^t)；更新 θ -= α·m̂_t/(√v̂_t + ε)。默认超参 β_1=0.9, β_2=0.999, ε=1e-8 的选择依据。AdamW：将权重衰减从梯度更新中解耦的必要性。","depth_hint":"推导偏差修正项的来源（初始化为零导致的冷启动偏差），对比 Adam 与 SGD+Momentum 的参数更新轨迹","status":"pending"},
  {"id":11,"title":"权重初始化：Xavier 与 He 方案的方差推导","slug":"weight-initialization","tags":["底层原理","训练优化","初始化"],"brief":"全零初始化的对称性问题：所有神经元计算相同梯度，层永远等价。随机初始化需要控制方差：太大→激活值爆炸，太小→梯度消失。Xavier 初始化 Var(W) = 2/(n_in + n_out) 适用于 Tanh（对称激活）。He 初始化 Var(W) = 2/n_in 适用于 ReLU（正半轴激活，有效扇入减半）。","depth_hint":"从信号方差在前向传播中的保持条件出发，推导 Xavier 公式","status":"pending"},
  {"id":12,"title":"学习率调度：Warmup、余弦退火与 OneCycleLR","slug":"learning-rate-schedule","tags":["底层原理","训练优化","学习率"],"brief":"固定学习率的问题：初期过大导致不稳定，后期过大无法收敛到极小。Warmup 阶段：前 N 步线性增大学习率，让 Adam 的二阶矩估计稳定后再走大步。余弦退火：lr(t) = lr_min + 0.5(lr_max-lr_min)(1+cos(πt/T))。Warmup + 余弦退火是 Transformer 训练的标准配置，Transformer 原始论文的 lr = d_model^{-0.5} · min(step^{-0.5}, step·warmup^{-1.5})。","depth_hint":"实现 Transformer 原始论文的学习率公式，绘制训练步数-学习率曲线","status":"pending"},

  {"id":13,"title":"词袋模型与 TF-IDF：统计语义的局限","slug":"bag-of-words-tfidf","tags":["底层原理","文本表示","NLP基础"],"brief":"词袋模型将文本表示为词频向量，丢失词序和语义关系。TF-IDF = TF(t,d) × IDF(t) = (词频/文档总词数) × log(总文档数/含该词文档数)，降低高频无信息词（the/的）的权重。维度诅咒：10 万词汇表的稀疏向量在余弦相似度计算上的效率问题。这些局限直接驱动了 Word2Vec 的诞生。","depth_hint":"计算一个具体文档集的 TF-IDF 矩阵，分析稀疏性问题","status":"pending"},
  {"id":14,"title":"Word2Vec CBOW：从上下文预测中心词","slug":"word2vec-cbow","tags":["底层原理","文本表示","Embedding"],"brief":"CBOW 模型：给定窗口内的上下文词，预测中心词。架构：上下文词 one-hot → 嵌入层（取均值）→ 线性层 → Softmax → 预测中心词。目标函数：最大化 log P(w_t | w_{t-c:t+c})。嵌入矩阵 W（V×d）是参数，训练完后每行是一个词向量。窗口大小的影响：小窗口捕捉句法关系，大窗口捕捉语义关系。","depth_hint":"推导 CBOW 的反向传播，分析嵌入层参数更新的稀疏性","status":"pending"},
  {"id":15,"title":"Word2Vec Skip-gram：负采样的数学原理","slug":"word2vec-skipgram-negative-sampling","tags":["底层原理","文本表示","Embedding"],"brief":"Skip-gram：给定中心词，预测上下文词。原始 Softmax 对整个词表计算分母，V=10万时代价极高。Negative Sampling 近似：对每个正样本采样 k 个负样本（按词频^(3/4)的分布采样），将多类分类转为 k+1 个二分类。目标函数：log σ(v_c·v_o) + Σ_k E[log σ(-v_c·v_k)]。k=5-20 在实践中与全 Softmax 效果接近。","depth_hint":"推导负采样目标函数的梯度，解释词频^(3/4)采样分布的设计原理","status":"pending"},
  {"id":16,"title":"BPE 分词：字节对编码的压缩理论基础","slug":"bpe-tokenization","tags":["底层原理","分词","Tokenization"],"brief":"Byte Pair Encoding 原是数据压缩算法，被 Sennrich 2016 引入 NLP。算法：从字符级词表出发，反复合并出现频率最高的相邻符号对，直到达到目标词表大小。优势：有效处理 OOV（Out-of-Vocabulary），在词表大小与覆盖率之间取得平衡。GPT-2 使用字节级 BPE（Byte-level BPE），将 UTF-8 字节作为基本单元，词表永远不会有 OOV。","depth_hint":"逐步演示 BPE 在一个小语料上的合并过程，分析 vocab_size=50k vs 100k 的覆盖率差异","status":"pending"},
  {"id":17,"title":"RNN 与梯度消失：BPTT 中的指数衰减","slug":"rnn-gradient-vanishing","tags":["底层原理","序列模型","RNN"],"brief":"RNN 隐状态 h_t = tanh(W_h h_{t-1} + W_x x_t)，梯度需经 T 步反向传播（BPTT）。∂h_t/∂h_k = Π_{i=k+1}^{t} ∂h_i/∂h_{i-1}，每步乘一次 W_h^T 和激活函数导数。tanh 导数最大 1，W_h 的谱范数若 <1 则梯度消失，>1 则梯度爆炸。100 步序列上的梯度幅值变化：|λ|^100 的数值分析。","depth_hint":"计算谱范数为 0.9 时 100 步序列的梯度幅值，用代码验证 BPTT 的指数衰减","status":"pending"},
  {"id":18,"title":"LSTM：遗忘门、输入门、输出门的参数方程","slug":"lstm-gates","tags":["底层原理","序列模型","LSTM"],"brief":"LSTM 引入细胞状态 C_t 作为信息高速公路。四个门：遗忘门 f_t=σ(W_f·[h_{t-1},x_t]+b_f)，输入门 i_t，候选细胞 C̃_t，输出门 o_t。关键：C_t = f_t⊙C_{t-1} + i_t⊙C̃_t，梯度可以通过加法直接反向传播（恒定误差传送带）。与 RNN 的参数量对比：LSTM 参数量是 RNN 的 4 倍。","depth_hint":"完整写出 LSTM 的前向传播方程，推导细胞状态梯度不消失的条件","status":"pending"},

  {"id":19,"title":"Seq2Seq 架构：编码器-解码器与信息瓶颈","slug":"seq2seq-encoder-decoder","tags":["底层原理","序列模型","Seq2Seq"],"brief":"机器翻译的 Seq2Seq：编码器将源序列压缩为固定维度上下文向量 c，解码器以 c 为初始状态自回归生成目标序列。信息瓶颈问题：无论源序列长度如何，c 的维度固定，长序列信息必然损失。实验数据：BLEU 分数随句子长度增加而下降的趋势。这直接驱动了注意力机制的发明。","depth_hint":"分析不同源序列长度下信息压缩率的变化，展示长序列翻译质量下降的实验数据","status":"pending"},
  {"id":20,"title":"Bahdanau 注意力：对齐分数的加性计算","slug":"bahdanau-attention","tags":["底层原理","注意力机制","Attention"],"brief":"Bahdanau 2015：不再用单一上下文向量，而是为每个解码步 t 计算动态上下文 c_t = Σ α_{t,i} h_i。对齐分数 e_{t,i} = v^T tanh(W_1 s_{t-1} + W_2 h_i)，softmax 归一化得 α_{t,i}。α 矩阵可视化：英法翻译中源词与目标词的对齐热图。注意力的计算复杂度：O(T_x × T_y)。","depth_hint":"展示注意力权重矩阵的可视化，推导加性注意力的参数量","status":"pending"},
  {"id":21,"title":"Self-Attention：序列内部元素的相互依赖建模","slug":"self-attention-mechanism","tags":["底层原理","注意力机制","Transformer"],"brief":"Self-Attention 用同一序列的元素同时充当查询（Q）、键（K）、值（V）。Q = XW^Q，K = XW^K，V = XW^V。注意力矩阵 A = softmax(QK^T/√d_k)V。与 Seq2Seq 注意力的区别：这里 Q 和 K/V 来自同一序列。√d_k 缩放：d_k 维度下 QK^T 的方差为 d_k，不缩放时 softmax 进入饱和区，梯度趋近于零。","depth_hint":"推导 QK^T 方差为 d_k 的数学过程，用 PyTorch 实现 Self-Attention","status":"pending"},
  {"id":22,"title":"多头注意力：并行子空间的语义分工","slug":"multi-head-attention","tags":["底层原理","注意力机制","Transformer"],"brief":"Multi-Head Attention：将 d_model 维拆分为 h 个 d_k=d_model/h 维的子空间，在每个子空间独立计算注意力。head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)；MHA = Concat(head_1,...,head_h)W^O。不同 head 可以捕捉不同类型的依赖：句法关系、语义关系、共指关系等（基于 Transformer 可解释性研究）。参数量 = 4 × d_model²（Q/K/V/O 四个矩阵）。","depth_hint":"计算 BERT-base（h=12, d_model=768）的 MHA 参数量，展示 head 专化的实验证据","status":"pending"},
  {"id":23,"title":"位置编码：正弦函数方案的数学构造","slug":"sinusoidal-positional-encoding","tags":["底层原理","Transformer","位置编码"],"brief":"Self-Attention 本身是排列等变的（置换输入顺序不影响输出），必须注入位置信息。Transformer 原始论文的方案：PE(pos, 2i) = sin(pos/10000^{2i/d_model})，PE(pos, 2i+1) = cos(...)。设计原理：不同维度对应不同频率，相对位置 PE(pos+k) 可以表示为 PE(pos) 的线性变换（旋转矩阵）。可学习位置编码（BERT）与固定编码（GPT）的比较。","depth_hint":"推导相对位置的线性变换性质，比较不同位置数 seq_len=512 时各维度的编码值","status":"pending"},
  {"id":24,"title":"Transformer Encoder：前馈层与残差连接的作用","slug":"transformer-encoder","tags":["底层原理","Transformer","架构"],"brief":"Transformer Encoder 层 = LayerNorm + MHA + 残差 + LayerNorm + FFN + 残差。FFN = max(0, xW_1+b_1)W_2+b_2，中间维度 d_ff=4×d_model（e.g., 768→3072→768），参数量是 MHA 的两倍。FFN 被认为存储键值对记忆（Geva 2021）。残差连接 x + Sublayer(x) 保证梯度下界为 1，Pre-Norm（先 LN 再 sublayer）比 Post-Norm 训练更稳定。","depth_hint":"对比 Pre-Norm 和 Post-Norm 在梯度流上的差异，计算 BERT-base 的 FFN 参数量","status":"pending"},

  {"id":25,"title":"GPT 架构：Decoder-only 与因果语言建模","slug":"gpt-architecture","tags":["模型解析","GPT","预训练"],"brief":"GPT 使用仅解码器（Decoder-only）架构，每个 token 只能看到自身及之前的 token（因果掩码）。预训练目标：最大化 Σ log P(x_t|x_1,...,x_{t-1})（自回归语言模型）。因果掩码实现：注意力矩阵 QK^T 上三角置为 -∞，softmax 后对应位置权重为 0。GPT-1 → GPT-2 → GPT-3 的规模演进：参数量、数据量、能力涌现（in-context learning）。","depth_hint":"实现因果掩码的代码，分析参数量从 117M（GPT-1）到 175B（GPT-3）的计算量变化","status":"pending"},
  {"id":26,"title":"BERT：掩码语言模型与双向上下文表示","slug":"bert-masked-lm","tags":["模型解析","BERT","预训练"],"brief":"BERT 使用 Encoder-only 架构，预训练任务：MLM（随机遮盖 15% token，其中 80% 替换为 [MASK]，10% 随机词，10% 不变）和 NSP（下句预测，后被证明无用）。双向上下文：每个 token 的表示融合了左右两侧信息。BERT 不能直接用于生成（因为是双向注意力），适合分类、NER、QA 等判别任务。[CLS] token 的表示用于句子级分类。","depth_hint":"分析 MLM 15% 遮盖率的选择依据，对比 BERT 与 GPT 在不同下游任务上的表现差异","status":"pending"},
  {"id":27,"title":"Transformer 的 FLOP 分析：注意力层与 FFN 的计算占比","slug":"transformer-flops-analysis","tags":["底层原理","Transformer","效率分析"],"brief":"一次前向传播的 FLOP 计算：注意力层 QKV 投影 3×2×n×d²，注意力矩阵 2×n²×d，输出投影 2×n×d²；FFN 两次矩阵乘法 2×2×n×d×4d = 16nd²。总 FLOP ≈ 24nd² + 4n²d（n 为序列长度，d 为模型维度）。序列长度 n 较小时 FFN 占主导（n≪d），n 较大时注意力项 4n²d 开始显著。训练 FLOP ≈ 6×参数量×token 数（来自 Kaplan 2020）。","depth_hint":"计算 GPT-3（175B，seq_len=2048，d=12288）单次前向传播的 FLOP","status":"pending"},
  {"id":28,"title":"缩放定律（Scaling Laws）：模型性能与计算量的幂律关系","slug":"scaling-laws","tags":["底层原理","缩放定律","训练"],"brief":"Kaplan 2020：验证集 loss 与模型参数量 N、数据集大小 D、计算量 C 呈幂律关系 L(N) ∝ N^{-α}（α≈0.076）。Chinchilla 2022 修正：Kaplan 低估了数据量的重要性，最优配置是 N_opt ∝ C^{0.5}（计算量等分给模型和数据）。实践含义：训练 70B 模型的最优 token 数约为 1.4T（20:1 比例）。","depth_hint":"推导 Chinchilla 最优分配公式，计算 LLaMA-2 70B 的实际训练配置与 Chinchilla 最优值的对比","status":"pending"},
  {"id":29,"title":"预训练数据：质量过滤与去重的工程流程","slug":"pretraining-data-pipeline","tags":["工程实践","预训练","数据工程"],"brief":"大规模预训练数据的处理流程：Common Crawl 原始抓取 → 语言识别（fastText）→ 规则过滤（最短/最长文本、特殊字符比例）→ 内容过滤（去除 NSFW）→ 去重（URL 级、段落级、文档级）→ 质量过滤（分类器过滤低质量文本）。MinHash LSH 近似去重：k-gram 特征的局部敏感哈希，n²→nk 的复杂度。数据混合比例：The Pile、SlimPajama、FineWeb 的配比策略。","depth_hint":"计算 1T token 数据集中 MinHash 去重的计算代价，对比精确去重与近似去重的效果","status":"pending"},
  {"id":30,"title":"混合精度训练：FP16/BF16 的数值稳定性与梯度缩放","slug":"mixed-precision-training","tags":["工程实践","训练优化","分布式训练"],"brief":"FP32（1+8+23 位）vs FP16（1+5+10 位）vs BF16（1+8+7 位）：BF16 与 FP32 指数位相同，动态范围一致，但精度更低（尾数位只有 7 位）。混合精度策略：前向/后向用 FP16/BF16，梯度更新和优化器状态保留 FP32 master copy。梯度缩放（Gradient Scaling）：将 loss 乘以 scale factor，防止 FP16 梯度下溢（值 <6×10^{-5} 时变为 0）。","depth_hint":"展示不同精度下的数值范围，用代码演示梯度下溢问题及 GradScaler 的解决方案","status":"pending"},

  {"id":31,"title":"KV Cache：推理阶段的空间换时间机制","slug":"kv-cache","tags":["底层原理","推理优化","KV Cache"],"brief":"自回归生成时，每步需要用到所有历史 token 的 K 和 V（前缀不变）。不缓存：每步重新计算所有 K、V，复杂度 O(T²)。KV Cache：缓存历史 K、V 矩阵，每步只计算新 token 的 K、V 并追加，复杂度降为 O(T)。内存代价：batch_size × seq_len × n_layers × n_heads × d_head × 2（K+V）× bytes。以 LLaMA-2 70B 为例：seq_len=4096 时 KV Cache 约 80GB。","depth_hint":"计算 LLaMA-2 70B 在 batch_size=1、seq_len=4096 时 KV Cache 的精确内存占用","status":"pending"},
  {"id":32,"title":"RoPE：旋转位置编码的复数域推导","slug":"rope-positional-encoding","tags":["底层原理","位置编码","RoPE"],"brief":"RoPE（Su 2021）：将位置信息编码为旋转矩阵，作用于 Q 和 K（不作用于 V）。核心性质：<f_q(x_m, m), f_k(x_n, n)> 只依赖于相对位置 m-n，而非绝对位置。实现：将 d 维向量视为 d/2 个复数，每个复数乘以 e^{imθ_i}（θ_i = 1/10000^{2i/d}）。为什么比正弦编码更好：在外推（训练长度之外）上表现更好，且对注意力计算天然友好。","depth_hint":"用复数乘法推导 RoPE 的内积公式，证明内积只依赖相对位置","status":"pending"},
  {"id":33,"title":"Flash Attention：IO 感知的注意力分块算法","slug":"flash-attention","tags":["底层原理","推理优化","Flash Attention"],"brief":"标准注意力的瓶颈：注意力矩阵 S=QK^T（n×n）需要写入 HBM（GPU 高带宽内存），再读回做 softmax，再写回，再读回乘 V——4 次 HBM 读写，内存访问是瓶颈而非计算。Flash Attention（Dao 2022）：Tiling 分块技术，在 SRAM（片上缓存）内完成分块的 softmax 和加权求和，只读写 O(n) HBM。速度提升：A100 上比标准注意力快 2-4 倍，内存占用从 O(n²) 降至 O(n)。","depth_hint":"解释分块 softmax 的 online normalization 算法，计算不同序列长度下的 HBM 读写量对比","status":"pending"},
  {"id":34,"title":"量化基础：INT8/INT4 的误差分析与 GPTQ","slug":"quantization-basics","tags":["工程实践","推理优化","量化"],"brief":"量化：用低精度整数替代浮点权重。对称量化：W_q = round(W / scale)，scale = max(|W|) / 127（INT8）。量化误差来源：四舍五入误差 + 截断误差（超出范围的值）。逐张量 vs 逐通道量化：逐通道 scale 更精确但实现复杂。GPTQ（Frantar 2022）：基于二阶 Hessian 信息的逐层量化，用未量化权重的误差补偿已量化权重，4-bit 精度损失极小。","depth_hint":"推导 GPTQ 的 OBQ（Optimal Brain Quantization）框架，计算量化前后的困惑度变化","status":"pending"},
  {"id":35,"title":"投机采样（Speculative Decoding）：草稿模型加速自回归生成","slug":"speculative-decoding","tags":["底层原理","推理优化","解码"],"brief":"自回归生成的瓶颈：每步只生成 1 个 token，GPU 利用率极低（memory-bound 而非 compute-bound）。Speculative Decoding（Leviathan 2023）：用小模型（draft）快速生成 k 个 token 草稿，大模型（target）并行验证所有草稿。接受概率 α = min(1, p_target/p_draft)。期望加速比：1/(1-α^k) 倍。条件：draft 模型分布与 target 接近（不影响输出分布的证明）。","depth_hint":"推导投机采样的接受概率公式，证明其与 target 模型等价（无偏），计算不同接受率下的加速比","status":"pending"},
  {"id":36,"title":"PagedAttention：KV Cache 的动态内存管理","slug":"paged-attention","tags":["工程实践","推理优化","vLLM"],"brief":"KV Cache 的内存碎片问题：请求长度不确定，预分配导致大量内存浪费（实测 20-40% 碎片率）。PagedAttention（Kwon 2023，vLLM 基础）：借鉴操作系统分页内存管理，将 KV Cache 切分为固定大小 block（如 16 token/block），按需分配。优势：几乎消除内存碎片，支持 prefix sharing（相同前缀的请求共享 KV Cache block）。吞吐量对比：vLLM 比 HuggingFace Transformers 推理吞吐量高 2-4 倍。","depth_hint":"用具体数字说明 padding-based 方法与 PagedAttention 的内存利用率对比","status":"pending"},

  {"id":37,"title":"指令微调（Instruction Tuning）：从预训练到对话模型","slug":"instruction-tuning","tags":["模型解析","对齐","微调"],"brief":"预训练 LLM 只能续写文本，不能执行指令。指令微调（Wei 2022，FLAN）：在 (instruction, output) 对上做有监督微调，让模型学会遵循自然语言指令。数据格式：System prompt + User instruction + Assistant response。关键发现：任务数量比任务内的数据量更重要（FLAN 用 60+ 任务 vs 1 任务）。InstructGPT 的三阶段：SFT → 奖励模型 → PPO，vs 纯 SFT 的 FLAN。","depth_hint":"对比 FLAN 和 InstructGPT 在不同任务上的表现，分析对话格式设计对模型行为的影响","status":"pending"},
  {"id":38,"title":"RLHF 奖励模型：偏好对比学习的训练机制","slug":"rlhf-reward-model","tags":["底层原理","对齐","RLHF"],"brief":"RLHF 第二阶段：训练奖励模型 R(x, y)，输入 (prompt, response) 对，输出标量分数。训练数据：人类标注员对同一 prompt 的两个响应进行偏好比较 (y_w ≻ y_l)。损失函数：-E[log σ(R(x,y_w) - R(x,y_l))]（Bradley-Terry 模型）。奖励模型通常以预训练 LLM 为初始化，替换最后一层为标量输出。奖励黑客（reward hacking）：模型学会欺骗奖励模型，而非真正改善质量。","depth_hint":"推导 Bradley-Terry 偏好模型，分析奖励模型的数据规模对最终对齐质量的影响","status":"pending"},
  {"id":39,"title":"PPO 在 LLM 对齐中的应用：KL 散度约束与 RL 稳定性","slug":"ppo-llm-alignment","tags":["底层原理","对齐","强化学习"],"brief":"RLHF 第三阶段：用 PPO 最大化奖励模型分数，同时用 KL 散度约束偏离 SFT 模型不能太远。目标：max E[R(x,y)] - β·KL(π_θ || π_ref)，β 控制对齐强度与能力保留的权衡。PPO 的 Clip 机制：限制策略更新步长，防止梯度爆炸。LLM 中的挑战：超长序列的 credit assignment，响应级别（非 token 级别）的稀疏奖励。","depth_hint":"给出 LLM RLHF 中 PPO 的实现伪代码，分析 β 系数选择对模型行为的影响","status":"pending"},
  {"id":40,"title":"DPO：绕过奖励模型的直接偏好优化","slug":"direct-preference-optimization","tags":["底层原理","对齐","DPO"],"brief":"DPO（Rafailov 2023）：从 RLHF 的 KL 约束优化目标出发，数学推导表明最优策略可以直接用参考模型表示，无需显式训练奖励模型。损失函数：-E[log σ(β log(π_θ(y_w|x)/π_ref(y_w|x)) - β log(π_θ(y_l|x)/π_ref(y_l|x)))]。优势：训练稳定，无需 PPO 的复杂超参调整。限制：对数据质量敏感，offline 数据分布与模型分布的 gap 会导致训练不稳定。","depth_hint":"推导 DPO 损失函数从 RLHF 最优解的推导过程，对比 DPO 与 PPO 在相同数据上的效果","status":"pending"},
  {"id":41,"title":"LoRA：低秩分解的参数高效微调","slug":"lora-peft","tags":["工程实践","微调","PEFT"],"brief":"LoRA（Hu 2021）：预训练权重 W₀ 冻结，微调时只训练低秩分解 ΔW = BA（B: d×r，A: r×k，r≪min(d,k)）。推理时合并：W = W₀ + BA，无额外推理开销。关键超参：秩 r（通常 4-64），α（缩放系数，实际学习率 = α/r），应用层（QKV 矩阵、FFN 矩阵）。参数效率：BERT-base 全参数 110M，r=8 时 LoRA 只需约 300K 参数（<0.3%）。","depth_hint":"分析 LoRA 秩 r 的选择对下游任务性能的影响，展示 LoRA 权重合并前后推理速度对比","status":"pending"},
  {"id":42,"title":"QLoRA：4-bit 量化与 LoRA 的组合微调","slug":"qlora","tags":["工程实践","微调","PEFT"],"brief":"QLoRA（Dettmers 2023）：用 NF4（Normal Float 4-bit）量化基础模型权重（减少 ~75% 显存），在量化基础上训练 BF16 精度的 LoRA 适配器。技术细节：NF4 是信息论最优的 4-bit 数据类型（均匀分布在标准正态分位数上）。Double Quantization：对量化常数本身再量化，节省约 0.5 bit/参数。Paged Optimizer：用 NVIDIA 统一内存管理优化器状态，防止 OOM。实现：65B 模型在单张 48GB GPU 上可微调。","depth_hint":"计算 65B 模型全参数微调 vs QLoRA 的显存需求对比，分析 NF4 的信息论最优性","status":"pending"},

  {"id":43,"title":"LayerNorm：Transformer 归一化的标准选择","slug":"layer-normalization","tags":["底层原理","训练优化","归一化"],"brief":"LayerNorm（Ba 2016）：对单个样本的所有特征维度做归一化，而非 BatchNorm 的跨样本归一化。LN(x) = γ(x-μ)/σ + β，其中 μ 和 σ 在特征维度上计算。不依赖 batch size，适合序列模型（batch size 可以是 1）。Pre-LN vs Post-LN：Pre-LN 把 LN 放在 sublayer 之前（梯度更稳定），Post-LN 放在残差之后（BERT 原始设计）。RMSNorm：去掉均值中心化，只做方差归一化，LLaMA/Qwen/Mistral 的选择。","depth_hint":"对比 Pre-LN 与 Post-LN 的梯度流差异，推导 RMSNorm 简化为何足够","status":"pending"},
  {"id":44,"title":"GQA 与 MQA：减少 KV 头数的注意力变体","slug":"gqa-mqa","tags":["底层原理","推理优化","注意力机制"],"brief":"标准 MHA：h 个 Q 头 + h 个 K 头 + h 个 V 头，KV Cache 内存 ∝ h。MQA（Shazeer 2019）：所有 Q 头共享同一组 K 和 V，KV Cache 减少 h 倍，但精度略降。GQA（Ainslie 2023）：G 组 K/V，每组 h/G 个 Q 头共享，权衡精度与内存。LLaMA-2 70B 使用 GQA（h=64 个 Q 头，8 个 KV 头），KV Cache 减少 8 倍。升级已有模型：MHA → GQA 可通过合并 KV 头的均值来实现（无需重训练）。","depth_hint":"计算 LLaMA-2 70B 在 GQA vs MHA 配置下 KV Cache 的内存差异，分析精度损失","status":"pending"},
  {"id":45,"title":"连续批处理（Continuous Batching）：提升 LLM 推理吞吐量","slug":"continuous-batching","tags":["工程实践","推理优化","批处理"],"brief":"传统静态批处理：等所有序列生成完毕才处理下一批，短序列等待长序列导致 GPU 利用率低。Continuous Batching（Orca，Yu 2022）：每步迭代后检查已完成序列，立即将新请求插入 batch，而不等待整批完成。实现细节：需要处理不同序列长度（使用 padding 或 ragged tensor）。结合 PagedAttention：完整的现代 LLM serving 系统（vLLM）的核心机制。吞吐量提升：对长短请求混合的工作负载提升 2-4 倍。","depth_hint":"对比静态批处理与连续批处理的时间线图，计算典型混合工作负载的 GPU 利用率差异","status":"pending"},
  {"id":46,"title":"解码策略：贪心、Beam Search、Top-p、Top-k 的对比","slug":"decoding-strategies","tags":["底层原理","推理","解码"],"brief":"贪心解码：每步选 argmax，速度最快，但次优（局部最优不等于全局最优）。Beam Search：维护 k 个最高概率路径，翻译任务的标准方案，但倾向于生成通用/重复文本。Top-k 采样：只从概率最高的 k 个 token 中采样，但 k 固定无法适应分布形状变化。Top-p（Nucleus）采样：选择概率累积 ≥ p 的最小 token 集合，分布尖峰时 k 小，平坦时 k 大。Temperature：采样前将 logits 除以 T，T<1 更确定，T>1 更多样。","depth_hint":"对比不同解码策略在相同 prompt 下的输出多样性与质量，分析 Beam Search 的重复惩罚机制","status":"pending"},
  {"id":47,"title":"困惑度（Perplexity）：语言模型的标准评测指标","slug":"perplexity-metric","tags":["底层原理","评测","语言模型"],"brief":"困惑度 PPL = exp(-1/N Σ log P(x_t|x_1,...,x_{t-1}))，即每个 token 的平均负对数似然的指数。直觉：PPL=10 表示模型平均在 10 个候选中等概率选择。PPL 对词表大小敏感（大词表 PPL 天然更低），跨模型比较需控制分词器。PPL 的局限：不直接反映下游任务性能（GPT-3 PPL 优但 few-shot 不一定胜 BERT）。Bits-per-character（BPC）：与分词无关的困惑度变体。","depth_hint":"计算相同文本在不同分词器（字符级 vs BPE）下的困惑度差异，分析 PPL 的局限性","status":"pending"},
  {"id":48,"title":"涌现能力（Emergent Abilities）：规模带来的不连续性","slug":"emergent-abilities","tags":["模型解析","缩放定律","涌现"],"brief":"Wei 2022：某些能力在小模型上几乎不存在，超过某个参数量阈值后突然出现（非线性跳跃）。典型例子：算术推理（<10B 接近随机，>60B 接近人类）、多步推理、上下文学习（in-context learning）。涌现的争议（Schaeffer 2023）：可能是评测指标的非线性（如 exact match）造成的假象，换用连续指标后涌现消失。Chain-of-Thought：大模型上生效（>100B），小模型上有害。","depth_hint":"分析涌现能力的评测方法，对比 exact match 与 continuous metric 下的能力曲线差异","status":"pending"},

  {"id":49,"title":"上下文学习（In-Context Learning）：无需更新参数的少样本适应","slug":"in-context-learning","tags":["模型解析","ICL","提示工程"],"brief":"ICL：在推理时给出 k 个示例（k-shot），模型无需梯度更新即可适应新任务。示例格式：[x_1, y_1], [x_2, y_2], ..., [x_k, y_k], [x_query, ?]。理论理解：模型在预训练时隐式学习了\"任务推断\"元学习能力（Garg 2022，Transformer 可以实现梯度下降）。示例顺序的影响：不同顺序导致差异高达 30% 的准确率波动（Brown 2020）。标签格式：即使错误标签，ICL 也能保持较好性能（Liu 2022）。","depth_hint":"分析 ICL 的理论机制（meta-learning 视角），设计实验验证示例顺序对性能的影响","status":"pending"},
  {"id":50,"title":"Chain-of-Thought：推理链的原理与触发条件","slug":"chain-of-thought","tags":["底层原理","提示工程","推理"],"brief":"CoT（Wei 2022）：在 few-shot 示例中加入中间推理步骤，模型在推理时自然生成推理链，复杂推理任务性能大幅提升。Zero-shot CoT（Kojima 2022）：仅加\"Let's think step by step\"，无需示例。触发条件：仅在 >100B 参数模型上有效，小模型 CoT 性能反而下降。理论解释：token 预算假说——中间 token 相当于草稿纸，扩展了模型的有效计算深度。Self-Consistency：生成多条推理链后取多数投票，进一步提升 5-10%。","depth_hint":"分析 CoT 在不同规模模型上的效果曲线，推导 token 预算如何扩展模型的计算能力","status":"pending"},
  {"id":51,"title":"RAG 基础架构：检索增强生成的组件与流程","slug":"rag-architecture","tags":["智能体","RAG","检索"],"brief":"RAG（Lewis 2020）：不依赖参数记忆，实时检索外部知识库，将检索结果拼入上下文。基本流程：用户查询 → 查询向量化（Embedding 模型）→ 向量数据库近似最近邻搜索 → Top-k 文档 → 与查询拼接 → LLM 生成。优势：知识可更新、可追溯来源、无幻觉（引用直接来自文档）。关键指标：检索召回率、精确率，最终答案的忠实度（Faithfulness）与相关度（Relevance）。","depth_hint":"对比 RAG 与参数化知识在实时信息、长尾知识、引用来源三个维度上的差异","status":"pending"},
  {"id":52,"title":"向量数据库：HNSW 索引的构建与近似最近邻搜索","slug":"vector-database-hnsw","tags":["工程实践","RAG","向量数据库"],"brief":"精确最近邻搜索：复杂度 O(nd)，n=百万向量时不可用。HNSW（Hierarchical NSW，Malkov 2018）：多层图索引，上层稀疏（长程跳跃），下层密集（精确邻居）。插入：从最高层开始贪心搜索，确定每层的邻居后插入。查询：同样从最高层贪心导航到底层，返回 ef 个候选。参数：M（每节点最大邻居数）、ef_construction（构建时搜索宽度）、ef_search（查询宽度）。Recall@k vs 查询延迟的 trade-off。","depth_hint":"对比暴力搜索、LSH、HNSW 在百万级向量上的查询延迟与 Recall@10 的对比实验","status":"pending"},
  {"id":53,"title":"混合专家模型（MoE）：稀疏激活的计算经济性","slug":"moe-sparse-activation","tags":["底层原理","模型架构","MoE"],"brief":"MoE：将 FFN 替换为 N 个专家 FFN，每次只激活 top-k 个（通常 k=2）。门控网络 G(x) = TopK(softmax(xW_g))。计算量：虽然参数量 ×N，但每次激活参数量不变，计算 FLOP 与稠密模型相同。负载均衡损失：防止所有 token 路由到同一专家，辅助损失 = α × Σ f_i · P_i（f_i 为专家实际使用率，P_i 为路由概率均值）。DeepSeek-V3 671B：256 个专家，每次激活 8 个，实际计算量等效 37B 稠密模型。","depth_hint":"推导负载均衡损失的梯度，分析专家崩塌（Expert Collapse）问题的成因与解决方案","status":"pending"},
  {"id":54,"title":"知识蒸馏：小模型向大模型学习的机制","slug":"knowledge-distillation","tags":["工程实践","模型压缩","蒸馏"],"brief":"Hinton 2015 知识蒸馏：学生模型不仅学 hard label（one-hot），还学教师模型的 soft label（Softmax 输出）。软标签携带类间相似度信息（如 cat 与 dog 的相似度高于 cat 与 car）。蒸馏损失 = α·CE(hard) + (1-α)·KL(p_teacher || p_student)×T²，温度 T>1 平滑软标签。LLM 蒸馏：中间层特征对齐（特征蒸馏），注意力矩阵对齐（注意力蒸馏）。DistilBERT：BERT 的 40% 参数，97% 性能。","depth_hint":"推导蒸馏损失中 T² 系数的来源，分析中间层蒸馏与最终层蒸馏的效果差异","status":"pending"},

  {"id":55,"title":"Mamba 与选择性状态空间模型：线性复杂度序列建模","slug":"mamba-ssm","tags":["底层原理","模型架构","SSM"],"brief":"标准注意力 O(n²) 的长序列局限。状态空间模型（SSM）：y_t = C·h_t，h_t = A·h_{t-1} + B·x_t，线性复杂度，但固定 A/B/C 表达能力受限。Mamba（Gu 2023）的关键创新：选择性 SSM，A/B/C 依赖输入 x（token-dependent），突破固定动态系统的局限。硬件感知的并行扫描算法：训练时用 parallel scan（O(n log n)），推理时用 RNN（O(1)/步）。在语言建模 PPL 上与 Transformer 相当，但推理速度更快。","depth_hint":"对比传统 SSM 与 Mamba 的选择性机制，推导 parallel scan 的计算复杂度","status":"pending"},
  {"id":56,"title":"模型合并：SLERP 与 Task Arithmetic 的参数空间操作","slug":"model-merging","tags":["工程实践","模型架构","模型合并"],"brief":"模型合并：在参数空间而非特征空间组合多个微调模型，无需重训练。线性合并（权重平均）：W_merge = (W_1+W_2)/2，效果差（不同微调方向的干扰）。SLERP（球面线性插值）：沿单位超球面的测地线插值，保持参数向量的范数。Task Arithmetic（Ilharco 2022）：任务向量 τ = W_ft - W_base，W_merge = W_base + λ(τ_1+τ_2+...)，任务向量加减实现能力增删。TIES-merging：解决任务向量符号冲突问题。","depth_hint":"推导 SLERP 公式，分析任务向量加法中符号冲突的产生原因与 TIES 的解决方案","status":"pending"},
  {"id":57,"title":"张量并行：大模型训练的矩阵切分策略","slug":"tensor-parallelism","tags":["工程实践","分布式训练","并行策略"],"brief":"单 GPU 放不下大模型时的解决方案。张量并行（Megatron-LM，Shoeybi 2019）：沿特定维度切分权重矩阵，每个 GPU 持有部分权重。MHA 的列并行：Q/K/V 矩阵按列切分，每头在一个 GPU 上；输出矩阵按行切分，AllReduce 汇聚。FFN 的列-行并行：第一层按列切分（每 GPU 独立计算 GELU），第二层按行切分后 AllReduce。通信量：每层 2 次 AllReduce，通信量 = 2×batch×seq×d（与模型大小无关）。","depth_hint":"绘制 MHA 在 4 卡张量并行下的矩阵切分图，计算通信量与计算量之比","status":"pending"},
  {"id":58,"title":"流水线并行：微批次调度与气泡效率","slug":"pipeline-parallelism","tags":["工程实践","分布式训练","并行策略"],"brief":"流水线并行：不同层分配到不同设备，数据依次流经各设备。朴素流水线：设备 1 处理完才激活设备 2，其他设备空闲（气泡率接近 1）。Gpipe 微批次：将 mini-batch 切为 m 个 micro-batch，流水线效率 = 1 - (p-1)/(p-1+m)，m≫p 时气泡率趋近于 0。1F1B 调度（PipeDream）：前向-反向交替，减少内存占用。激活检查点（activation checkpointing）：减少中间激活内存，代价是重新计算一次前向。","depth_hint":"计算不同 m 和 p 下的流水线气泡率，对比 GPipe 与 1F1B 的内存占用","status":"pending"},
  {"id":59,"title":"ZeRO：优化器状态、梯度、参数的三阶分片","slug":"zero-optimizer","tags":["工程实践","分布式训练","ZeRO"],"brief":"混合精度训练的显存占用：fp16 参数 2 bytes，fp16 梯度 2 bytes，fp32 master weights 4 bytes，fp32 Adam 一阶矩 4 bytes + 二阶矩 4 bytes，总计 16 bytes/参数。ZeRO-1：将优化器状态（8 bytes）分片到 N 卡，每卡显存节省 8/N bytes。ZeRO-2：加上梯度分片（2 bytes）。ZeRO-3：加上参数分片（2 bytes），每卡显存降低 16/N 倍，代价是每次前向/反向需 AllGather 参数。","depth_hint":"计算 ZeRO-3 在 64 卡下训练 70B 模型的显存需求，对比 ZeRO 各阶段的通信量","status":"pending"},
  {"id":60,"title":"幻觉（Hallucination）的成因：知识边界与自信度校准","slug":"llm-hallucination","tags":["模型解析","安全","幻觉"],"brief":"幻觉类型：事实错误（声称不存在的信息）、矛盾（与上文或世界知识矛盾）、无中生有（合理但错误的细节）。成因分析：1. 参数化知识不完整（长尾知识覆盖不足）；2. 训练数据中的噪声和错误；3. 自回归生成的曝光偏差（exposure bias）；4. 强化学习阶段的奖励黑客。缓解方案：RAG（用外部知识替代参数记忆）、自反射（让模型质疑自己的输出）、不确定性估计（输出置信度）。Truthfulness 基准（TruthfulQA）。","depth_hint":"分析不同类型幻觉的产生机制，对比 RAG 与参数记忆在知识密集型任务上的幻觉率","status":"pending"},

  {"id":61,"title":"长上下文：YaRN 与 LongRoPE 的外推方案","slug":"long-context-rope-extension","tags":["底层原理","长上下文","位置编码"],"brief":"RoPE 的外推问题：训练时 max_seq=4096，推理时 seq>4096 时 RoPE 角频率超出训练分布，注意力分数崩溃。位置插值（Chen 2023）：将位置索引缩放到训练范围内（下采样），牺牲短文本精度换长文本泛化。YaRN（Peng 2023）：非均匀缩放，低频维度不缩放，高频维度强缩放，保持短文本性能。NTK-aware 插值：基于神经切线核的频率调整。实践中通常结合少量长文本继续训练（context window extension fine-tuning）。","depth_hint":"分析 RoPE 不同频率维度的外推能力差异，推导 YaRN 的非均匀缩放系数","status":"pending"},
  {"id":62,"title":"ReAct 框架：推理与行动的交替循环","slug":"react-agent-framework","tags":["智能体","Agent","ReAct"],"brief":"ReAct（Yao 2022）：Reasoning + Acting 的交替循环。格式：Thought: [推理步骤] → Action: [工具调用] → Observation: [工具返回] → Thought: ... → Final Answer。与 Chain-of-Thought 的区别：CoT 只推理不行动，ReAct 将推理与工具调用交织。优势：推理步骤可以利用工具的实时信息修正，减少幻觉；行动步骤有推理理由支撑，更可解释。局限：需要模型具备足够强的指令遵循能力。","depth_hint":"展示 ReAct 在 HotpotQA 多跳问题上的完整推理链，分析与纯 CoT 的性能差异","status":"pending"},
  {"id":63,"title":"函数调用（Function Calling）：结构化工具接口的设计","slug":"function-calling","tags":["工程实践","Agent","工具调用"],"brief":"LLM 与外部系统交互的标准化接口。OpenAI API 格式：JSON Schema 定义工具（名称、描述、参数类型），模型返回工具调用请求，系统执行后将结果作为 tool message 返回。并行调用：一次响应中触发多个工具调用（减少延迟）。工具描述的重要性：描述越精确，正确路由率越高。JSON 强制输出：Constrained Decoding / Grammar Sampling 确保输出合法 JSON（不依赖模型对 JSON 格式的记忆）。","depth_hint":"分析 JSON Schema 定义对模型路由准确率的影响，对比不同 Constrained Decoding 方案的延迟开销","status":"pending"},
  {"id":64,"title":"多模态基础：CLIP 的对比学习对齐机制","slug":"clip-contrastive-learning","tags":["底层原理","多模态","CLIP"],"brief":"CLIP（Radford 2021）：用 4 亿图文对训练，对比学习对齐视觉编码器和语言编码器。训练目标：n×n 批次中，n 个正样本对（图像_i，文本_i）相似度最大化，n²-n 个负样本对相似度最小化。InfoNCE 损失：-E[log exp(sim(z_i,z_j)/τ) / Σ_k exp(sim(z_i,z_k)/τ)]。Zero-shot 分类：将类别名称编码为文本，与图像向量计算余弦相似度排名。为什么有效：互联网图文对天然包含语义对齐信息（文章配图、社交媒体帖子）。","depth_hint":"推导 CLIP 的 InfoNCE 损失，计算 batch_size=32768 时正负样本比例对训练的影响","status":"pending"},
  {"id":65,"title":"LLaVA：视觉指令微调的数据构建与训练策略","slug":"llava-visual-instruction","tags":["模型解析","多模态","LLaVA"],"brief":"LLaVA（Liu 2023）：用线性投影层将 CLIP 视觉编码器的输出对齐到语言模型的嵌入空间，数据量极小（665K 样本）却效果优秀。两阶段训练：1. 预训练阶段只训练投影层（冻结视觉编码器和 LLM），对齐视觉特征与文本空间；2. 指令微调阶段解冻 LLM，全参数训练视觉对话能力。数据构建：用 GPT-4 将图像描述转化为对话格式（因为当时 GPT-4 不能接受图像输入，用描述代替）。","depth_hint":"分析视觉 token 数量对 LLaVA 上下文长度和性能的影响，比较不同投影层设计（线性/MLP/交叉注意力）","status":"pending"},
  {"id":66,"title":"Constitutional AI：AI 自我批评与修订的对齐框架","slug":"constitutional-ai","tags":["模型解析","对齐","Anthropic"],"brief":"Constitutional AI（Bai 2022，Anthropic）：用一套原则（Constitution）驱动模型自我批评和修订，减少对人类有害输出标注的依赖。流程：1. 模型生成有害响应（红队阶段）；2. 模型根据 Constitution 自我批评（「以上回复违反了哪条原则？」）；3. 模型根据批评修订响应；4. 用修订后的数据做 SFT；5. 模型对原始 vs 修订响应生成偏好标签，用于 RLHF。与 RLAIF 的关系：RLAIF 是 CAI 的扩展，用 AI 反馈替代人类偏好标注。","depth_hint":"对比 CAI 与传统 RLHF 在数据效率和有害输出控制上的差异","status":"pending"},

  {"id":67,"title":"Attention 稀疏化：局部注意力、滑动窗口与 Longformer","slug":"sparse-attention","tags":["底层原理","长上下文","稀疏注意力"],"brief":"全局注意力 O(n²) 的长序列代价。局部窗口注意力：每个 token 只与前后 w 个 token 交互，O(n×w)，但失去全局感知。Longformer（Beltagy 2020）：滑动窗口局部注意力 + 少量全局 token（[CLS] 和任务相关 token）。BigBird：局部 + 全局 + 随机稀疏，理论上等价于完整注意力（图连接性证明）。稀疏注意力的 CUDA 实现挑战：不规则内存访问难以优化，Flash Attention 的成功使得 O(n²) 的实际代价不那么大。","depth_hint":"对比不同稀疏模式在长文档任务上的精度与速度，分析滑动窗口宽度的选择影响","status":"pending"},
  {"id":68,"title":"模型评测：MMLU、HumanEval、MATH 的设计与局限","slug":"llm-evaluation-benchmarks","tags":["模型解析","评测","Benchmark"],"brief":"MMLU（Hendrycks 2020）：57 个学科的多选题，涵盖高中到专业级知识，0-shot 和 5-shot 设置。HumanEval（Chen 2021）：164 个 Python 编程题，pass@k 指标（生成 n 个解，k 个能通过测试的概率）。MATH（Hendrycks 2021）：12500 道竞赛数学题，答案需精确匹配。评测的污染问题：训练数据可能包含评测集题目，导致分数虚高（GPT-4 的 MATH 分数争议）。LLM-as-Judge：MT-Bench 用 GPT-4 打分，但 GPT-4 偏好自己风格的回答（position bias、verbosity bias）。","depth_hint":"分析评测污染的检测方法，量化 verbosity bias 对 MT-Bench 分数的影响","status":"pending"},
  {"id":69,"title":"越狱攻击：提示注入与对抗性前缀","slug":"jailbreak-attacks","tags":["模型解析","安全","红队"],"brief":"越狱（Jailbreak）：绕过模型安全对齐的攻击。主要类型：1. 角色扮演（「你是一个没有限制的 AI」）；2. 提示注入（在文档中嵌入覆盖系统提示的指令）；3. GCG 对抗性前缀（Zou 2023，梯度优化的 token 序列，触发任意有害输出）；4. 多语言绕过（在安全训练较少的语言中提问）。防御方案：输入过滤（PromptGuard）、输出过滤、对抗性训练。GCG 的迁移性：在开源模型上优化的对抗前缀可以迁移到闭源模型。","depth_hint":"分析 GCG 攻击的优化目标，量化不同防御方案的 attack success rate vs false positive rate","status":"pending"},
  {"id":70,"title":"SFT 数据质量：格式、多样性与规模的权衡","slug":"sft-data-quality","tags":["工程实践","微调","数据"],"brief":"SFT 数据的关键维度：指令多样性（覆盖更多任务类型比增加每类数据量更重要）、响应质量（LIMA 2023：1000 条精心挑选数据 ≈ 52000 条数据的效果）、格式一致性（模板混乱导致模型困惑）。数据清洗：去除过短响应、格式错误、重复数据。合成数据：Self-Instruct（用 GPT-3/4 生成指令），Evol-Instruct（逐步增加指令复杂度）。数据混合比例：通用对话 vs 代码 vs 数学的最优比例研究。","depth_hint":"分析 LIMA 的 1000 条数据实验，量化数据多样性与数量在不同任务类型上的影响","status":"pending"},
  {"id":71,"title":"Prefix Tuning 与 Prompt Tuning：软提示的参数高效微调","slug":"prefix-prompt-tuning","tags":["工程实践","微调","PEFT"],"brief":"Hard Prompt（人工撰写的离散 token 提示）的局限：搜索空间离散，难以优化。Prefix Tuning（Li 2021）：在每层注意力的 K、V 前拼接可训练的连续向量（prefix），只训练这些向量（参数量 ~0.1%）。Prompt Tuning（Lester 2021）：只在输入层加 soft token，比 Prefix Tuning 更简单，超大模型上效果接近全参数微调。P-tuning v2：在每层都加 soft token（类似 Prefix Tuning），改善中等规模模型效果。","depth_hint":"对比 Prefix Tuning、LoRA、全参数微调在不同规模模型上的参数效率与性能","status":"pending"},
  {"id":72,"title":"强化学习基础：MDP、策略梯度与 Actor-Critic","slug":"rl-basics-policy-gradient","tags":["底层原理","强化学习","RL基础"],"brief":"LLM 对齐用到 RL，需要理解基础概念。MDP 框架：状态 s（context），动作 a（下一个 token），奖励 r（仅在句子结束时非零），策略 π（LLM 本身）。REINFORCE：∇J(θ) = E[G_t ∇log π_θ(a_t|s_t)]，高方差问题（用基线 baseline 缓解）。Actor-Critic：Actor 学策略，Critic 估计值函数（减少方差）。PPO：使用 Clip 限制策略更新幅度，是目前 LLM RLHF 的主流算法。","depth_hint":"推导 REINFORCE 的策略梯度公式，分析基线选择对方差的影响","status":"pending"},

  {"id":73,"title":"ViT：将 Transformer 用于图像的 Patch Embedding 机制","slug":"vision-transformer-vit","tags":["底层原理","多模态","ViT"],"brief":"ViT（Dosovitskiy 2020）：将图像切分为固定大小 patch（16×16），每个 patch 线性映射为 token，然后输入标准 Transformer Encoder。位置编码：1D 或 2D 可学习位置编码。[CLS] token 的全局表示用于分类。与 CNN 的根本区别：没有局部感受野和平移等变性，需要更多训练数据（在 ImageNet 上不如 ResNet，在 JFT-300M 上超越）。DeiT：纯 ImageNet 上通过知识蒸馏训练 ViT，教师模型是 CNN（蒸馏 token）。","depth_hint":"计算 ViT-B/16（224×224 输入）的序列长度、patch 数量和参数量，分析 patch size 对计算量的影响","status":"pending"},
  {"id":74,"title":"Adapter 层：在 Transformer 层间插入轻量模块","slug":"adapter-layers","tags":["工程实践","微调","PEFT"],"brief":"Adapter（Houlsby 2019）：在每个 Transformer 层的 MHA 和 FFN 之后插入小型瓶颈网络（Down-project → 非线性 → Up-project），参数量约 1-3%。残差设计：适配器输出与输入相加，初始化为接近恒等变换（Up-project 初始化为 0），避免训练初期破坏预训练权重。与 LoRA 的比较：Adapter 在推理时有额外层（轻微延迟），LoRA 可合并到原始权重（零额外延迟）。MAM Adapter：MHA 用 Prefix Tuning，FFN 用 Adapter 的混合方案。","depth_hint":"对比 Adapter 与 LoRA 在推理延迟和参数效率上的量化差异","status":"pending"},
  {"id":75,"title":"结构化剪枝：注意力头和 FFN 神经元的移除策略","slug":"structured-pruning","tags":["工程实践","模型压缩","剪枝"],"brief":"非结构化剪枝（权重置零）难以在 GPU 上加速（稀疏矩阵乘法效率低）。结构化剪枝：整块移除注意力头、FFN 中间层神经元，移除后模型可以正常密集推理。重要性估计：基于梯度（|w ∂L/∂w|）、基于激活（神经元激活频率）、基于 Taylor 展开（二阶近似）。LLM-Pruner（Ma 2023）：基于梯度的依赖图剪枝，移除 20% 参数后 PPL 增加约 5%。压缩后通常需要 LoRA 微调恢复性能。","depth_hint":"推导 Taylor 展开重要性分数，量化剪枝 20%/30%/50% 时的性能与速度变化","status":"pending"},
  {"id":76,"title":"Test-Time Compute：推理时计算扩展的收益","slug":"test-time-compute","tags":["底层原理","推理","扩展定律"],"brief":"训练时计算扩展（Scaling Laws）在一定规模后收益递减。Test-Time Compute（Snell 2024）：在推理阶段增加计算（更长的 CoT、更多候选答案、搜索树）可以提升性能。Self-Consistency：生成 N 条推理链，多数投票，性能随 N 增加（但有上界）。Best-of-N：生成 N 个答案，用奖励模型选最好的，性能 ∝ log N。MCTS for LLMs：蒙特卡洛树搜索，中间步骤有过程奖励模型打分（OpenAI o1 的猜测机制）。","depth_hint":"绘制 Best-of-N 性能随 N 的对数增长曲线，分析何时 TTC 比训练更大模型更高效","status":"pending"},
  {"id":77,"title":"过程奖励模型（PRM）：步骤级别的推理质量评估","slug":"process-reward-model","tags":["底层原理","推理","对齐"],"brief":"结果奖励（ORM）：只评估最终答案对错。稀疏奖励导致信用分配困难。过程奖励（PRM，Lightman 2023）：对每个推理步骤打分，提供密集监督。数据标注：人工标注每步是否正确（成本高），或用 MC 采样估计（从该步出发生成 N 个解，正确率作为分数）。PRM 在 MATH 上：将 GPT-4 的 pass@1 从 69% 提升到 78%（Best-of-N + PRM 选择）。与 RLHF 的关系：PRM 可以替代 ORM 作为 PPO 的奖励信号，提供更密集的梯度。","depth_hint":"对比 ORM 与 PRM 在多步数学推理上的奖励信号密度与训练效果","status":"pending"},
  {"id":78,"title":"LLM 安全：红队测试、对抗训练与安全评估框架","slug":"llm-safety-red-teaming","tags":["模型解析","安全","评测"],"brief":"红队测试：系统性地探测模型漏洞，分手动红队（人工创意攻击）和自动红队（用另一个 LLM 生成攻击）。攻击类别：有害内容生成、隐私信息泄露、偏见歧视、错误信息。Anthropic/Google 的 AI 安全评估框架（RSP）：设定能力触发点（如 CBRN 知识、网络攻击能力），超过阈值触发额外安全措施。安全-能力权衡：Anthropic 研究表明对齐训练会轻微降低某些能力（alignment tax），但量化因模型而异。","depth_hint":"分析自动红队与手动红队在攻击覆盖率和成功率上的对比，量化 alignment tax 的具体表现","status":"pending"},

  {"id":79,"title":"Qwen 架构解析：技术报告关键设计选择","slug":"qwen-architecture","tags":["模型解析","开源模型","Qwen"],"brief":"Qwen2.5 系列（阿里通义）的架构选择：GQA（节省 KV Cache）、RoPE 位置编码（支持上下文扩展）、SwiGLU 激活函数、Pre-RMSNorm（训练稳定性）。词表大小 151643（覆盖多语言）。上下文扩展到 128K：YaRN 插值 + 长文本继续训练。代码和数学能力：在预训练数据中大幅增加代码和数学比例。MoE 变体：Qwen2-57B-A14B（57B 参数，每次激活 14B）。","depth_hint":"对比 Qwen2.5-72B 与 LLaMA-3-70B 在架构细节上的差异，分析词表大小对多语言任务的影响","status":"pending"},
  {"id":80,"title":"LLaMA-3 架构解析：Meta 开源旗舰模型的技术细节","slug":"llama3-architecture","tags":["模型解析","开源模型","LLaMA"],"brief":"LLaMA-3 技术报告（Meta 2024）关键设计：GQA（8 个 KV 头）、RoPE（θ=500000，提升外推能力）、128K 词表（tiktoken BPE）、SwiGLU FFN。训练数据：15T token（主要是英文和代码）。指令版本：RLHF 流程使用 SFT + DPO + PPO 多阶段。工具调用：原生支持 JSON Schema 格式函数调用。与 Qwen2.5/Mistral 的定位对比：Meta 以开放权重为战略重点。","depth_hint":"计算 LLaMA-3 70B 的 KV Cache 内存占用（batch=1，seq=8192），对比 GQA 与 MHA 的差异","status":"pending"},
  {"id":81,"title":"AWQ：激活感知的权重量化","slug":"awq-quantization","tags":["工程实践","推理优化","量化"],"brief":"GPTQ 的问题：逐层量化，忽略激活分布。AWQ（Lin 2023）的关键发现：并非所有权重同等重要，对应激活值较大的权重通道更重要（量化这些通道误差更大）。解决方案：对重要通道的权重乘以缩放系数 s（放大权重、缩小激活），使量化精度更高，量化后再除以 s（合并到下一层）。AWQ 不需要反向传播，只需校准数据的前向传播（~128 样本）。4-bit AWQ 困惑度接近 FP16，优于 GPTQ 4-bit。","depth_hint":"推导 AWQ 的缩放系数搜索目标函数，对比 AWQ 与 GPTQ 在不同量化位宽下的精度","status":"pending"},
  {"id":82,"title":"模型水印：LLM 生成文本的可检测性","slug":"llm-text-watermarking","tags":["工程实践","安全","水印"],"brief":"为什么需要水印：区分人类写作与 AI 生成内容。绿色 token 水印（Kirchenbauer 2023）：将词表分为绿色集合（G）和红色集合（R），生成时软性偏向绿色 token（logits += δ）。检测：计算文本中绿色 token 的比例，超过阈值则为 AI 生成。关键性质：无法被不知道密钥的人绕过（密钥决定 G/R 划分）；但知道水印存在者可以通过改写绕过。语义不变性问题：水印不能嵌入到语义层面（翻译后消失）。","depth_hint":"分析绿色 token 水印的统计检验方法，计算 FPR=1% 时所需的最短文本长度","status":"pending"},
  {"id":83,"title":"RLAIF：用 AI 反馈替代人类偏好标注","slug":"rlaif-ai-feedback","tags":["底层原理","对齐","RLAIF"],"brief":"RLHF 的瓶颈：人类标注成本高、一致性差（不同标注员偏好不同）。RLAIF（Bai 2022，Lee 2023）：用更强的 LLM（如 Claude）评判两个响应的优劣，替代人类偏好标注。技术细节：Prompt 设计对 AI 判断质量至关重要（Chain-of-thought 后再给出偏好判断）。Constitutional AI：RLAIF 的扩展，用原则列表指导 AI 判断。局限：AI 标注员会继承训练 LLM 的偏见；无法超越标注 LLM 的能力上界。","depth_hint":"对比 RLAIF 与人类标注的一致性（Kappa 系数），分析 AI 标注员的偏见来源","status":"pending"},
  {"id":84,"title":"数据并行：DDP 与梯度同步的通信开销","slug":"data-parallel-ddp","tags":["工程实践","分布式训练","数据并行"],"brief":"数据并行：每个 GPU 持有完整模型副本，处理不同数据，梯度同步后参数更新相同。参数服务器（PS）架构：中心化参数存储，Worker 拉取参数、推送梯度，PS 成为通信瓶颈。AllReduce（Ring-AllReduce）：无中心节点，每个 GPU 既发送又接收，通信量 = 2(N-1)/N × 参数量（N 为 GPU 数），接近最优。PyTorch DDP：NCCL 后端 AllReduce，梯度通信与反向传播重叠（Backward Hook 触发通信）。通信量估算：175B 模型每步通信 ~350GB 梯度（FP16）。","depth_hint":"推导 Ring-AllReduce 的通信复杂度公式，计算 8 卡 DDP 训练 7B 模型的通信效率","status":"pending"},

  {"id":85,"title":"Retrieval-Augmented Fine-tuning（RAFT）：领域知识注入策略","slug":"raft-retrieval-finetuning","tags":["工程实践","RAG","微调"],"brief":"标准 RAG 的问题：LLM 没有学会从文档中提取关键信息（只靠上下文学习，不够稳定）。RAFT（Zhang 2024）：在 SFT 数据中混合带有干扰文档的 QA 对，训练模型学会忽略无关文档、引用相关内容作答（CoT 格式）。训练数据构建：80% 含相关文档的正样本 + 20% 只含干扰文档的负样本（增强鲁棒性）。与 RAG 对比：RAFT 在领域专业任务（医疗、法律）上显著优于纯 RAG，因为模型学会了领域内的信息提取模式。","depth_hint":"对比 RAFT 与标准 RAG 在领域专业问答中的引用准确率差异","status":"pending"},
  {"id":86,"title":"Speculative Decoding 的变体：Medusa、Eagle、Lookahead","slug":"speculative-decoding-variants","tags":["工程实践","推理优化","解码"],"brief":"原始 Speculative Decoding 的局限：需要独立的草稿模型（维护成本高，且加速比依赖草稿模型质量）。Medusa（Cai 2024）：在原始 LLM 之上添加多个预测头，每个头预测 k 步之后的 token，无需独立草稿模型，平均加速 2-3 倍。Eagle（Li 2024）：用 1 层 Transformer 作为草稿模型，输入 LLM 最后一层的隐藏状态，接受率更高（加速 3 倍）。Lookahead：n-gram 草稿，无需额外模型。","depth_hint":"对比 Medusa、Eagle、原始 Speculative Decoding 在 LLaMA-2 70B 上的加速比与硬件要求","status":"pending"},
  {"id":87,"title":"对话状态管理：多轮上下文的截断与压缩策略","slug":"conversation-state-management","tags":["工程实践","推理","对话系统"],"brief":"多轮对话的上下文长度随轮次线性增长，最终超出模型上下文窗口。截断策略：先进先出（丢失重要的早期信息）、保留系统提示+最近 N 轮。摘要压缩：用 LLM 定期将早期对话压缩为摘要（内存损失 vs 长度节省）。KV Cache 复用：对话中不变的前缀（system prompt）可以预计算并缓存 KV，节省推理开销。Prompt Caching（Anthropic）：最长前缀匹配的 KV Cache 服务端复用，API 调用中重复前缀享受折扣。","depth_hint":"计算不同截断策略下 100 轮对话的信息保留率，分析 KV Cache 前缀复用的成本节省","status":"pending"},
  {"id":88,"title":"SWE-bench：代码 Agent 的真实工程能力评测","slug":"swe-bench-evaluation","tags":["模型解析","评测","Agent"],"brief":"SWE-bench（Jimenez 2023）：从 GitHub 真实 issue 中收集 2294 个任务，要求模型修改代码库解决 issue 并通过测试。难度：需要理解大型代码库（平均 39K LOC），定位问题、修改多文件、不破坏现有测试。Lite 版本（300 tasks）用于快速评估。评测方法：自动化测试（Docker 隔离环境），无需人工评判。代表性系统：Devin（13.86%）、SWE-agent（12.47%）、OpenHands（41%，2024 年最新）。分析为何难：需要多步工具调用、上下文理解和工程判断。","depth_hint":"分析 SWE-bench 的难点分布（单文件 vs 多文件修改），对比不同 Agent 架构在任务类型上的优劣","status":"pending"},
  {"id":89,"title":"神经网络的几何视角：流形假设与嵌入空间","slug":"neural-network-geometry","tags":["底层原理","表示学习","几何"],"brief":"流形假设（Manifold Hypothesis）：高维数据（图像、文本）实际分布在低维流形上。神经网络的作用：学习将流形展开（unfolding）为线性可分空间。Embedding 空间的几何性质：线性插值语义连续性（词类比关系的向量加减）、语义聚类结构。表示空间的各向异性问题：BERT 的 token 嵌入分布呈锥形（各向异性），余弦相似度失去意义（SimCSE 2021 解决了这一问题）。对比学习如何解决各向异性：均匀分布在超球面上。","depth_hint":"分析 BERT 嵌入的各向异性问题，推导 SimCSE 的对比学习目标如何改善分布","status":"pending"},
  {"id":90,"title":"未来方向：Test-Time Training、World Models 与 LLM 开放问题","slug":"llm-future-directions","tags":["模型解析","前沿研究","综述"],"brief":"Test-Time Training（TTT）：在推理时对当前输入进行短暂梯度更新，动态适应分布偏移（TTT-RNN 将内存编码进模型权重）。World Models：LLM 是否学到了世界的因果结构，还是只是模式匹配（Bengio 的争论）。组合泛化（Compositional Generalization）：模型是否真正理解概念，还是只记忆了训练中的组合。长文本理解的质量：RULER 等基准测试显示模型在超长文本中存在注意力漂移。持续学习：如何在不灾难性遗忘的情况下更新知识。","depth_hint":"分析当前 LLM 在组合泛化和因果推理上的失败案例，讨论 TTT 的理论基础与实际限制","status":"pending"}
]
