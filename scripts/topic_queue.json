[
  {
    "id": 1,
    "title": "向量与矩阵：神经网络计算的数学基础",
    "slug": "vectors-and-matrices-ml",
    "tags": [
      "底层原理",
      "数学基础",
      "线性代数"
    ],
    "brief": "神经网络的每一层都是矩阵乘法加非线性变换。讲清向量的内积、L2 范数、余弦相似度；矩阵乘法的维度规则（m×n 乘 n×p 得 m×p）；转置、逆矩阵的计算意义；以及为什么 GPU 擅长矩阵运算（SIMD 并行）。",
    "depth_hint": "包含维度变换的具体数值示例，用 NumPy 代码验证每个操作",
    "status": "pending"
  },
  {
    "id": 2,
    "title": "梯度与链式法则：反向传播的数学基础",
    "slug": "gradient-chain-rule",
    "tags": [
      "底层原理",
      "数学基础",
      "微积分"
    ],
    "brief": "从单变量导数推广到多变量偏导数和梯度向量。链式法则 dz/dx = dz/dy × dy/dx 是反向传播的核心。讲清方向导数、梯度下降的几何意义（梯度指向最陡上升方向），以及计算图中每条边对应一次链式法则应用。",
    "depth_hint": "手推一个三层网络的链式求导过程，包含具体数值",
    "status": "pending"
  },
  {
    "id": 3,
    "title": "概率论基础：从随机变量到最大似然估计",
    "slug": "probability-basics-mle",
    "tags": [
      "底层原理",
      "数学基础",
      "概率论"
    ],
    "brief": "随机变量、概率分布（伯努利、高斯、多项分布）、期望与方差的定义。贝叶斯定理 P(θ|X) ∝ P(X|θ)P(θ)。最大似然估计（MLE）：为什么交叉熵损失等价于对数似然最大化——这是分类任务损失函数选择的理论基础。",
    "depth_hint": "推导高斯分布的 MLE 得到样本均值和方差，推导分类交叉熵的 MLE 来源",
    "status": "pending"
  },
  {
    "id": 4,
    "title": "信息熵与交叉熵：损失函数的信息论基础",
    "slug": "entropy-cross-entropy",
    "tags": [
      "底层原理",
      "数学基础",
      "信息论"
    ],
    "brief": "香农熵 H(p) = -Σ p(x) log p(x) 衡量分布的不确定性。交叉熵 H(p,q) = -Σ p(x) log q(x) 衡量用 q 编码真实分布 p 的额外代价。KL 散度 KL(p||q) = H(p,q) - H(p)，非对称性及其含义。为什么分类任务用交叉熵而非 MSE：梯度行为的对比分析。",
    "depth_hint": "包含二分类交叉熵和 MSE 在 sigmoid 输出上的梯度对比计算",
    "status": "pending"
  },
  {
    "id": 5,
    "title": "Softmax 函数：多分类输出的概率化",
    "slug": "softmax-function",
    "tags": [
      "底层原理",
      "数学基础",
      "激活函数"
    ],
    "brief": "Softmax(z_i) = exp(z_i) / Σ exp(z_j)，将任意实数向量映射为概率分布。数值稳定性问题：exp 溢出与 log-sum-exp 技巧。温度参数 τ：Softmax(z/τ) 在 τ→0 时趋向 argmax，τ→∞ 时趋向均匀分布。在注意力机制中 τ=√d_k 的作用。",
    "depth_hint": "展示 exp 溢出的数值示例，实现数值稳定版 Softmax，对比不同温度下的输出分布",
    "status": "pending"
  },
  {
    "id": 6,
    "title": "感知机到多层感知机：深度学习的起点",
    "slug": "perceptron-to-mlp",
    "tags": [
      "底层原理",
      "神经网络",
      "MLP"
    ],
    "brief": "1958 年 Rosenblatt 感知机：y = sign(w·x + b)，线性可分的几何意义。XOR 问题证明单层感知机的局限。MLP 通过隐藏层引入非线性，Universal Approximation Theorem：两层 MLP 可以以任意精度逼近任意连续函数（但不说明需要多少神经元）。前向传播的矩阵形式。",
    "depth_hint": "手工实现一个解决 XOR 问题的两层 MLP，展示权重矩阵维度",
    "status": "pending"
  },
  {
    "id": 7,
    "title": "激活函数全景：Sigmoid、ReLU、GELU 的数学性质与选择",
    "slug": "activation-functions",
    "tags": [
      "底层原理",
      "神经网络",
      "激活函数"
    ],
    "brief": "Sigmoid σ(x) = 1/(1+e^{-x})：饱和区梯度 <0.25，10 层后梯度缩小 100 万倍。ReLU max(0,x)：梯度直通，稀疏激活，但 Dead ReLU 问题（负区梯度恒零）。GELU x·Φ(x)（Φ 为标准正态 CDF）：平滑近似 ReLU，BERT/GPT 的默认选择。SwiGLU = Swish(xW) ⊙ (xV)：LLaMA 系列的 FFN 激活。",
    "depth_hint": "绘制每种激活函数的函数值与导数曲线，分析梯度消失的具体数值",
    "status": "pending"
  },
  {
    "id": 8,
    "title": "批归一化（BatchNorm）：内部协变量偏移的定义与解法",
    "slug": "batch-normalization",
    "tags": [
      "底层原理",
      "训练优化",
      "归一化"
    ],
    "brief": "内部协变量偏移（Internal Covariate Shift）：每层输入分布随参数更新而变化，导致后层需要不断适应。BatchNorm 对 mini-batch 内每个特征维度做 z-score 归一化，再用可学习的 γ、β 缩放偏移。推理时用训练集的运行均值/方差（不是 batch 统计量）。局限：小 batch size 时不稳定，RNN/Transformer 中不适用。",
    "depth_hint": "展示 BN 的前向与反向传播公式，分析为什么 batch size=1 时 BN 退化",
    "status": "pending"
  },
  {
    "id": 9,
    "title": "梯度下降三形态：SGD、Mini-batch、Full-batch 的收敛分析",
    "slug": "gradient-descent-variants",
    "tags": [
      "底层原理",
      "训练优化",
      "优化器"
    ],
    "brief": "Full-batch GD：精确梯度，但内存无法放下全部数据，且鞍点难以逃脱。SGD：高方差梯度带来随机性，有助于逃脱局部极小，但收敛震荡。Mini-batch：权衡两者，batch size 影响梯度噪声与计算效率。线性缩放规则：batch size ×k 时学习率 ×k（适用范围与限制）。",
    "depth_hint": "对比三种形态在凸/非凸函数上的收敛轨迹可视化描述，给出线性缩放规则的证明",
    "status": "pending"
  },
  {
    "id": 10,
    "title": "Adam 优化器：一阶矩与二阶矩的联合自适应",
    "slug": "adam-optimizer",
    "tags": [
      "底层原理",
      "训练优化",
      "优化器"
    ],
    "brief": "Adam = Momentum + RMSProp。m_t = β_1 m_{t-1} + (1-β_1)g_t（一阶矩，方向）；v_t = β_2 v_{t-1} + (1-β_2)g_t²（二阶矩，幅度）；偏差修正 m̂_t = m_t/(1-β_1^t)；更新 θ -= α·m̂_t/(√v̂_t + ε)。默认超参 β_1=0.9, β_2=0.999, ε=1e-8 的选择依据。AdamW：将权重衰减从梯度更新中解耦的必要性。",
    "depth_hint": "推导偏差修正项的来源（初始化为零导致的冷启动偏差），对比 Adam 与 SGD+Momentum 的参数更新轨迹",
    "status": "pending"
  },
  {
    "id": 11,
    "title": "权重初始化：Xavier 与 He 方案的方差推导",
    "slug": "weight-initialization",
    "tags": [
      "底层原理",
      "训练优化",
      "初始化"
    ],
    "brief": "全零初始化的对称性问题：所有神经元计算相同梯度，层永远等价。随机初始化需要控制方差：太大→激活值爆炸，太小→梯度消失。Xavier 初始化 Var(W) = 2/(n_in + n_out) 适用于 Tanh（对称激活）。He 初始化 Var(W) = 2/n_in 适用于 ReLU（正半轴激活，有效扇入减半）。",
    "depth_hint": "从信号方差在前向传播中的保持条件出发，推导 Xavier 公式",
    "status": "pending"
  },
  {
    "id": 12,
    "title": "学习率调度：Warmup、余弦退火与 OneCycleLR",
    "slug": "learning-rate-schedule",
    "tags": [
      "底层原理",
      "训练优化",
      "学习率"
    ],
    "brief": "固定学习率的问题：初期过大导致不稳定，后期过大无法收敛到极小。Warmup 阶段：前 N 步线性增大学习率，让 Adam 的二阶矩估计稳定后再走大步。余弦退火：lr(t) = lr_min + 0.5(lr_max-lr_min)(1+cos(πt/T))。Warmup + 余弦退火是 Transformer 训练的标准配置，Transformer 原始论文的 lr = d_model^{-0.5} · min(step^{-0.5}, step·warmup^{-1.5})。",
    "depth_hint": "实现 Transformer 原始论文的学习率公式，绘制训练步数-学习率曲线",
    "status": "pending"
  },
  {
    "id": 13,
    "title": "词袋模型与 TF-IDF：统计语义的局限",
    "slug": "bag-of-words-tfidf",
    "tags": [
      "底层原理",
      "文本表示",
      "NLP基础"
    ],
    "brief": "词袋模型将文本表示为词频向量，丢失词序和语义关系。TF-IDF = TF(t,d) × IDF(t) = (词频/文档总词数) × log(总文档数/含该词文档数)，降低高频无信息词（the/的）的权重。维度诅咒：10 万词汇表的稀疏向量在余弦相似度计算上的效率问题。这些局限直接驱动了 Word2Vec 的诞生。",
    "depth_hint": "计算一个具体文档集的 TF-IDF 矩阵，分析稀疏性问题",
    "status": "pending"
  },
  {
    "id": 14,
    "title": "Word2Vec CBOW：从上下文预测中心词",
    "slug": "word2vec-cbow",
    "tags": [
      "底层原理",
      "文本表示",
      "Embedding"
    ],
    "brief": "CBOW 模型：给定窗口内的上下文词，预测中心词。架构：上下文词 one-hot → 嵌入层（取均值）→ 线性层 → Softmax → 预测中心词。目标函数：最大化 log P(w_t | w_{t-c:t+c})。嵌入矩阵 W（V×d）是参数，训练完后每行是一个词向量。窗口大小的影响：小窗口捕捉句法关系，大窗口捕捉语义关系。",
    "depth_hint": "推导 CBOW 的反向传播，分析嵌入层参数更新的稀疏性",
    "status": "pending"
  },
  {
    "id": 15,
    "title": "Word2Vec Skip-gram：负采样的数学原理",
    "slug": "word2vec-skipgram-negative-sampling",
    "tags": [
      "底层原理",
      "文本表示",
      "Embedding"
    ],
    "brief": "Skip-gram：给定中心词，预测上下文词。原始 Softmax 对整个词表计算分母，V=10万时代价极高。Negative Sampling 近似：对每个正样本采样 k 个负样本（按词频^(3/4)的分布采样），将多类分类转为 k+1 个二分类。目标函数：log σ(v_c·v_o) + Σ_k E[log σ(-v_c·v_k)]。k=5-20 在实践中与全 Softmax 效果接近。",
    "depth_hint": "推导负采样目标函数的梯度，解释词频^(3/4)采样分布的设计原理",
    "status": "pending"
  },
  {
    "id": 16,
    "title": "BPE 分词：字节对编码的压缩理论基础",
    "slug": "bpe-tokenization",
    "tags": [
      "底层原理",
      "分词",
      "Tokenization"
    ],
    "brief": "Byte Pair Encoding 原是数据压缩算法，被 Sennrich 2016 引入 NLP。算法：从字符级词表出发，反复合并出现频率最高的相邻符号对，直到达到目标词表大小。优势：有效处理 OOV（Out-of-Vocabulary），在词表大小与覆盖率之间取得平衡。GPT-2 使用字节级 BPE（Byte-level BPE），将 UTF-8 字节作为基本单元，词表永远不会有 OOV。",
    "depth_hint": "逐步演示 BPE 在一个小语料上的合并过程，分析 vocab_size=50k vs 100k 的覆盖率差异",
    "status": "pending"
  },
  {
    "id": 17,
    "title": "RNN 与梯度消失：BPTT 中的指数衰减",
    "slug": "rnn-gradient-vanishing",
    "tags": [
      "底层原理",
      "序列模型",
      "RNN"
    ],
    "brief": "RNN 隐状态 h_t = tanh(W_h h_{t-1} + W_x x_t)，梯度需经 T 步反向传播（BPTT）。∂h_t/∂h_k = Π_{i=k+1}^{t} ∂h_i/∂h_{i-1}，每步乘一次 W_h^T 和激活函数导数。tanh 导数最大 1，W_h 的谱范数若 <1 则梯度消失，>1 则梯度爆炸。100 步序列上的梯度幅值变化：|λ|^100 的数值分析。",
    "depth_hint": "计算谱范数为 0.9 时 100 步序列的梯度幅值，用代码验证 BPTT 的指数衰减",
    "status": "pending"
  },
  {
    "id": 18,
    "title": "LSTM：遗忘门、输入门、输出门的参数方程",
    "slug": "lstm-gates",
    "tags": [
      "底层原理",
      "序列模型",
      "LSTM"
    ],
    "brief": "LSTM 引入细胞状态 C_t 作为信息高速公路。四个门：遗忘门 f_t=σ(W_f·[h_{t-1},x_t]+b_f)，输入门 i_t，候选细胞 C̃_t，输出门 o_t。关键：C_t = f_t⊙C_{t-1} + i_t⊙C̃_t，梯度可以通过加法直接反向传播（恒定误差传送带）。与 RNN 的参数量对比：LSTM 参数量是 RNN 的 4 倍。",
    "depth_hint": "完整写出 LSTM 的前向传播方程，推导细胞状态梯度不消失的条件",
    "status": "pending"
  },
  {
    "id": 19,
    "title": "Seq2Seq 架构：编码器-解码器与信息瓶颈",
    "slug": "seq2seq-encoder-decoder",
    "tags": [
      "底层原理",
      "序列模型",
      "Seq2Seq"
    ],
    "brief": "机器翻译的 Seq2Seq：编码器将源序列压缩为固定维度上下文向量 c，解码器以 c 为初始状态自回归生成目标序列。信息瓶颈问题：无论源序列长度如何，c 的维度固定，长序列信息必然损失。实验数据：BLEU 分数随句子长度增加而下降的趋势。这直接驱动了注意力机制的发明。",
    "depth_hint": "分析不同源序列长度下信息压缩率的变化，展示长序列翻译质量下降的实验数据",
    "status": "pending"
  },
  {
    "id": 20,
    "title": "Bahdanau 注意力：对齐分数的加性计算",
    "slug": "bahdanau-attention",
    "tags": [
      "底层原理",
      "注意力机制",
      "Attention"
    ],
    "brief": "Bahdanau 2015：不再用单一上下文向量，而是为每个解码步 t 计算动态上下文 c_t = Σ α_{t,i} h_i。对齐分数 e_{t,i} = v^T tanh(W_1 s_{t-1} + W_2 h_i)，softmax 归一化得 α_{t,i}。α 矩阵可视化：英法翻译中源词与目标词的对齐热图。注意力的计算复杂度：O(T_x × T_y)。",
    "depth_hint": "展示注意力权重矩阵的可视化，推导加性注意力的参数量",
    "status": "pending"
  },
  {
    "id": 21,
    "title": "Self-Attention：序列内部元素的相互依赖建模",
    "slug": "self-attention-mechanism",
    "tags": [
      "底层原理",
      "注意力机制",
      "Transformer"
    ],
    "brief": "Self-Attention 用同一序列的元素同时充当查询（Q）、键（K）、值（V）。Q = XW^Q，K = XW^K，V = XW^V。注意力矩阵 A = softmax(QK^T/√d_k)V。与 Seq2Seq 注意力的区别：这里 Q 和 K/V 来自同一序列。√d_k 缩放：d_k 维度下 QK^T 的方差为 d_k，不缩放时 softmax 进入饱和区，梯度趋近于零。",
    "depth_hint": "推导 QK^T 方差为 d_k 的数学过程，用 PyTorch 实现 Self-Attention",
    "status": "pending"
  },
  {
    "id": 22,
    "title": "多头注意力：并行子空间的语义分工",
    "slug": "multi-head-attention",
    "tags": [
      "底层原理",
      "注意力机制",
      "Transformer"
    ],
    "brief": "Multi-Head Attention：将 d_model 维拆分为 h 个 d_k=d_model/h 维的子空间，在每个子空间独立计算注意力。head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)；MHA = Concat(head_1,...,head_h)W^O。不同 head 可以捕捉不同类型的依赖：句法关系、语义关系、共指关系等（基于 Transformer 可解释性研究）。参数量 = 4 × d_model²（Q/K/V/O 四个矩阵）。",
    "depth_hint": "计算 BERT-base（h=12, d_model=768）的 MHA 参数量，展示 head 专化的实验证据",
    "status": "pending"
  },
  {
    "id": 23,
    "title": "位置编码：正弦函数方案的数学构造",
    "slug": "sinusoidal-positional-encoding",
    "tags": [
      "底层原理",
      "Transformer",
      "位置编码"
    ],
    "brief": "Self-Attention 本身是排列等变的（置换输入顺序不影响输出），必须注入位置信息。Transformer 原始论文的方案：PE(pos, 2i) = sin(pos/10000^{2i/d_model})，PE(pos, 2i+1) = cos(...)。设计原理：不同维度对应不同频率，相对位置 PE(pos+k) 可以表示为 PE(pos) 的线性变换（旋转矩阵）。可学习位置编码（BERT）与固定编码（GPT）的比较。",
    "depth_hint": "推导相对位置的线性变换性质，比较不同位置数 seq_len=512 时各维度的编码值",
    "status": "pending"
  },
  {
    "id": 24,
    "title": "Transformer Encoder：前馈层与残差连接的作用",
    "slug": "transformer-encoder",
    "tags": [
      "底层原理",
      "Transformer",
      "架构"
    ],
    "brief": "Transformer Encoder 层 = LayerNorm + MHA + 残差 + LayerNorm + FFN + 残差。FFN = max(0, xW_1+b_1)W_2+b_2，中间维度 d_ff=4×d_model（e.g., 768→3072→768），参数量是 MHA 的两倍。FFN 被认为存储键值对记忆（Geva 2021）。残差连接 x + Sublayer(x) 保证梯度下界为 1，Pre-Norm（先 LN 再 sublayer）比 Post-Norm 训练更稳定。",
    "depth_hint": "对比 Pre-Norm 和 Post-Norm 在梯度流上的差异，计算 BERT-base 的 FFN 参数量",
    "status": "pending"
  },
  {
    "id": 25,
    "title": "GPT 架构：Decoder-only 与因果语言建模",
    "slug": "gpt-architecture",
    "tags": [
      "模型解析",
      "GPT",
      "预训练"
    ],
    "brief": "GPT 使用仅解码器（Decoder-only）架构，每个 token 只能看到自身及之前的 token（因果掩码）。预训练目标：最大化 Σ log P(x_t|x_1,...,x_{t-1})（自回归语言模型）。因果掩码实现：注意力矩阵 QK^T 上三角置为 -∞，softmax 后对应位置权重为 0。GPT-1 → GPT-2 → GPT-3 的规模演进：参数量、数据量、能力涌现（in-context learning）。",
    "depth_hint": "实现因果掩码的代码，分析参数量从 117M（GPT-1）到 175B（GPT-3）的计算量变化",
    "status": "pending"
  },
  {
    "id": 26,
    "title": "BERT：掩码语言模型与双向上下文表示",
    "slug": "bert-masked-lm",
    "tags": [
      "模型解析",
      "BERT",
      "预训练"
    ],
    "brief": "BERT 使用 Encoder-only 架构，预训练任务：MLM（随机遮盖 15% token，其中 80% 替换为 [MASK]，10% 随机词，10% 不变）和 NSP（下句预测，后被证明无用）。双向上下文：每个 token 的表示融合了左右两侧信息。BERT 不能直接用于生成（因为是双向注意力），适合分类、NER、QA 等判别任务。[CLS] token 的表示用于句子级分类。",
    "depth_hint": "分析 MLM 15% 遮盖率的选择依据，对比 BERT 与 GPT 在不同下游任务上的表现差异",
    "status": "pending"
  },
  {
    "id": 27,
    "title": "Transformer 的 FLOP 分析：注意力层与 FFN 的计算占比",
    "slug": "transformer-flops-analysis",
    "tags": [
      "底层原理",
      "Transformer",
      "效率分析"
    ],
    "brief": "一次前向传播的 FLOP 计算：注意力层 QKV 投影 3×2×n×d²，注意力矩阵 2×n²×d，输出投影 2×n×d²；FFN 两次矩阵乘法 2×2×n×d×4d = 16nd²。总 FLOP ≈ 24nd² + 4n²d（n 为序列长度，d 为模型维度）。序列长度 n 较小时 FFN 占主导（n≪d），n 较大时注意力项 4n²d 开始显著。训练 FLOP ≈ 6×参数量×token 数（来自 Kaplan 2020）。",
    "depth_hint": "计算 GPT-3（175B，seq_len=2048，d=12288）单次前向传播的 FLOP",
    "status": "pending"
  },
  {
    "id": 28,
    "title": "缩放定律（Scaling Laws）：模型性能与计算量的幂律关系",
    "slug": "scaling-laws",
    "tags": [
      "底层原理",
      "缩放定律",
      "训练"
    ],
    "brief": "Kaplan 2020：验证集 loss 与模型参数量 N、数据集大小 D、计算量 C 呈幂律关系 L(N) ∝ N^{-α}（α≈0.076）。Chinchilla 2022 修正：Kaplan 低估了数据量的重要性，最优配置是 N_opt ∝ C^{0.5}（计算量等分给模型和数据）。实践含义：训练 70B 模型的最优 token 数约为 1.4T（20:1 比例）。",
    "depth_hint": "推导 Chinchilla 最优分配公式，计算 LLaMA-2 70B 的实际训练配置与 Chinchilla 最优值的对比",
    "status": "pending"
  },
  {
    "id": 29,
    "title": "预训练数据：质量过滤与去重的工程流程",
    "slug": "pretraining-data-pipeline",
    "tags": [
      "工程实践",
      "预训练",
      "数据工程"
    ],
    "brief": "大规模预训练数据的处理流程：Common Crawl 原始抓取 → 语言识别（fastText）→ 规则过滤（最短/最长文本、特殊字符比例）→ 内容过滤（去除 NSFW）→ 去重（URL 级、段落级、文档级）→ 质量过滤（分类器过滤低质量文本）。MinHash LSH 近似去重：k-gram 特征的局部敏感哈希，n²→nk 的复杂度。数据混合比例：The Pile、SlimPajama、FineWeb 的配比策略。",
    "depth_hint": "计算 1T token 数据集中 MinHash 去重的计算代价，对比精确去重与近似去重的效果",
    "status": "pending"
  },
  {
    "id": 30,
    "title": "混合精度训练：FP16/BF16 的数值稳定性与梯度缩放",
    "slug": "mixed-precision-training",
    "tags": [
      "工程实践",
      "训练优化",
      "分布式训练"
    ],
    "brief": "FP32（1+8+23 位）vs FP16（1+5+10 位）vs BF16（1+8+7 位）：BF16 与 FP32 指数位相同，动态范围一致，但精度更低（尾数位只有 7 位）。混合精度策略：前向/后向用 FP16/BF16，梯度更新和优化器状态保留 FP32 master copy。梯度缩放（Gradient Scaling）：将 loss 乘以 scale factor，防止 FP16 梯度下溢（值 <6×10^{-5} 时变为 0）。",
    "depth_hint": "展示不同精度下的数值范围，用代码演示梯度下溢问题及 GradScaler 的解决方案",
    "status": "pending"
  },
  {
    "id": 31,
    "title": "KV Cache：推理阶段的空间换时间机制",
    "slug": "kv-cache",
    "tags": [
      "底层原理",
      "推理优化",
      "KV Cache"
    ],
    "brief": "自回归生成时，每步需要用到所有历史 token 的 K 和 V（前缀不变）。不缓存：每步重新计算所有 K、V，复杂度 O(T²)。KV Cache：缓存历史 K、V 矩阵，每步只计算新 token 的 K、V 并追加，复杂度降为 O(T)。内存代价：batch_size × seq_len × n_layers × n_heads × d_head × 2（K+V）× bytes。以 LLaMA-2 70B 为例：seq_len=4096 时 KV Cache 约 80GB。",
    "depth_hint": "计算 LLaMA-2 70B 在 batch_size=1、seq_len=4096 时 KV Cache 的精确内存占用",
    "status": "pending"
  },
  {
    "id": 32,
    "title": "RoPE：旋转位置编码的复数域推导",
    "slug": "rope-positional-encoding",
    "tags": [
      "底层原理",
      "位置编码",
      "RoPE"
    ],
    "brief": "RoPE（Su 2021）：将位置信息编码为旋转矩阵，作用于 Q 和 K（不作用于 V）。核心性质：<f_q(x_m, m), f_k(x_n, n)> 只依赖于相对位置 m-n，而非绝对位置。实现：将 d 维向量视为 d/2 个复数，每个复数乘以 e^{imθ_i}（θ_i = 1/10000^{2i/d}）。为什么比正弦编码更好：在外推（训练长度之外）上表现更好，且对注意力计算天然友好。",
    "depth_hint": "用复数乘法推导 RoPE 的内积公式，证明内积只依赖相对位置",
    "status": "pending"
  },
  {
    "id": 33,
    "title": "Flash Attention：IO 感知的注意力分块算法",
    "slug": "flash-attention",
    "tags": [
      "底层原理",
      "推理优化",
      "Flash Attention"
    ],
    "brief": "标准注意力的瓶颈：注意力矩阵 S=QK^T（n×n）需要写入 HBM（GPU 高带宽内存），再读回做 softmax，再写回，再读回乘 V——4 次 HBM 读写，内存访问是瓶颈而非计算。Flash Attention（Dao 2022）：Tiling 分块技术，在 SRAM（片上缓存）内完成分块的 softmax 和加权求和，只读写 O(n) HBM。速度提升：A100 上比标准注意力快 2-4 倍，内存占用从 O(n²) 降至 O(n)。",
    "depth_hint": "解释分块 softmax 的 online normalization 算法，计算不同序列长度下的 HBM 读写量对比",
    "status": "pending"
  },
  {
    "id": 34,
    "title": "量化基础：INT8/INT4 的误差分析与 GPTQ",
    "slug": "quantization-basics",
    "tags": [
      "工程实践",
      "推理优化",
      "量化"
    ],
    "brief": "量化：用低精度整数替代浮点权重。对称量化：W_q = round(W / scale)，scale = max(|W|) / 127（INT8）。量化误差来源：四舍五入误差 + 截断误差（超出范围的值）。逐张量 vs 逐通道量化：逐通道 scale 更精确但实现复杂。GPTQ（Frantar 2022）：基于二阶 Hessian 信息的逐层量化，用未量化权重的误差补偿已量化权重，4-bit 精度损失极小。",
    "depth_hint": "推导 GPTQ 的 OBQ（Optimal Brain Quantization）框架，计算量化前后的困惑度变化",
    "status": "pending"
  },
  {
    "id": 35,
    "title": "投机采样（Speculative Decoding）：草稿模型加速自回归生成",
    "slug": "speculative-decoding",
    "tags": [
      "底层原理",
      "推理优化",
      "解码"
    ],
    "brief": "自回归生成的瓶颈：每步只生成 1 个 token，GPU 利用率极低（memory-bound 而非 compute-bound）。Speculative Decoding（Leviathan 2023）：用小模型（draft）快速生成 k 个 token 草稿，大模型（target）并行验证所有草稿。接受概率 α = min(1, p_target/p_draft)。期望加速比：1/(1-α^k) 倍。条件：draft 模型分布与 target 接近（不影响输出分布的证明）。",
    "depth_hint": "推导投机采样的接受概率公式，证明其与 target 模型等价（无偏），计算不同接受率下的加速比",
    "status": "pending"
  },
  {
    "id": 36,
    "title": "PagedAttention：KV Cache 的动态内存管理",
    "slug": "paged-attention",
    "tags": [
      "工程实践",
      "推理优化",
      "vLLM"
    ],
    "brief": "KV Cache 的内存碎片问题：请求长度不确定，预分配导致大量内存浪费（实测 20-40% 碎片率）。PagedAttention（Kwon 2023，vLLM 基础）：借鉴操作系统分页内存管理，将 KV Cache 切分为固定大小 block（如 16 token/block），按需分配。优势：几乎消除内存碎片，支持 prefix sharing（相同前缀的请求共享 KV Cache block）。吞吐量对比：vLLM 比 HuggingFace Transformers 推理吞吐量高 2-4 倍。",
    "depth_hint": "用具体数字说明 padding-based 方法与 PagedAttention 的内存利用率对比",
    "status": "pending"
  },
  {
    "id": 37,
    "title": "指令微调（Instruction Tuning）：从预训练到对话模型",
    "slug": "instruction-tuning",
    "tags": [
      "模型解析",
      "对齐",
      "微调"
    ],
    "brief": "预训练 LLM 只能续写文本，不能执行指令。指令微调（Wei 2022，FLAN）：在 (instruction, output) 对上做有监督微调，让模型学会遵循自然语言指令。数据格式：System prompt + User instruction + Assistant response。关键发现：任务数量比任务内的数据量更重要（FLAN 用 60+ 任务 vs 1 任务）。InstructGPT 的三阶段：SFT → 奖励模型 → PPO，vs 纯 SFT 的 FLAN。",
    "depth_hint": "对比 FLAN 和 InstructGPT 在不同任务上的表现，分析对话格式设计对模型行为的影响",
    "status": "pending"
  },
  {
    "id": 38,
    "title": "RLHF 奖励模型：偏好对比学习的训练机制",
    "slug": "rlhf-reward-model",
    "tags": [
      "底层原理",
      "对齐",
      "RLHF"
    ],
    "brief": "RLHF 第二阶段：训练奖励模型 R(x, y)，输入 (prompt, response) 对，输出标量分数。训练数据：人类标注员对同一 prompt 的两个响应进行偏好比较 (y_w ≻ y_l)。损失函数：-E[log σ(R(x,y_w) - R(x,y_l))]（Bradley-Terry 模型）。奖励模型通常以预训练 LLM 为初始化，替换最后一层为标量输出。奖励黑客（reward hacking）：模型学会欺骗奖励模型，而非真正改善质量。",
    "depth_hint": "推导 Bradley-Terry 偏好模型，分析奖励模型的数据规模对最终对齐质量的影响",
    "status": "pending"
  },
  {
    "id": 39,
    "title": "PPO 在 LLM 对齐中的应用：KL 散度约束与 RL 稳定性",
    "slug": "ppo-llm-alignment",
    "tags": [
      "底层原理",
      "对齐",
      "强化学习"
    ],
    "brief": "RLHF 第三阶段：用 PPO 最大化奖励模型分数，同时用 KL 散度约束偏离 SFT 模型不能太远。目标：max E[R(x,y)] - β·KL(π_θ || π_ref)，β 控制对齐强度与能力保留的权衡。PPO 的 Clip 机制：限制策略更新步长，防止梯度爆炸。LLM 中的挑战：超长序列的 credit assignment，响应级别（非 token 级别）的稀疏奖励。",
    "depth_hint": "给出 LLM RLHF 中 PPO 的实现伪代码，分析 β 系数选择对模型行为的影响",
    "status": "pending"
  },
  {
    "id": 40,
    "title": "DPO：绕过奖励模型的直接偏好优化",
    "slug": "direct-preference-optimization",
    "tags": [
      "底层原理",
      "对齐",
      "DPO"
    ],
    "brief": "DPO（Rafailov 2023）：从 RLHF 的 KL 约束优化目标出发，数学推导表明最优策略可以直接用参考模型表示，无需显式训练奖励模型。损失函数：-E[log σ(β log(π_θ(y_w|x)/π_ref(y_w|x)) - β log(π_θ(y_l|x)/π_ref(y_l|x)))]。优势：训练稳定，无需 PPO 的复杂超参调整。限制：对数据质量敏感，offline 数据分布与模型分布的 gap 会导致训练不稳定。",
    "depth_hint": "推导 DPO 损失函数从 RLHF 最优解的推导过程，对比 DPO 与 PPO 在相同数据上的效果",
    "status": "pending"
  },
  {
    "id": 41,
    "title": "LoRA：低秩分解的参数高效微调",
    "slug": "lora-peft",
    "tags": [
      "工程实践",
      "微调",
      "PEFT"
    ],
    "brief": "LoRA（Hu 2021）：预训练权重 W₀ 冻结，微调时只训练低秩分解 ΔW = BA（B: d×r，A: r×k，r≪min(d,k)）。推理时合并：W = W₀ + BA，无额外推理开销。关键超参：秩 r（通常 4-64），α（缩放系数，实际学习率 = α/r），应用层（QKV 矩阵、FFN 矩阵）。参数效率：BERT-base 全参数 110M，r=8 时 LoRA 只需约 300K 参数（<0.3%）。",
    "depth_hint": "分析 LoRA 秩 r 的选择对下游任务性能的影响，展示 LoRA 权重合并前后推理速度对比",
    "status": "pending"
  },
  {
    "id": 42,
    "title": "QLoRA：4-bit 量化与 LoRA 的组合微调",
    "slug": "qlora",
    "tags": [
      "工程实践",
      "微调",
      "PEFT"
    ],
    "brief": "QLoRA（Dettmers 2023）：用 NF4（Normal Float 4-bit）量化基础模型权重（减少 ~75% 显存），在量化基础上训练 BF16 精度的 LoRA 适配器。技术细节：NF4 是信息论最优的 4-bit 数据类型（均匀分布在标准正态分位数上）。Double Quantization：对量化常数本身再量化，节省约 0.5 bit/参数。Paged Optimizer：用 NVIDIA 统一内存管理优化器状态，防止 OOM。实现：65B 模型在单张 48GB GPU 上可微调。",
    "depth_hint": "计算 65B 模型全参数微调 vs QLoRA 的显存需求对比，分析 NF4 的信息论最优性",
    "status": "pending"
  },
  {
    "id": 43,
    "title": "LayerNorm：Transformer 归一化的标准选择",
    "slug": "layer-normalization",
    "tags": [
      "底层原理",
      "训练优化",
      "归一化"
    ],
    "brief": "LayerNorm（Ba 2016）：对单个样本的所有特征维度做归一化，而非 BatchNorm 的跨样本归一化。LN(x) = γ(x-μ)/σ + β，其中 μ 和 σ 在特征维度上计算。不依赖 batch size，适合序列模型（batch size 可以是 1）。Pre-LN vs Post-LN：Pre-LN 把 LN 放在 sublayer 之前（梯度更稳定），Post-LN 放在残差之后（BERT 原始设计）。RMSNorm：去掉均值中心化，只做方差归一化，LLaMA/Qwen/Mistral 的选择。",
    "depth_hint": "对比 Pre-LN 与 Post-LN 的梯度流差异，推导 RMSNorm 简化为何足够",
    "status": "pending"
  },
  {
    "id": 44,
    "title": "GQA 与 MQA：减少 KV 头数的注意力变体",
    "slug": "gqa-mqa",
    "tags": [
      "底层原理",
      "推理优化",
      "注意力机制"
    ],
    "brief": "标准 MHA：h 个 Q 头 + h 个 K 头 + h 个 V 头，KV Cache 内存 ∝ h。MQA（Shazeer 2019）：所有 Q 头共享同一组 K 和 V，KV Cache 减少 h 倍，但精度略降。GQA（Ainslie 2023）：G 组 K/V，每组 h/G 个 Q 头共享，权衡精度与内存。LLaMA-2 70B 使用 GQA（h=64 个 Q 头，8 个 KV 头），KV Cache 减少 8 倍。升级已有模型：MHA → GQA 可通过合并 KV 头的均值来实现（无需重训练）。",
    "depth_hint": "计算 LLaMA-2 70B 在 GQA vs MHA 配置下 KV Cache 的内存差异，分析精度损失",
    "status": "pending"
  },
  {
    "id": 45,
    "title": "连续批处理（Continuous Batching）：提升 LLM 推理吞吐量",
    "slug": "continuous-batching",
    "tags": [
      "工程实践",
      "推理优化",
      "批处理"
    ],
    "brief": "传统静态批处理：等所有序列生成完毕才处理下一批，短序列等待长序列导致 GPU 利用率低。Continuous Batching（Orca，Yu 2022）：每步迭代后检查已完成序列，立即将新请求插入 batch，而不等待整批完成。实现细节：需要处理不同序列长度（使用 padding 或 ragged tensor）。结合 PagedAttention：完整的现代 LLM serving 系统（vLLM）的核心机制。吞吐量提升：对长短请求混合的工作负载提升 2-4 倍。",
    "depth_hint": "对比静态批处理与连续批处理的时间线图，计算典型混合工作负载的 GPU 利用率差异",
    "status": "pending"
  },
  {
    "id": 46,
    "title": "解码策略：贪心、Beam Search、Top-p、Top-k 的对比",
    "slug": "decoding-strategies",
    "tags": [
      "底层原理",
      "推理",
      "解码"
    ],
    "brief": "贪心解码：每步选 argmax，速度最快，但次优（局部最优不等于全局最优）。Beam Search：维护 k 个最高概率路径，翻译任务的标准方案，但倾向于生成通用/重复文本。Top-k 采样：只从概率最高的 k 个 token 中采样，但 k 固定无法适应分布形状变化。Top-p（Nucleus）采样：选择概率累积 ≥ p 的最小 token 集合，分布尖峰时 k 小，平坦时 k 大。Temperature：采样前将 logits 除以 T，T<1 更确定，T>1 更多样。",
    "depth_hint": "对比不同解码策略在相同 prompt 下的输出多样性与质量，分析 Beam Search 的重复惩罚机制",
    "status": "pending"
  },
  {
    "id": 47,
    "title": "困惑度（Perplexity）：语言模型的标准评测指标",
    "slug": "perplexity-metric",
    "tags": [
      "底层原理",
      "评测",
      "语言模型"
    ],
    "brief": "困惑度 PPL = exp(-1/N Σ log P(x_t|x_1,...,x_{t-1}))，即每个 token 的平均负对数似然的指数。直觉：PPL=10 表示模型平均在 10 个候选中等概率选择。PPL 对词表大小敏感（大词表 PPL 天然更低），跨模型比较需控制分词器。PPL 的局限：不直接反映下游任务性能（GPT-3 PPL 优但 few-shot 不一定胜 BERT）。Bits-per-character（BPC）：与分词无关的困惑度变体。",
    "depth_hint": "计算相同文本在不同分词器（字符级 vs BPE）下的困惑度差异，分析 PPL 的局限性",
    "status": "pending"
  },
  {
    "id": 48,
    "title": "涌现能力（Emergent Abilities）：规模带来的不连续性",
    "slug": "emergent-abilities",
    "tags": [
      "模型解析",
      "缩放定律",
      "涌现"
    ],
    "brief": "Wei 2022：某些能力在小模型上几乎不存在，超过某个参数量阈值后突然出现（非线性跳跃）。典型例子：算术推理（<10B 接近随机，>60B 接近人类）、多步推理、上下文学习（in-context learning）。涌现的争议（Schaeffer 2023）：可能是评测指标的非线性（如 exact match）造成的假象，换用连续指标后涌现消失。Chain-of-Thought：大模型上生效（>100B），小模型上有害。",
    "depth_hint": "分析涌现能力的评测方法，对比 exact match 与 continuous metric 下的能力曲线差异",
    "status": "pending"
  },
  {
    "id": 49,
    "title": "上下文学习（In-Context Learning）：无需更新参数的少样本适应",
    "slug": "in-context-learning",
    "tags": [
      "模型解析",
      "ICL",
      "提示工程"
    ],
    "brief": "ICL：在推理时给出 k 个示例（k-shot），模型无需梯度更新即可适应新任务。示例格式：[x_1, y_1], [x_2, y_2], ..., [x_k, y_k], [x_query, ?]。理论理解：模型在预训练时隐式学习了\"任务推断\"元学习能力（Garg 2022，Transformer 可以实现梯度下降）。示例顺序的影响：不同顺序导致差异高达 30% 的准确率波动（Brown 2020）。标签格式：即使错误标签，ICL 也能保持较好性能（Liu 2022）。",
    "depth_hint": "分析 ICL 的理论机制（meta-learning 视角），设计实验验证示例顺序对性能的影响",
    "status": "pending"
  },
  {
    "id": 50,
    "title": "Chain-of-Thought：推理链的原理与触发条件",
    "slug": "chain-of-thought",
    "tags": [
      "底层原理",
      "提示工程",
      "推理"
    ],
    "brief": "CoT（Wei 2022）：在 few-shot 示例中加入中间推理步骤，模型在推理时自然生成推理链，复杂推理任务性能大幅提升。Zero-shot CoT（Kojima 2022）：仅加\"Let's think step by step\"，无需示例。触发条件：仅在 >100B 参数模型上有效，小模型 CoT 性能反而下降。理论解释：token 预算假说——中间 token 相当于草稿纸，扩展了模型的有效计算深度。Self-Consistency：生成多条推理链后取多数投票，进一步提升 5-10%。",
    "depth_hint": "分析 CoT 在不同规模模型上的效果曲线，推导 token 预算如何扩展模型的计算能力",
    "status": "pending"
  },
  {
    "id": 51,
    "title": "RAG 基础架构：检索增强生成的组件与流程",
    "slug": "rag-architecture",
    "tags": [
      "智能体",
      "RAG",
      "检索"
    ],
    "brief": "RAG（Lewis 2020）：不依赖参数记忆，实时检索外部知识库，将检索结果拼入上下文。基本流程：用户查询 → 查询向量化（Embedding 模型）→ 向量数据库近似最近邻搜索 → Top-k 文档 → 与查询拼接 → LLM 生成。优势：知识可更新、可追溯来源、无幻觉（引用直接来自文档）。关键指标：检索召回率、精确率，最终答案的忠实度（Faithfulness）与相关度（Relevance）。",
    "depth_hint": "对比 RAG 与参数化知识在实时信息、长尾知识、引用来源三个维度上的差异",
    "status": "pending"
  },
  {
    "id": 52,
    "title": "向量数据库：HNSW 索引的构建与近似最近邻搜索",
    "slug": "vector-database-hnsw",
    "tags": [
      "工程实践",
      "RAG",
      "向量数据库"
    ],
    "brief": "精确最近邻搜索：复杂度 O(nd)，n=百万向量时不可用。HNSW（Hierarchical NSW，Malkov 2018）：多层图索引，上层稀疏（长程跳跃），下层密集（精确邻居）。插入：从最高层开始贪心搜索，确定每层的邻居后插入。查询：同样从最高层贪心导航到底层，返回 ef 个候选。参数：M（每节点最大邻居数）、ef_construction（构建时搜索宽度）、ef_search（查询宽度）。Recall@k vs 查询延迟的 trade-off。",
    "depth_hint": "对比暴力搜索、LSH、HNSW 在百万级向量上的查询延迟与 Recall@10 的对比实验",
    "status": "pending"
  },
  {
    "id": 53,
    "title": "混合专家模型（MoE）：稀疏激活的计算经济性",
    "slug": "moe-sparse-activation",
    "tags": [
      "底层原理",
      "模型架构",
      "MoE"
    ],
    "brief": "MoE：将 FFN 替换为 N 个专家 FFN，每次只激活 top-k 个（通常 k=2）。门控网络 G(x) = TopK(softmax(xW_g))。计算量：虽然参数量 ×N，但每次激活参数量不变，计算 FLOP 与稠密模型相同。负载均衡损失：防止所有 token 路由到同一专家，辅助损失 = α × Σ f_i · P_i（f_i 为专家实际使用率，P_i 为路由概率均值）。DeepSeek-V3 671B：256 个专家，每次激活 8 个，实际计算量等效 37B 稠密模型。",
    "depth_hint": "推导负载均衡损失的梯度，分析专家崩塌（Expert Collapse）问题的成因与解决方案",
    "status": "pending"
  },
  {
    "id": 54,
    "title": "知识蒸馏：小模型向大模型学习的机制",
    "slug": "knowledge-distillation",
    "tags": [
      "工程实践",
      "模型压缩",
      "蒸馏"
    ],
    "brief": "Hinton 2015 知识蒸馏：学生模型不仅学 hard label（one-hot），还学教师模型的 soft label（Softmax 输出）。软标签携带类间相似度信息（如 cat 与 dog 的相似度高于 cat 与 car）。蒸馏损失 = α·CE(hard) + (1-α)·KL(p_teacher || p_student)×T²，温度 T>1 平滑软标签。LLM 蒸馏：中间层特征对齐（特征蒸馏），注意力矩阵对齐（注意力蒸馏）。DistilBERT：BERT 的 40% 参数，97% 性能。",
    "depth_hint": "推导蒸馏损失中 T² 系数的来源，分析中间层蒸馏与最终层蒸馏的效果差异",
    "status": "pending"
  },
  {
    "id": 55,
    "title": "Mamba 与选择性状态空间模型：线性复杂度序列建模",
    "slug": "mamba-ssm",
    "tags": [
      "底层原理",
      "模型架构",
      "SSM"
    ],
    "brief": "标准注意力 O(n²) 的长序列局限。状态空间模型（SSM）：y_t = C·h_t，h_t = A·h_{t-1} + B·x_t，线性复杂度，但固定 A/B/C 表达能力受限。Mamba（Gu 2023）的关键创新：选择性 SSM，A/B/C 依赖输入 x（token-dependent），突破固定动态系统的局限。硬件感知的并行扫描算法：训练时用 parallel scan（O(n log n)），推理时用 RNN（O(1)/步）。在语言建模 PPL 上与 Transformer 相当，但推理速度更快。",
    "depth_hint": "对比传统 SSM 与 Mamba 的选择性机制，推导 parallel scan 的计算复杂度",
    "status": "pending"
  },
  {
    "id": 56,
    "title": "模型合并：SLERP 与 Task Arithmetic 的参数空间操作",
    "slug": "model-merging",
    "tags": [
      "工程实践",
      "模型架构",
      "模型合并"
    ],
    "brief": "模型合并：在参数空间而非特征空间组合多个微调模型，无需重训练。线性合并（权重平均）：W_merge = (W_1+W_2)/2，效果差（不同微调方向的干扰）。SLERP（球面线性插值）：沿单位超球面的测地线插值，保持参数向量的范数。Task Arithmetic（Ilharco 2022）：任务向量 τ = W_ft - W_base，W_merge = W_base + λ(τ_1+τ_2+...)，任务向量加减实现能力增删。TIES-merging：解决任务向量符号冲突问题。",
    "depth_hint": "推导 SLERP 公式，分析任务向量加法中符号冲突的产生原因与 TIES 的解决方案",
    "status": "pending"
  },
  {
    "id": 57,
    "title": "张量并行：大模型训练的矩阵切分策略",
    "slug": "tensor-parallelism",
    "tags": [
      "工程实践",
      "分布式训练",
      "并行策略"
    ],
    "brief": "单 GPU 放不下大模型时的解决方案。张量并行（Megatron-LM，Shoeybi 2019）：沿特定维度切分权重矩阵，每个 GPU 持有部分权重。MHA 的列并行：Q/K/V 矩阵按列切分，每头在一个 GPU 上；输出矩阵按行切分，AllReduce 汇聚。FFN 的列-行并行：第一层按列切分（每 GPU 独立计算 GELU），第二层按行切分后 AllReduce。通信量：每层 2 次 AllReduce，通信量 = 2×batch×seq×d（与模型大小无关）。",
    "depth_hint": "绘制 MHA 在 4 卡张量并行下的矩阵切分图，计算通信量与计算量之比",
    "status": "pending"
  },
  {
    "id": 58,
    "title": "流水线并行：微批次调度与气泡效率",
    "slug": "pipeline-parallelism",
    "tags": [
      "工程实践",
      "分布式训练",
      "并行策略"
    ],
    "brief": "流水线并行：不同层分配到不同设备，数据依次流经各设备。朴素流水线：设备 1 处理完才激活设备 2，其他设备空闲（气泡率接近 1）。Gpipe 微批次：将 mini-batch 切为 m 个 micro-batch，流水线效率 = 1 - (p-1)/(p-1+m)，m≫p 时气泡率趋近于 0。1F1B 调度（PipeDream）：前向-反向交替，减少内存占用。激活检查点（activation checkpointing）：减少中间激活内存，代价是重新计算一次前向。",
    "depth_hint": "计算不同 m 和 p 下的流水线气泡率，对比 GPipe 与 1F1B 的内存占用",
    "status": "pending"
  },
  {
    "id": 59,
    "title": "ZeRO：优化器状态、梯度、参数的三阶分片",
    "slug": "zero-optimizer",
    "tags": [
      "工程实践",
      "分布式训练",
      "ZeRO"
    ],
    "brief": "混合精度训练的显存占用：fp16 参数 2 bytes，fp16 梯度 2 bytes，fp32 master weights 4 bytes，fp32 Adam 一阶矩 4 bytes + 二阶矩 4 bytes，总计 16 bytes/参数。ZeRO-1：将优化器状态（8 bytes）分片到 N 卡，每卡显存节省 8/N bytes。ZeRO-2：加上梯度分片（2 bytes）。ZeRO-3：加上参数分片（2 bytes），每卡显存降低 16/N 倍，代价是每次前向/反向需 AllGather 参数。",
    "depth_hint": "计算 ZeRO-3 在 64 卡下训练 70B 模型的显存需求，对比 ZeRO 各阶段的通信量",
    "status": "pending"
  },
  {
    "id": 60,
    "title": "幻觉（Hallucination）的成因：知识边界与自信度校准",
    "slug": "llm-hallucination",
    "tags": [
      "模型解析",
      "安全",
      "幻觉"
    ],
    "brief": "幻觉类型：事实错误（声称不存在的信息）、矛盾（与上文或世界知识矛盾）、无中生有（合理但错误的细节）。成因分析：1. 参数化知识不完整（长尾知识覆盖不足）；2. 训练数据中的噪声和错误；3. 自回归生成的曝光偏差（exposure bias）；4. 强化学习阶段的奖励黑客。缓解方案：RAG（用外部知识替代参数记忆）、自反射（让模型质疑自己的输出）、不确定性估计（输出置信度）。Truthfulness 基准（TruthfulQA）。",
    "depth_hint": "分析不同类型幻觉的产生机制，对比 RAG 与参数记忆在知识密集型任务上的幻觉率",
    "status": "pending"
  },
  {
    "id": 61,
    "title": "长上下文：YaRN 与 LongRoPE 的外推方案",
    "slug": "long-context-rope-extension",
    "tags": [
      "底层原理",
      "长上下文",
      "位置编码"
    ],
    "brief": "RoPE 的外推问题：训练时 max_seq=4096，推理时 seq>4096 时 RoPE 角频率超出训练分布，注意力分数崩溃。位置插值（Chen 2023）：将位置索引缩放到训练范围内（下采样），牺牲短文本精度换长文本泛化。YaRN（Peng 2023）：非均匀缩放，低频维度不缩放，高频维度强缩放，保持短文本性能。NTK-aware 插值：基于神经切线核的频率调整。实践中通常结合少量长文本继续训练（context window extension fine-tuning）。",
    "depth_hint": "分析 RoPE 不同频率维度的外推能力差异，推导 YaRN 的非均匀缩放系数",
    "status": "pending"
  },
  {
    "id": 62,
    "title": "ReAct 框架：推理与行动的交替循环",
    "slug": "react-agent-framework",
    "tags": [
      "智能体",
      "Agent",
      "ReAct"
    ],
    "brief": "ReAct（Yao 2022）：Reasoning + Acting 的交替循环。格式：Thought: [推理步骤] → Action: [工具调用] → Observation: [工具返回] → Thought: ... → Final Answer。与 Chain-of-Thought 的区别：CoT 只推理不行动，ReAct 将推理与工具调用交织。优势：推理步骤可以利用工具的实时信息修正，减少幻觉；行动步骤有推理理由支撑，更可解释。局限：需要模型具备足够强的指令遵循能力。",
    "depth_hint": "展示 ReAct 在 HotpotQA 多跳问题上的完整推理链，分析与纯 CoT 的性能差异",
    "status": "pending"
  },
  {
    "id": 63,
    "title": "函数调用（Function Calling）：结构化工具接口的设计",
    "slug": "function-calling",
    "tags": [
      "工程实践",
      "Agent",
      "工具调用"
    ],
    "brief": "LLM 与外部系统交互的标准化接口。OpenAI API 格式：JSON Schema 定义工具（名称、描述、参数类型），模型返回工具调用请求，系统执行后将结果作为 tool message 返回。并行调用：一次响应中触发多个工具调用（减少延迟）。工具描述的重要性：描述越精确，正确路由率越高。JSON 强制输出：Constrained Decoding / Grammar Sampling 确保输出合法 JSON（不依赖模型对 JSON 格式的记忆）。",
    "depth_hint": "分析 JSON Schema 定义对模型路由准确率的影响，对比不同 Constrained Decoding 方案的延迟开销",
    "status": "pending"
  },
  {
    "id": 64,
    "title": "多模态基础：CLIP 的对比学习对齐机制",
    "slug": "clip-contrastive-learning",
    "tags": [
      "底层原理",
      "多模态",
      "CLIP"
    ],
    "brief": "CLIP（Radford 2021）：用 4 亿图文对训练，对比学习对齐视觉编码器和语言编码器。训练目标：n×n 批次中，n 个正样本对（图像_i，文本_i）相似度最大化，n²-n 个负样本对相似度最小化。InfoNCE 损失：-E[log exp(sim(z_i,z_j)/τ) / Σ_k exp(sim(z_i,z_k)/τ)]。Zero-shot 分类：将类别名称编码为文本，与图像向量计算余弦相似度排名。为什么有效：互联网图文对天然包含语义对齐信息（文章配图、社交媒体帖子）。",
    "depth_hint": "推导 CLIP 的 InfoNCE 损失，计算 batch_size=32768 时正负样本比例对训练的影响",
    "status": "pending"
  },
  {
    "id": 65,
    "title": "LLaVA：视觉指令微调的数据构建与训练策略",
    "slug": "llava-visual-instruction",
    "tags": [
      "模型解析",
      "多模态",
      "LLaVA"
    ],
    "brief": "LLaVA（Liu 2023）：用线性投影层将 CLIP 视觉编码器的输出对齐到语言模型的嵌入空间，数据量极小（665K 样本）却效果优秀。两阶段训练：1. 预训练阶段只训练投影层（冻结视觉编码器和 LLM），对齐视觉特征与文本空间；2. 指令微调阶段解冻 LLM，全参数训练视觉对话能力。数据构建：用 GPT-4 将图像描述转化为对话格式（因为当时 GPT-4 不能接受图像输入，用描述代替）。",
    "depth_hint": "分析视觉 token 数量对 LLaVA 上下文长度和性能的影响，比较不同投影层设计（线性/MLP/交叉注意力）",
    "status": "pending"
  },
  {
    "id": 66,
    "title": "Constitutional AI：AI 自我批评与修订的对齐框架",
    "slug": "constitutional-ai",
    "tags": [
      "模型解析",
      "对齐",
      "Anthropic"
    ],
    "brief": "Constitutional AI（Bai 2022，Anthropic）：用一套原则（Constitution）驱动模型自我批评和修订，减少对人类有害输出标注的依赖。流程：1. 模型生成有害响应（红队阶段）；2. 模型根据 Constitution 自我批评（「以上回复违反了哪条原则？」）；3. 模型根据批评修订响应；4. 用修订后的数据做 SFT；5. 模型对原始 vs 修订响应生成偏好标签，用于 RLHF。与 RLAIF 的关系：RLAIF 是 CAI 的扩展，用 AI 反馈替代人类偏好标注。",
    "depth_hint": "对比 CAI 与传统 RLHF 在数据效率和有害输出控制上的差异",
    "status": "pending"
  },
  {
    "id": 67,
    "title": "Attention 稀疏化：局部注意力、滑动窗口与 Longformer",
    "slug": "sparse-attention",
    "tags": [
      "底层原理",
      "长上下文",
      "稀疏注意力"
    ],
    "brief": "全局注意力 O(n²) 的长序列代价。局部窗口注意力：每个 token 只与前后 w 个 token 交互，O(n×w)，但失去全局感知。Longformer（Beltagy 2020）：滑动窗口局部注意力 + 少量全局 token（[CLS] 和任务相关 token）。BigBird：局部 + 全局 + 随机稀疏，理论上等价于完整注意力（图连接性证明）。稀疏注意力的 CUDA 实现挑战：不规则内存访问难以优化，Flash Attention 的成功使得 O(n²) 的实际代价不那么大。",
    "depth_hint": "对比不同稀疏模式在长文档任务上的精度与速度，分析滑动窗口宽度的选择影响",
    "status": "pending"
  },
  {
    "id": 68,
    "title": "模型评测：MMLU、HumanEval、MATH 的设计与局限",
    "slug": "llm-evaluation-benchmarks",
    "tags": [
      "模型解析",
      "评测",
      "Benchmark"
    ],
    "brief": "MMLU（Hendrycks 2020）：57 个学科的多选题，涵盖高中到专业级知识，0-shot 和 5-shot 设置。HumanEval（Chen 2021）：164 个 Python 编程题，pass@k 指标（生成 n 个解，k 个能通过测试的概率）。MATH（Hendrycks 2021）：12500 道竞赛数学题，答案需精确匹配。评测的污染问题：训练数据可能包含评测集题目，导致分数虚高（GPT-4 的 MATH 分数争议）。LLM-as-Judge：MT-Bench 用 GPT-4 打分，但 GPT-4 偏好自己风格的回答（position bias、verbosity bias）。",
    "depth_hint": "分析评测污染的检测方法，量化 verbosity bias 对 MT-Bench 分数的影响",
    "status": "pending"
  },
  {
    "id": 69,
    "title": "越狱攻击：提示注入与对抗性前缀",
    "slug": "jailbreak-attacks",
    "tags": [
      "模型解析",
      "安全",
      "红队"
    ],
    "brief": "越狱（Jailbreak）：绕过模型安全对齐的攻击。主要类型：1. 角色扮演（「你是一个没有限制的 AI」）；2. 提示注入（在文档中嵌入覆盖系统提示的指令）；3. GCG 对抗性前缀（Zou 2023，梯度优化的 token 序列，触发任意有害输出）；4. 多语言绕过（在安全训练较少的语言中提问）。防御方案：输入过滤（PromptGuard）、输出过滤、对抗性训练。GCG 的迁移性：在开源模型上优化的对抗前缀可以迁移到闭源模型。",
    "depth_hint": "分析 GCG 攻击的优化目标，量化不同防御方案的 attack success rate vs false positive rate",
    "status": "pending"
  },
  {
    "id": 70,
    "title": "SFT 数据质量：格式、多样性与规模的权衡",
    "slug": "sft-data-quality",
    "tags": [
      "工程实践",
      "微调",
      "数据"
    ],
    "brief": "SFT 数据的关键维度：指令多样性（覆盖更多任务类型比增加每类数据量更重要）、响应质量（LIMA 2023：1000 条精心挑选数据 ≈ 52000 条数据的效果）、格式一致性（模板混乱导致模型困惑）。数据清洗：去除过短响应、格式错误、重复数据。合成数据：Self-Instruct（用 GPT-3/4 生成指令），Evol-Instruct（逐步增加指令复杂度）。数据混合比例：通用对话 vs 代码 vs 数学的最优比例研究。",
    "depth_hint": "分析 LIMA 的 1000 条数据实验，量化数据多样性与数量在不同任务类型上的影响",
    "status": "pending"
  },
  {
    "id": 71,
    "title": "Prefix Tuning 与 Prompt Tuning：软提示的参数高效微调",
    "slug": "prefix-prompt-tuning",
    "tags": [
      "工程实践",
      "微调",
      "PEFT"
    ],
    "brief": "Hard Prompt（人工撰写的离散 token 提示）的局限：搜索空间离散，难以优化。Prefix Tuning（Li 2021）：在每层注意力的 K、V 前拼接可训练的连续向量（prefix），只训练这些向量（参数量 ~0.1%）。Prompt Tuning（Lester 2021）：只在输入层加 soft token，比 Prefix Tuning 更简单，超大模型上效果接近全参数微调。P-tuning v2：在每层都加 soft token（类似 Prefix Tuning），改善中等规模模型效果。",
    "depth_hint": "对比 Prefix Tuning、LoRA、全参数微调在不同规模模型上的参数效率与性能",
    "status": "pending"
  },
  {
    "id": 72,
    "title": "强化学习基础：MDP、策略梯度与 Actor-Critic",
    "slug": "rl-basics-policy-gradient",
    "tags": [
      "底层原理",
      "强化学习",
      "RL基础"
    ],
    "brief": "LLM 对齐用到 RL，需要理解基础概念。MDP 框架：状态 s（context），动作 a（下一个 token），奖励 r（仅在句子结束时非零），策略 π（LLM 本身）。REINFORCE：∇J(θ) = E[G_t ∇log π_θ(a_t|s_t)]，高方差问题（用基线 baseline 缓解）。Actor-Critic：Actor 学策略，Critic 估计值函数（减少方差）。PPO：使用 Clip 限制策略更新幅度，是目前 LLM RLHF 的主流算法。",
    "depth_hint": "推导 REINFORCE 的策略梯度公式，分析基线选择对方差的影响",
    "status": "pending"
  },
  {
    "id": 73,
    "title": "ViT：将 Transformer 用于图像的 Patch Embedding 机制",
    "slug": "vision-transformer-vit",
    "tags": [
      "底层原理",
      "多模态",
      "ViT"
    ],
    "brief": "ViT（Dosovitskiy 2020）：将图像切分为固定大小 patch（16×16），每个 patch 线性映射为 token，然后输入标准 Transformer Encoder。位置编码：1D 或 2D 可学习位置编码。[CLS] token 的全局表示用于分类。与 CNN 的根本区别：没有局部感受野和平移等变性，需要更多训练数据（在 ImageNet 上不如 ResNet，在 JFT-300M 上超越）。DeiT：纯 ImageNet 上通过知识蒸馏训练 ViT，教师模型是 CNN（蒸馏 token）。",
    "depth_hint": "计算 ViT-B/16（224×224 输入）的序列长度、patch 数量和参数量，分析 patch size 对计算量的影响",
    "status": "pending"
  },
  {
    "id": 74,
    "title": "Adapter 层：在 Transformer 层间插入轻量模块",
    "slug": "adapter-layers",
    "tags": [
      "工程实践",
      "微调",
      "PEFT"
    ],
    "brief": "Adapter（Houlsby 2019）：在每个 Transformer 层的 MHA 和 FFN 之后插入小型瓶颈网络（Down-project → 非线性 → Up-project），参数量约 1-3%。残差设计：适配器输出与输入相加，初始化为接近恒等变换（Up-project 初始化为 0），避免训练初期破坏预训练权重。与 LoRA 的比较：Adapter 在推理时有额外层（轻微延迟），LoRA 可合并到原始权重（零额外延迟）。MAM Adapter：MHA 用 Prefix Tuning，FFN 用 Adapter 的混合方案。",
    "depth_hint": "对比 Adapter 与 LoRA 在推理延迟和参数效率上的量化差异",
    "status": "pending"
  },
  {
    "id": 75,
    "title": "结构化剪枝：注意力头和 FFN 神经元的移除策略",
    "slug": "structured-pruning",
    "tags": [
      "工程实践",
      "模型压缩",
      "剪枝"
    ],
    "brief": "非结构化剪枝（权重置零）难以在 GPU 上加速（稀疏矩阵乘法效率低）。结构化剪枝：整块移除注意力头、FFN 中间层神经元，移除后模型可以正常密集推理。重要性估计：基于梯度（|w ∂L/∂w|）、基于激活（神经元激活频率）、基于 Taylor 展开（二阶近似）。LLM-Pruner（Ma 2023）：基于梯度的依赖图剪枝，移除 20% 参数后 PPL 增加约 5%。压缩后通常需要 LoRA 微调恢复性能。",
    "depth_hint": "推导 Taylor 展开重要性分数，量化剪枝 20%/30%/50% 时的性能与速度变化",
    "status": "pending"
  },
  {
    "id": 76,
    "title": "Test-Time Compute：推理时计算扩展的收益",
    "slug": "test-time-compute",
    "tags": [
      "底层原理",
      "推理",
      "扩展定律"
    ],
    "brief": "训练时计算扩展（Scaling Laws）在一定规模后收益递减。Test-Time Compute（Snell 2024）：在推理阶段增加计算（更长的 CoT、更多候选答案、搜索树）可以提升性能。Self-Consistency：生成 N 条推理链，多数投票，性能随 N 增加（但有上界）。Best-of-N：生成 N 个答案，用奖励模型选最好的，性能 ∝ log N。MCTS for LLMs：蒙特卡洛树搜索，中间步骤有过程奖励模型打分（OpenAI o1 的猜测机制）。",
    "depth_hint": "绘制 Best-of-N 性能随 N 的对数增长曲线，分析何时 TTC 比训练更大模型更高效",
    "status": "pending"
  },
  {
    "id": 77,
    "title": "过程奖励模型（PRM）：步骤级别的推理质量评估",
    "slug": "process-reward-model",
    "tags": [
      "底层原理",
      "推理",
      "对齐"
    ],
    "brief": "结果奖励（ORM）：只评估最终答案对错。稀疏奖励导致信用分配困难。过程奖励（PRM，Lightman 2023）：对每个推理步骤打分，提供密集监督。数据标注：人工标注每步是否正确（成本高），或用 MC 采样估计（从该步出发生成 N 个解，正确率作为分数）。PRM 在 MATH 上：将 GPT-4 的 pass@1 从 69% 提升到 78%（Best-of-N + PRM 选择）。与 RLHF 的关系：PRM 可以替代 ORM 作为 PPO 的奖励信号，提供更密集的梯度。",
    "depth_hint": "对比 ORM 与 PRM 在多步数学推理上的奖励信号密度与训练效果",
    "status": "pending"
  },
  {
    "id": 78,
    "title": "LLM 安全：红队测试、对抗训练与安全评估框架",
    "slug": "llm-safety-red-teaming",
    "tags": [
      "模型解析",
      "安全",
      "评测"
    ],
    "brief": "红队测试：系统性地探测模型漏洞，分手动红队（人工创意攻击）和自动红队（用另一个 LLM 生成攻击）。攻击类别：有害内容生成、隐私信息泄露、偏见歧视、错误信息。Anthropic/Google 的 AI 安全评估框架（RSP）：设定能力触发点（如 CBRN 知识、网络攻击能力），超过阈值触发额外安全措施。安全-能力权衡：Anthropic 研究表明对齐训练会轻微降低某些能力（alignment tax），但量化因模型而异。",
    "depth_hint": "分析自动红队与手动红队在攻击覆盖率和成功率上的对比，量化 alignment tax 的具体表现",
    "status": "pending"
  },
  {
    "id": 79,
    "title": "Qwen 架构解析：技术报告关键设计选择",
    "slug": "qwen-architecture",
    "tags": [
      "模型解析",
      "开源模型",
      "Qwen"
    ],
    "brief": "Qwen2.5 系列（阿里通义）的架构选择：GQA（节省 KV Cache）、RoPE 位置编码（支持上下文扩展）、SwiGLU 激活函数、Pre-RMSNorm（训练稳定性）。词表大小 151643（覆盖多语言）。上下文扩展到 128K：YaRN 插值 + 长文本继续训练。代码和数学能力：在预训练数据中大幅增加代码和数学比例。MoE 变体：Qwen2-57B-A14B（57B 参数，每次激活 14B）。",
    "depth_hint": "对比 Qwen2.5-72B 与 LLaMA-3-70B 在架构细节上的差异，分析词表大小对多语言任务的影响",
    "status": "pending"
  },
  {
    "id": 80,
    "title": "LLaMA-3 架构解析：Meta 开源旗舰模型的技术细节",
    "slug": "llama3-architecture",
    "tags": [
      "模型解析",
      "开源模型",
      "LLaMA"
    ],
    "brief": "LLaMA-3 技术报告（Meta 2024）关键设计：GQA（8 个 KV 头）、RoPE（θ=500000，提升外推能力）、128K 词表（tiktoken BPE）、SwiGLU FFN。训练数据：15T token（主要是英文和代码）。指令版本：RLHF 流程使用 SFT + DPO + PPO 多阶段。工具调用：原生支持 JSON Schema 格式函数调用。与 Qwen2.5/Mistral 的定位对比：Meta 以开放权重为战略重点。",
    "depth_hint": "计算 LLaMA-3 70B 的 KV Cache 内存占用（batch=1，seq=8192），对比 GQA 与 MHA 的差异",
    "status": "pending"
  },
  {
    "id": 81,
    "title": "AWQ：激活感知的权重量化",
    "slug": "awq-quantization",
    "tags": [
      "工程实践",
      "推理优化",
      "量化"
    ],
    "brief": "GPTQ 的问题：逐层量化，忽略激活分布。AWQ（Lin 2023）的关键发现：并非所有权重同等重要，对应激活值较大的权重通道更重要（量化这些通道误差更大）。解决方案：对重要通道的权重乘以缩放系数 s（放大权重、缩小激活），使量化精度更高，量化后再除以 s（合并到下一层）。AWQ 不需要反向传播，只需校准数据的前向传播（~128 样本）。4-bit AWQ 困惑度接近 FP16，优于 GPTQ 4-bit。",
    "depth_hint": "推导 AWQ 的缩放系数搜索目标函数，对比 AWQ 与 GPTQ 在不同量化位宽下的精度",
    "status": "pending"
  },
  {
    "id": 82,
    "title": "模型水印：LLM 生成文本的可检测性",
    "slug": "llm-text-watermarking",
    "tags": [
      "工程实践",
      "安全",
      "水印"
    ],
    "brief": "为什么需要水印：区分人类写作与 AI 生成内容。绿色 token 水印（Kirchenbauer 2023）：将词表分为绿色集合（G）和红色集合（R），生成时软性偏向绿色 token（logits += δ）。检测：计算文本中绿色 token 的比例，超过阈值则为 AI 生成。关键性质：无法被不知道密钥的人绕过（密钥决定 G/R 划分）；但知道水印存在者可以通过改写绕过。语义不变性问题：水印不能嵌入到语义层面（翻译后消失）。",
    "depth_hint": "分析绿色 token 水印的统计检验方法，计算 FPR=1% 时所需的最短文本长度",
    "status": "pending"
  },
  {
    "id": 83,
    "title": "RLAIF：用 AI 反馈替代人类偏好标注",
    "slug": "rlaif-ai-feedback",
    "tags": [
      "底层原理",
      "对齐",
      "RLAIF"
    ],
    "brief": "RLHF 的瓶颈：人类标注成本高、一致性差（不同标注员偏好不同）。RLAIF（Bai 2022，Lee 2023）：用更强的 LLM（如 Claude）评判两个响应的优劣，替代人类偏好标注。技术细节：Prompt 设计对 AI 判断质量至关重要（Chain-of-thought 后再给出偏好判断）。Constitutional AI：RLAIF 的扩展，用原则列表指导 AI 判断。局限：AI 标注员会继承训练 LLM 的偏见；无法超越标注 LLM 的能力上界。",
    "depth_hint": "对比 RLAIF 与人类标注的一致性（Kappa 系数），分析 AI 标注员的偏见来源",
    "status": "pending"
  },
  {
    "id": 84,
    "title": "数据并行：DDP 与梯度同步的通信开销",
    "slug": "data-parallel-ddp",
    "tags": [
      "工程实践",
      "分布式训练",
      "数据并行"
    ],
    "brief": "数据并行：每个 GPU 持有完整模型副本，处理不同数据，梯度同步后参数更新相同。参数服务器（PS）架构：中心化参数存储，Worker 拉取参数、推送梯度，PS 成为通信瓶颈。AllReduce（Ring-AllReduce）：无中心节点，每个 GPU 既发送又接收，通信量 = 2(N-1)/N × 参数量（N 为 GPU 数），接近最优。PyTorch DDP：NCCL 后端 AllReduce，梯度通信与反向传播重叠（Backward Hook 触发通信）。通信量估算：175B 模型每步通信 ~350GB 梯度（FP16）。",
    "depth_hint": "推导 Ring-AllReduce 的通信复杂度公式，计算 8 卡 DDP 训练 7B 模型的通信效率",
    "status": "pending"
  },
  {
    "id": 85,
    "title": "Retrieval-Augmented Fine-tuning（RAFT）：领域知识注入策略",
    "slug": "raft-retrieval-finetuning",
    "tags": [
      "工程实践",
      "RAG",
      "微调"
    ],
    "brief": "标准 RAG 的问题：LLM 没有学会从文档中提取关键信息（只靠上下文学习，不够稳定）。RAFT（Zhang 2024）：在 SFT 数据中混合带有干扰文档的 QA 对，训练模型学会忽略无关文档、引用相关内容作答（CoT 格式）。训练数据构建：80% 含相关文档的正样本 + 20% 只含干扰文档的负样本（增强鲁棒性）。与 RAG 对比：RAFT 在领域专业任务（医疗、法律）上显著优于纯 RAG，因为模型学会了领域内的信息提取模式。",
    "depth_hint": "对比 RAFT 与标准 RAG 在领域专业问答中的引用准确率差异",
    "status": "pending"
  },
  {
    "id": 86,
    "title": "Speculative Decoding 的变体：Medusa、Eagle、Lookahead",
    "slug": "speculative-decoding-variants",
    "tags": [
      "工程实践",
      "推理优化",
      "解码"
    ],
    "brief": "原始 Speculative Decoding 的局限：需要独立的草稿模型（维护成本高，且加速比依赖草稿模型质量）。Medusa（Cai 2024）：在原始 LLM 之上添加多个预测头，每个头预测 k 步之后的 token，无需独立草稿模型，平均加速 2-3 倍。Eagle（Li 2024）：用 1 层 Transformer 作为草稿模型，输入 LLM 最后一层的隐藏状态，接受率更高（加速 3 倍）。Lookahead：n-gram 草稿，无需额外模型。",
    "depth_hint": "对比 Medusa、Eagle、原始 Speculative Decoding 在 LLaMA-2 70B 上的加速比与硬件要求",
    "status": "pending"
  },
  {
    "id": 87,
    "title": "对话状态管理：多轮上下文的截断与压缩策略",
    "slug": "conversation-state-management",
    "tags": [
      "工程实践",
      "推理",
      "对话系统"
    ],
    "brief": "多轮对话的上下文长度随轮次线性增长，最终超出模型上下文窗口。截断策略：先进先出（丢失重要的早期信息）、保留系统提示+最近 N 轮。摘要压缩：用 LLM 定期将早期对话压缩为摘要（内存损失 vs 长度节省）。KV Cache 复用：对话中不变的前缀（system prompt）可以预计算并缓存 KV，节省推理开销。Prompt Caching（Anthropic）：最长前缀匹配的 KV Cache 服务端复用，API 调用中重复前缀享受折扣。",
    "depth_hint": "计算不同截断策略下 100 轮对话的信息保留率，分析 KV Cache 前缀复用的成本节省",
    "status": "pending"
  },
  {
    "id": 88,
    "title": "SWE-bench：代码 Agent 的真实工程能力评测",
    "slug": "swe-bench-evaluation",
    "tags": [
      "模型解析",
      "评测",
      "Agent"
    ],
    "brief": "SWE-bench（Jimenez 2023）：从 GitHub 真实 issue 中收集 2294 个任务，要求模型修改代码库解决 issue 并通过测试。难度：需要理解大型代码库（平均 39K LOC），定位问题、修改多文件、不破坏现有测试。Lite 版本（300 tasks）用于快速评估。评测方法：自动化测试（Docker 隔离环境），无需人工评判。代表性系统：Devin（13.86%）、SWE-agent（12.47%）、OpenHands（41%，2024 年最新）。分析为何难：需要多步工具调用、上下文理解和工程判断。",
    "depth_hint": "分析 SWE-bench 的难点分布（单文件 vs 多文件修改），对比不同 Agent 架构在任务类型上的优劣",
    "status": "pending"
  },
  {
    "id": 89,
    "title": "神经网络的几何视角：流形假设与嵌入空间",
    "slug": "neural-network-geometry",
    "tags": [
      "底层原理",
      "表示学习",
      "几何"
    ],
    "brief": "流形假设（Manifold Hypothesis）：高维数据（图像、文本）实际分布在低维流形上。神经网络的作用：学习将流形展开（unfolding）为线性可分空间。Embedding 空间的几何性质：线性插值语义连续性（词类比关系的向量加减）、语义聚类结构。表示空间的各向异性问题：BERT 的 token 嵌入分布呈锥形（各向异性），余弦相似度失去意义（SimCSE 2021 解决了这一问题）。对比学习如何解决各向异性：均匀分布在超球面上。",
    "depth_hint": "分析 BERT 嵌入的各向异性问题，推导 SimCSE 的对比学习目标如何改善分布",
    "status": "pending"
  },
  {
    "id": 90,
    "title": "未来方向：Test-Time Training、World Models 与 LLM 开放问题",
    "slug": "llm-future-directions",
    "tags": [
      "模型解析",
      "前沿研究",
      "综述"
    ],
    "brief": "Test-Time Training（TTT）：在推理时对当前输入进行短暂梯度更新，动态适应分布偏移（TTT-RNN 将内存编码进模型权重）。World Models：LLM 是否学到了世界的因果结构，还是只是模式匹配（Bengio 的争论）。组合泛化（Compositional Generalization）：模型是否真正理解概念，还是只记忆了训练中的组合。长文本理解的质量：RULER 等基准测试显示模型在超长文本中存在注意力漂移。持续学习：如何在不灾难性遗忘的情况下更新知识。",
    "depth_hint": "分析当前 LLM 在组合泛化和因果推理上的失败案例，讨论 TTT 的理论基础与实际限制",
    "status": "pending"
  },
  {
    "id": 91,
    "title": "FlashAttention 的 IO 感知分块策略",
    "slug": "flashattention-io-aware-tiling",
    "tags": [
      "底层原理",
      "注意力机制",
      "GPU优化"
    ],
    "brief": "核心要点：FlashAttention 将 Q/K/V 按 SRAM 容量分块（典型块大小 Bc=M/(4d)），通过在线 softmax 算法避免 O(N²) 的 HBM 读写。分析 A100 上 SRAM(192KB) vs HBM(80GB) 的带宽差异（19TB/s vs 2TB/s），推导分块后 IO 复杂度从 O(N²d) 降至 O(N²d²/M)。",
    "depth_hint": "需推导在线 softmax 的数值稳定性修正公式，给出不同序列长度下的 FLOPs vs IO 瓶颈分析",
    "status": "pending"
  },
  {
    "id": 92,
    "title": "FlashAttention-2 的并行化改进",
    "slug": "flashattention-2-parallelism",
    "tags": [
      "底层原理",
      "注意力机制",
      "GPU优化"
    ],
    "brief": "核心要点：FlashAttention-2 将外层循环从 K/V 改为 Q 维度，使每个 warp 独立计算完整的 attention 输出，消除 warp 间同步。非因果 mask 下达到理论 FLOPs 的 72%（A100），因果 mask 通过跳过空块进一步提速约 1.7x。",
    "depth_hint": "需对比 FA1 和 FA2 的循环结构差异，给出 warp 级别的工作分配图和实测吞吐量对比",
    "status": "pending"
  },
  {
    "id": 93,
    "title": "FlashAttention-3 与 Hopper 架构适配",
    "slug": "flashattention-3-hopper-wgmma",
    "tags": [
      "底层原理",
      "注意力机制",
      "GPU优化"
    ],
    "brief": "核心要点：FlashAttention-3 利用 H100 的 WGMMA 指令和 TMA 异步数据搬运，实现 GEMM 与 softmax 的流水线重叠。通过 FP8 量化路径支持 E4M3 格式，在保持精度的前提下将 H100 利用率提升至 75%，峰值达 740 TFLOPs。",
    "depth_hint": "需解释 WGMMA 与 HMMA 的差异、ping-pong 调度策略，给出 FP16/FP8 精度对比数据",
    "status": "pending"
  },
  {
    "id": 94,
    "title": "GPU 内存层次与 Attention 的算术强度",
    "slug": "gpu-memory-hierarchy-attention-intensity",
    "tags": [
      "底层原理",
      "GPU优化",
      "注意力机制"
    ],
    "brief": "核心要点：Attention 的算术强度(AI)为 O(N)，随序列增长从 memory-bound 转为 compute-bound。A100 的 roofline 模型显示临界点约 N≈1024（FP16）。分析 SRAM/L2/HBM 三级存储对 kernel 设计的约束，解释为何标准 attention 在短序列时也慢。",
    "depth_hint": "需画 roofline 模型图，计算不同 N 和 d 下的算术强度，标注 memory-bound 与 compute-bound 区间",
    "status": "pending"
  },
  {
    "id": 95,
    "title": "在线 Softmax 算法：数值稳定的流式计算",
    "slug": "online-softmax-streaming",
    "tags": [
      "底层原理",
      "注意力机制",
      "数值计算"
    ],
    "brief": "核心要点：在线 softmax（Milakov & Gimelshein, 2018）在单次遍历中同步维护最大值 m 和指数和 l，避免三遍扫描。FlashAttention 将其扩展到分块矩阵乘，通过 rescaling 因子 e^(m_old - m_new) 合并不同块的 partial softmax 结果。",
    "depth_hint": "需给出三遍 softmax vs 在线 softmax 的伪代码对比，推导分块合并时的修正因子",
    "status": "pending"
  },
  {
    "id": 96,
    "title": "FlashAttention 反向传播的重计算策略",
    "slug": "flashattention-backward-recomputation",
    "tags": [
      "底层原理",
      "注意力机制",
      "训练优化"
    ],
    "brief": "核心要点：FlashAttention 反向传播不存储 O(N²) 的 attention 矩阵 P，而是仅保存 O(N) 的 logsumexp 值 L，反向时从 Q/K/V 重计算 P。这将内存从 O(N²) 降至 O(N)，而重计算的额外 FLOPs 被 HBM 读写减少所补偿。",
    "depth_hint": "需推导反向传播中 dQ/dK/dV 的分块计算公式，分析重计算 vs 存储的 IO-FLOPs 权衡",
    "status": "pending"
  },
  {
    "id": 97,
    "title": "Longformer 的滑动窗口与全局注意力",
    "slug": "longformer-sliding-window-global-attention",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "长序列"
    ],
    "brief": "核心要点：Longformer 将 O(N²) attention 分解为滑动窗口局部注意力（窗口大小 w=512）和少量全局 token 注意力。总复杂度 O(Nw + Ng·N)，其中 Ng 为全局 token 数。在 4096 长度下内存减少 8x，通过扩张滑窗（dilation=1）扩大感受野。",
    "depth_hint": "需给出稀疏注意力矩阵的可视化模式，对比不同窗口大小对下游任务（QA/分类）的影响",
    "status": "pending"
  },
  {
    "id": 98,
    "title": "BigBird 的随机注意力与图连通性证明",
    "slug": "bigbird-random-attention-graph-connectivity",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "理论分析"
    ],
    "brief": "核心要点：BigBird 结合局部窗口（w）、全局 token（g）和随机连接（r）三种稀疏模式。论文证明当 r=Ω(1) 时稀疏图是 O(N/w) 阶扩展图，保证信息在 O(N/w) 层内全局传播。理论上是 Turing 完备的，与稠密 attention 等价。",
    "depth_hint": "需解释 Erdős-Rényi 随机图的连通性引理，给出三种模式对 attention 矩阵稀疏率的量化分析",
    "status": "pending"
  },
  {
    "id": 99,
    "title": "稀疏注意力的 CUDA 实现：块稀疏矩阵乘",
    "slug": "sparse-attention-block-sparse-matmul",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "GPU优化"
    ],
    "brief": "核心要点：Triton 的块稀疏 kernel 将稀疏模式编码为 block mask（块大小 64×64），通过 gather/scatter 操作仅计算非零块的 QK^T。相比 dense attention，在 N=4096 时加速 2-3x，但在短序列时因索引开销反而更慢。",
    "depth_hint": "需给出块稀疏 attention 的 Triton 伪代码，对比 cuSPARSE 与自定义 kernel 的性能",
    "status": "pending"
  },
  {
    "id": 100,
    "title": "Performer 的 FAVOR+ 随机特征近似",
    "slug": "performer-favor-plus-random-features",
    "tags": [
      "底层原理",
      "线性注意力",
      "核方法"
    ],
    "brief": "核心要点：Performer 用随机特征映射 φ(x) = exp(ωᵀx - ||x||²/2)/√m 近似 softmax kernel，将 Attention 分解为 φ(Q)(φ(K)ᵀV)，复杂度从 O(N²d) 降至 O(Nmd)。正交随机特征（FAVOR+）将方差降低 O(1/m) 到 O(exp(-m))。",
    "depth_hint": "需推导 softmax kernel 的随机特征展开，对比正交 vs iid 特征的方差界，给出近似误差实验",
    "status": "pending"
  },
  {
    "id": 101,
    "title": "线性注意力的因果 Mask 与递推形式",
    "slug": "linear-attention-causal-recurrent-form",
    "tags": [
      "底层原理",
      "线性注意力",
      "序列建模"
    ],
    "brief": "核心要点：去掉 softmax 后，因果注意力可写成递推 S_t = S_{t-1} + φ(k_t)v_tᵀ，输出 o_t = φ(q_t)ᵀS_t / φ(q_t)ᵀz_t。S 是 d×d 的状态矩阵，推理复杂度 O(d²) per step，与 RNN 等价但可并行训练。这是 SSM 与 attention 统一视角的关键。",
    "depth_hint": "需推导从双向注意力到因果递推的变换过程，对比 chunk-wise 并行与逐步递推的效率",
    "status": "pending"
  },
  {
    "id": 102,
    "title": "Mamba 的选择性扫描机制",
    "slug": "mamba-selective-scan-mechanism",
    "tags": [
      "底层原理",
      "状态空间模型",
      "线性注意力"
    ],
    "brief": "核心要点：Mamba 使 S4 的参数（Δ, B, C）成为输入依赖的，通过选择性扫描让模型能动态过滤无关信息。离散化 Ā=exp(ΔA), B̄=ΔB 后执行并行前缀扫描（scan），硬件上通过 kernel fusion 避免 O(N) 中间状态的 HBM 读写。",
    "depth_hint": "需给出 S4 → Mamba 的参数化变化对比表，推导选择性扫描的并行前缀和实现",
    "status": "pending"
  },
  {
    "id": 103,
    "title": "S4 的 HiPPO 矩阵与长程记忆",
    "slug": "s4-hippo-matrix-long-range-memory",
    "tags": [
      "底层原理",
      "状态空间模型",
      "理论分析"
    ],
    "brief": "核心要点：S4 使用 HiPPO 矩阵 A_nk = -(2n+1)^{1/2}(2k+1)^{1/2}（n>k 时），使状态压缩历史输入在 Legendre 多项式基上的最优投影。该初始化解决了 SSM 的长程依赖问题，在 Path-X（16K 长度）任务上首次超过随机水平。",
    "depth_hint": "需推导 HiPPO-LegS 的连续时间 ODE，解释对角化后的频域卷积加速",
    "status": "pending"
  },
  {
    "id": 104,
    "title": "Mamba-2 的 SSD 层与矩阵分解",
    "slug": "mamba-2-ssd-structured-matrix",
    "tags": [
      "底层原理",
      "状态空间模型",
      "注意力机制"
    ],
    "brief": "核心要点：Mamba-2 证明选择性 SSM 等价于半可分矩阵（semiseparable matrix）乘法，将 SSM 与线性 attention 统一到结构化矩阵框架下。SSD 层支持 chunk-wise 并行：chunk 内用矩阵乘，chunk 间用递推，在 A100 上比 Mamba-1 快 2-8x。",
    "depth_hint": "需推导半可分矩阵的定义及其与因果线性注意力的等价关系，给出 chunk 大小对速度的影响",
    "status": "pending"
  },
  {
    "id": 105,
    "title": "GLA：门控线性注意力的硬件高效实现",
    "slug": "gated-linear-attention-hardware-efficient",
    "tags": [
      "底层原理",
      "线性注意力",
      "GPU优化"
    ],
    "brief": "核心要点：GLA 在线性注意力递推中加入标量门控 g_t ∈ (0,1)，状态更新为 S_t = g_t·S_{t-1} + k_t v_tᵀ。通过 chunk-wise 并行和 materialization 技巧，在 Triton 中实现高效 kernel，训练吞吐量接近 FlashAttention-2 的 Transformer。",
    "depth_hint": "需对比 GLA/RetNet/Mamba 的门控机制差异，给出 chunk-wise 算法的伪代码",
    "status": "pending"
  },
  {
    "id": 106,
    "title": "RetNet 的多尺度指数衰减与并行-递推双模式",
    "slug": "retnet-multi-scale-retention",
    "tags": [
      "底层原理",
      "线性注意力",
      "序列建模"
    ],
    "brief": "核心要点：RetNet 将衰减因子 γ 引入线性注意力，retention(Q,K,V) = (QKᵀ ⊙ D)V，其中 D_nm = γ^(n-m)。多头使用不同 γ 值形成多尺度记忆。支持并行（训练，O(N²)）、递推（推理，O(1) 内存）和 chunk-wise（混合）三种模式。",
    "depth_hint": "需推导三种计算模式的等价性，给出不同 γ 值对记忆长度的影响分析",
    "status": "pending"
  },
  {
    "id": 107,
    "title": "FlashAttention 在多头注意力中的分组查询优化",
    "slug": "flashattention-grouped-query-optimization",
    "tags": [
      "底层原理",
      "注意力机制",
      "推理优化"
    ],
    "brief": "核心要点：GQA 将 K/V 头数减少至 Q 头的 1/G（如 G=8），FlashAttention 通过共享 K/V 的分块 tile 减少 HBM 读取。在 Llama-2 70B（GQA, G=8）上，KV cache 减少 8x，FlashDecoding 将 batch decode 在长序列时加速 2-4x。",
    "depth_hint": "需对比 MHA/MQA/GQA 的 IO 开销公式，给出 FlashDecoding 的 split-K 并行策略",
    "status": "pending"
  },
  {
    "id": 108,
    "title": "PagedAttention：KV Cache 的虚拟内存管理",
    "slug": "paged-attention-kv-cache-virtual-memory",
    "tags": [
      "底层原理",
      "注意力机制",
      "推理优化"
    ],
    "brief": "核心要点：vLLM 的 PagedAttention 将 KV cache 按固定大小（如 16 tokens）分页存储，通过块表（block table）映射逻辑位置到物理地址。内存碎片从平均 60% 降至 <4%，批处理吞吐量提升 2-4x。支持 copy-on-write 实现 beam search 的 KV 共享。",
    "depth_hint": "需画出块表映射示意图，对比连续分配 vs 分页分配的内存利用率",
    "status": "pending"
  },
  {
    "id": 109,
    "title": "Ring Attention：跨设备的序列并行",
    "slug": "ring-attention-sequence-parallelism",
    "tags": [
      "底层原理",
      "注意力机制",
      "分布式计算"
    ],
    "brief": "核心要点：Ring Attention 将长序列切分到多设备，每设备持有局部 Q 块，K/V 块通过环形通信轮转。当 FLOP 时间 ≥ 通信时间（即 block_size·d ≥ bandwidth），通信完全被计算掩盖。理论上可处理 device_num × single_device_length 的序列。",
    "depth_hint": "需推导通信-计算重叠的条件，给出 8 卡 A100 上不同序列长度的实测加速比",
    "status": "pending"
  },
  {
    "id": 110,
    "title": "稀疏注意力模式的信息论分析",
    "slug": "sparse-attention-information-theoretic-analysis",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "理论分析"
    ],
    "brief": "核心要点：attention 矩阵的熵 H(A) 反映信息集中度，实证发现深层 attention 更稀疏（H 更低）。稀疏化相当于 top-k 截断，信息损失可用 KL(A_full || A_sparse) 量化。理论表明保留 O(N·log N) 个连接即可以 1-δ 概率保持 attention 输出的 ε-近似。",
    "depth_hint": "需给出 attention 熵随层深变化的实测曲线，推导 top-k 近似的误差界",
    "status": "pending"
  },
  {
    "id": 111,
    "title": "线性注意力的表达力局限与修补",
    "slug": "linear-attention-expressiveness-limitations",
    "tags": [
      "底层原理",
      "线性注意力",
      "理论分析"
    ],
    "brief": "核心要点：线性注意力等价于 d×d 状态的线性 RNN，状态容量有限，无法精确执行 copying/retrieval 等需要 O(N) 状态的任务。Based（2024）通过 Taylor 展开近似 softmax 提升短程表达力，HGRN2 通过外积状态扩展解决部分容量问题。",
    "depth_hint": "需构造线性注意力失败的 synthetic 任务示例，对比不同修补方案的 associative recall 准确率",
    "status": "pending"
  },
  {
    "id": 112,
    "title": "多头潜在注意力：DeepSeek-V2 的 KV 压缩",
    "slug": "multi-head-latent-attention-deepseek-v2",
    "tags": [
      "底层原理",
      "注意力机制",
      "推理优化"
    ],
    "brief": "核心要点：MLA 将 K/V 投影到低秩潜在空间 c_t = W_DKV·x_t（维度 d_c << n_h·d_h），推理时仅缓存 c_t。DeepSeek-V2 用 d_c=512 替代 128 头×128 维的 KV，cache 减少 93.3%。通过吸收矩阵合并避免显式还原 K/V。",
    "depth_hint": "需推导 MLA 的矩阵吸收技巧，对比 MLA/GQA/MQA 的 KV cache 大小和精度",
    "status": "pending"
  },
  {
    "id": 113,
    "title": "Performer 正交随机特征的方差分析",
    "slug": "performer-orthogonal-random-features-variance",
    "tags": [
      "底层原理",
      "线性注意力",
      "核方法"
    ],
    "brief": "核心要点：FAVOR+ 使用正交随机矩阵替代 iid 高斯矩阵生成特征映射，估计量方差从 O(1/m) 指数级降至 O(exp(-m/d))。通过 Gram-Schmidt 或 QR 分解获得正交基，每 d 个特征为一组正交块。当 m≥64 时近似误差已接近 float32 精度极限。",
    "depth_hint": "需推导正交 vs iid 特征的方差界对比，给出不同 m 值下 softmax 近似的 MSE 曲线",
    "status": "pending"
  },
  {
    "id": 114,
    "title": "因果卷积与 SSM 的频域加速",
    "slug": "causal-convolution-ssm-fft-acceleration",
    "tags": [
      "底层原理",
      "状态空间模型",
      "数值计算"
    ],
    "brief": "核心要点：离散化 SSM 等价于因果卷积 y = K * x，卷积核 K_t = C·Ā^t·B̄。训练时用 FFT 在 O(N log N) 内完成，推理时切换为递推形式。S4 通过对角化 A 将卷积核计算从 O(N·d²) 降至 O(N·d)，DSS 进一步简化为纯对角 SSM。",
    "depth_hint": "需推导 SSM → 卷积的变换过程，对比时域递推与频域卷积的数值稳定性",
    "status": "pending"
  },
  {
    "id": 115,
    "title": "Dilated Attention：跨尺度的稀疏注意力",
    "slug": "dilated-attention-multi-scale-sparse",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "长序列"
    ],
    "brief": "核心要点：LongNet 的 Dilated Attention 以指数级增长的间隔采样注意力位置（dilation rate 1,2,4,...），不同头使用不同 dilation 覆盖多尺度依赖。总复杂度 O(N·log N)，理论上支持 1B token 序列。实质是 attention 版的空洞卷积。",
    "depth_hint": "需给出多头 dilation 分配策略的示意图，分析不同 dilation 组合对长程依赖捕获的影响",
    "status": "pending"
  },
  {
    "id": 116,
    "title": "Hash-based 稀疏注意力：Reformer 的 LSH 机制",
    "slug": "reformer-lsh-attention-mechanism",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "近似算法"
    ],
    "brief": "核心要点：Reformer 用 LSH（局部敏感哈希）将 Q/K 映射到相同桶中，仅在桶内计算注意力。每轮哈希复杂度 O(N·log N)，多轮（n_rounds=8）降低碰撞遗漏概率。可逆残差网络进一步将激活内存从 O(N·L) 降至 O(N)。",
    "depth_hint": "需解释 angular LSH 的哈希函数设计，推导碰撞概率与 attention 权重的关系",
    "status": "pending"
  },
  {
    "id": 117,
    "title": "FlashAttention 与稀疏模式的融合",
    "slug": "flashattention-sparse-pattern-fusion",
    "tags": [
      "底层原理",
      "注意力机制",
      "稀疏注意力"
    ],
    "brief": "核心要点：FlashAttention 通过 block mask 支持任意稀疏模式（滑动窗口、因果、固定模式），对全零块直接跳过 GEMM 计算。FlexAttention（PyTorch 2.5）将 mask 函数 JIT 编译为自定义 kernel，用户可组合任意稀疏模式而无需手写 CUDA。",
    "depth_hint": "需给出 FlexAttention 的 score_mod/mask_mod API 示例，对比手写稀疏 kernel 的性能差距",
    "status": "pending"
  },
  {
    "id": 118,
    "title": "RWKV 的线性 Attention 与 Token Shift",
    "slug": "rwkv-linear-attention-token-shift",
    "tags": [
      "底层原理",
      "线性注意力",
      "序列建模"
    ],
    "brief": "核心要点：RWKV 将 attention 替换为 WKV 算子：wkv_t = Σ e^{-(t-i)w+k_i}·v_i / Σ e^{-(t-i)w+k_i}，其中 w 是可学习的衰减率。Token shift（时间混合）r_t = μ·x_t + (1-μ)·x_{t-1} 引入局部上下文。RWKV-6 进一步支持数据依赖的衰减。",
    "depth_hint": "需推导 WKV 递推计算的数值稳定技巧，对比 RWKV-5/6 与 Mamba 在 language modeling 上的 perplexity",
    "status": "pending"
  },
  {
    "id": 119,
    "title": "注意力 Sink 现象与 StreamingLLM",
    "slug": "attention-sink-streaming-llm",
    "tags": [
      "底层原理",
      "注意力机制",
      "长序列"
    ],
    "brief": "核心要点：LLM 推理时前几个 token 获得异常高的 attention 权重（attention sink），即使它们语义无关。StreamingLLM 保留开头 4 个 sink token + 滑动窗口（如 1024），实现无限长度流式推理，perplexity 稳定在窗口大小对应水平，而纯滑窗方案在窗口外立即崩溃。",
    "depth_hint": "需可视化 attention sink 在不同模型（Llama/Mistral）中的分布，分析 sink token 数量对 PPL 的影响",
    "status": "pending"
  },
  {
    "id": 120,
    "title": "Mixture of Attention Heads：动态稀疏注意力路由",
    "slug": "mixture-of-attention-heads-routing",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "模型架构"
    ],
    "brief": "核心要点：将 MoE 思想应用于注意力头，每个 token 通过 router 选择 k 个活跃头（如 k=2/8），非活跃头跳过计算。SwitchHead（2024）在保持质量的前提下将 attention 计算减少 4x。路由策略包括 token-level 和 query-level 两种粒度。",
    "depth_hint": "需对比 token-level vs query-level 路由的 load balancing 策略，给出头选择频率分布",
    "status": "pending"
  },
  {
    "id": 121,
    "title": "MoE 路由机制：Top-k 门控的数学形式",
    "slug": "moe-topk-gating-math",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：MoE 层中 Top-k 路由的完整数学描述，包括门控函数 G(x)=Softmax(W_g·x) 的 Top-k 截断、专家输出的加权求和 y=ΣG_i(x)·E_i(x)，以及 k=1 与 k=2 对模型容量和训练稳定性的影响。分析 token-to-expert 的分配矩阵性质。",
    "depth_hint": "推导 Top-1 与 Top-2 路由的梯度传播差异，给出门控稀疏化的具体数值实验",
    "status": "pending"
  },
  {
    "id": 122,
    "title": "MoE 负载均衡损失的设计与调参",
    "slug": "moe-load-balancing-loss",
    "tags": [
      "底层原理",
      "MoE",
      "负载均衡"
    ],
    "brief": "核心要点：辅助负载均衡损失 L_aux=α·N·Σf_i·P_i 的推导，其中 f_i 是专家 i 实际接收的 token 比例，P_i 是路由概率均值。分析 α 系数从 0.01 到 1.0 对训练稳定性的影响，以及过大 α 导致路由退化为均匀分布的问题。",
    "depth_hint": "给出不同 α 值下专家利用率和模型 loss 的对比实验数据",
    "status": "pending"
  },
  {
    "id": 123,
    "title": "Switch Transformer：单专家路由的简化设计",
    "slug": "switch-transformer-single-expert-routing",
    "tags": [
      "底层原理",
      "MoE",
      "Switch Transformer"
    ],
    "brief": "核心要点：Switch Transformer 将 Top-2 简化为 Top-1 路由，每个 token 只发送到一个专家，减少计算和通信开销。分析其容量因子 CF 的定义 CF=(tokens_per_expert)/(total_tokens/N)，CF=1.0~1.25 的选择对溢出 token 丢弃率的影响。",
    "depth_hint": "复现 Switch-Base 128 专家 vs T5-Base 的预训练加速比和下游任务对比",
    "status": "pending"
  },
  {
    "id": 124,
    "title": "Mixtral 8x7B 架构：稀疏 MoE 的工程实现",
    "slug": "mixtral-8x7b-architecture",
    "tags": [
      "底层原理",
      "MoE",
      "Mixtral"
    ],
    "brief": "核心要点：Mixtral 8x7B 的具体架构——每层 8 个 FFN 专家、Top-2 路由、总参数 46.7B 但每 token 仅激活 12.9B。分析其与 Mistral 7B 共享的注意力层设计，以及专家 FFN 的 SwiGLU 结构。对比同等激活参数的 dense 模型性能。",
    "depth_hint": "拆解 Mixtral 各层参数量计算，给出推理时显存占用与 dense 模型的对比",
    "status": "pending"
  },
  {
    "id": 125,
    "title": "专家坍缩问题：路由退化的成因与对策",
    "slug": "moe-expert-collapse",
    "tags": [
      "底层原理",
      "MoE",
      "训练稳定性"
    ],
    "brief": "核心要点：MoE 训练中常见的专家坍缩现象——少数专家接收大部分 token，其余专家权重停止更新。分析正反馈循环机制：高频专家梯度更大→参数更优→吸引更多 token。介绍 dropout、噪声注入、专家并行初始化等缓解方案。",
    "depth_hint": "可视化训练过程中专家负载分布的演变，给出坍缩检测指标",
    "status": "pending"
  },
  {
    "id": 126,
    "title": "Noisy Top-k Gating：路由探索的噪声机制",
    "slug": "noisy-topk-gating",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Shazeer 等人提出的噪声门控 H(x)=W_g·x + N·Softplus(W_noise·x)，其中 N~N(0,1)。噪声项在训练时鼓励探索不同专家，推理时关闭。分析噪声幅度对路由多样性和训练收敛速度的权衡，以及与 ε-greedy 探索策略的对比。",
    "depth_hint": "推导噪声门控的梯度估计，对比有无噪声时的专家利用率曲线",
    "status": "pending"
  },
  {
    "id": 127,
    "title": "Expert Capacity 与 Token 丢弃机制",
    "slug": "moe-expert-capacity-token-dropping",
    "tags": [
      "底层原理",
      "MoE",
      "负载均衡"
    ],
    "brief": "核心要点：MoE 中专家容量 C=CF·(T/N) 的设定，当某专家接收 token 超过 C 时多余 token 被丢弃（通过残差连接跳过该层）。分析 CF 过小导致信息丢失、CF 过大导致计算浪费的权衡。ST-MoE 中动态容量调整的改进方案。",
    "depth_hint": "统计不同 CF 值下的 token 丢弃率及其对下游任务 accuracy 的影响",
    "status": "pending"
  },
  {
    "id": 128,
    "title": "GShard：MoE 的分布式训练框架",
    "slug": "gshard-distributed-moe-training",
    "tags": [
      "底层原理",
      "MoE",
      "分布式训练"
    ],
    "brief": "核心要点：GShard 将 MoE 层的专家分布到不同设备上，实现 Expert Parallelism。分析 All-to-All 通信原语在 token 分发和结果收集中的开销，以及 GShard 的 random routing 辅助策略——Top-2 中第二专家按概率随机选择以改善负载均衡。",
    "depth_hint": "给出 All-to-All 通信量公式，对比不同设备数下的通信/计算比",
    "status": "pending"
  },
  {
    "id": 129,
    "title": "Expert Parallelism 的通信瓶颈分析",
    "slug": "expert-parallelism-communication-bottleneck",
    "tags": [
      "底层原理",
      "MoE",
      "分布式系统"
    ],
    "brief": "核心要点：Expert Parallelism 中每个 MoE 层需要两次 All-to-All 通信（分发 token 和收集结果），通信量为 O(B·S·d/P) 其中 P 为设备数。分析 NVLink vs InfiniBand 带宽对 MoE 扩展性的限制，以及通信与计算的 overlap 策略。",
    "depth_hint": "计算 8/16/64 GPU 下 All-to-All 延迟占比，给出 overlap pipeline 设计",
    "status": "pending"
  },
  {
    "id": 130,
    "title": "ST-MoE：稳定训练的大规模 MoE",
    "slug": "st-moe-stable-training",
    "tags": [
      "底层原理",
      "MoE",
      "训练稳定性"
    ],
    "brief": "核心要点：ST-MoE 论文总结了 MoE 训练不稳定的系统性原因和解决方案。关键技术：Router z-loss L_z=1/B·Σ(log Σexp(x_i))² 惩罚门控 logit 过大、辅助损失权重调度、精度敏感层使用 float32。分析这些改进如何将训练失败率从 >30% 降至接近 0%。",
    "depth_hint": "推导 router z-loss 的梯度形式，给出训练稳定性前后的 loss 曲线对比",
    "status": "pending"
  },
  {
    "id": 131,
    "title": "Base Layer：线性分配的确定性路由",
    "slug": "base-layer-linear-assignment-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Base Layer 将 token-to-expert 分配建模为线性分配问题（匈牙利算法），保证每个专家精确接收相同数量的 token，彻底消除负载不均。分析其 O(n³) 复杂度的实际可行性（在 token 数较少时），以及与 Top-k 路由在质量上的对比。",
    "depth_hint": "给出线性分配的对偶形式和 Sinkhorn 近似算法的迭代收敛分析",
    "status": "pending"
  },
  {
    "id": 132,
    "title": "Expert Choice 路由：让专家选择 Token",
    "slug": "expert-choice-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Expert Choice 反转了传统路由方向——由专家选择 Top-k 个 token 而非 token 选择专家。每个专家固定处理 k=T·CF/N 个 token，天然实现完美负载均衡。分析这种设计下单个 token 可能被 0 或多个专家处理的异质性及其对模型表达力的影响。",
    "depth_hint": "对比 Expert Choice 与 Token Choice 在相同参数量下的训练 loss 和下游任务指标",
    "status": "pending"
  },
  {
    "id": 133,
    "title": "Hash Layer：无参数路由的固定映射",
    "slug": "hash-layer-fixed-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Hash Layer 使用确定性哈希函数（如对 token ID 取模）将 token 映射到专家，完全消除可学习路由器。分析其优势（零路由开销、完美负载均衡、无训练不稳定）和劣势（无法根据语义调整分配），以及在不同规模下与学习型路由的性能差距。",
    "depth_hint": "实验对比 Hash 路由与 Top-2 路由在 8/64/128 专家下的困惑度差异",
    "status": "pending"
  },
  {
    "id": 134,
    "title": "Soft MoE：可微分的专家混合",
    "slug": "soft-moe-differentiable-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Soft MoE（Google 2023）用可微分的 Softmax 分配替代离散 Top-k 路由。每个专家接收所有 token 的加权组合 x̃_j=Σ D_{ij}·x_i，D 为分配矩阵。消除了 token 丢弃和负载不均问题，但增加了计算量。分析其与标准 MoE 在视觉任务中的对比。",
    "depth_hint": "推导 Soft MoE 的前向计算复杂度，对比离散路由的理论计算量",
    "status": "pending"
  },
  {
    "id": 135,
    "title": "MoE 中 FFN 专家的功能分化",
    "slug": "moe-expert-specialization-analysis",
    "tags": [
      "底层原理",
      "MoE",
      "可解释性"
    ],
    "brief": "核心要点：分析训练后 MoE 各专家的功能分化现象——某些专家偏好处理特定语言、特定词性或特定语义领域的 token。通过统计各专家实际接收的 token 分布，揭示路由器学到的隐式聚类模式。探讨分化程度与模型性能的关系。",
    "depth_hint": "可视化不同层不同专家的 token 类型分布热力图，量化专家间的 KL 散度",
    "status": "pending"
  },
  {
    "id": 136,
    "title": "Mixtral 的路由模式：逐层专家分配分析",
    "slug": "mixtral-routing-pattern-analysis",
    "tags": [
      "底层原理",
      "MoE",
      "Mixtral"
    ],
    "brief": "核心要点：深入分析 Mixtral 8x7B 各层的路由行为——浅层路由偏向语法特征（词性、位置），深层路由偏向语义特征（主题、实体类型）。统计各层的专家利用率方差、路由熵以及跨层路由一致性。揭示不同层对专家多样性的需求差异。",
    "depth_hint": "计算 Mixtral 各层的路由熵 H=-Σp_i·log(p_i)，绘制逐层变化曲线",
    "status": "pending"
  },
  {
    "id": 137,
    "title": "DeepSeek-MoE：细粒度专家分割策略",
    "slug": "deepseek-moe-fine-grained-experts",
    "tags": [
      "底层原理",
      "MoE",
      "DeepSeek"
    ],
    "brief": "核心要点：DeepSeek-MoE 将传统 MoE 的 N 个大专家拆分为 mN 个小专家（m=细粒度因子），增加组合灵活性。同时引入共享专家（始终激活）处理通用知识。分析 16B 总参数/2.8B 激活参数配置下，细粒度分割对专家利用率和性能的提升。",
    "depth_hint": "对比不同细粒度因子 m=1,2,4 下的专家激活频率分布和下游任务指标",
    "status": "pending"
  },
  {
    "id": 138,
    "title": "共享专家机制：MoE 中的通用知识锚定",
    "slug": "moe-shared-expert-mechanism",
    "tags": [
      "底层原理",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：DeepSeek-MoE 和 DeepSeek-V2 引入的共享专家（shared expert）始终参与计算，不受路由控制。其作用是捕获所有 token 共有的通用表示，减少路由专家间的知识冗余。分析共享专家数量（1~4个）对模型容量和训练效率的影响。",
    "depth_hint": "对比有无共享专家时路由专家的激活模式差异和整体 loss 变化",
    "status": "pending"
  },
  {
    "id": 139,
    "title": "MoE 推理优化：专家缓存与预取策略",
    "slug": "moe-inference-expert-caching",
    "tags": [
      "底层原理",
      "MoE",
      "推理优化"
    ],
    "brief": "核心要点：MoE 模型推理时所有专家权重需驻留显存（如 Mixtral 8x7B 需约 90GB），远超实际激活量。分析 expert offloading 策略——将非活跃专家卸载到 CPU/SSD，按路由预测预取即将使用的专家。LRU 缓存的命中率与推理延迟的权衡分析。",
    "depth_hint": "测量不同缓存大小（2/4/6 专家驻留 GPU）下的推理吞吐量和延迟",
    "status": "pending"
  },
  {
    "id": 140,
    "title": "MoE 量化：专家级别的异构量化",
    "slug": "moe-heterogeneous-quantization",
    "tags": [
      "底层原理",
      "MoE",
      "量化"
    ],
    "brief": "核心要点：MoE 中不同专家的重要性不同（高频专家对精度更敏感），可对不同专家使用不同位宽的量化。分析 QMoE 等方法将高频专家保持 FP16 而低频专家压缩至 2-bit 的策略，以及 Mixtral 在 GPTQ/AWQ 量化下的精度-显存权衡。",
    "depth_hint": "给出 Mixtral 8x7B 在 4-bit/3-bit/2-bit 量化下的困惑度和显存占用数据",
    "status": "pending"
  },
  {
    "id": 141,
    "title": "V-MoE：视觉 Transformer 中的 MoE",
    "slug": "vmoe-vision-moe",
    "tags": [
      "底层原理",
      "MoE",
      "视觉模型"
    ],
    "brief": "核心要点：V-MoE 将 MoE 应用于 Vision Transformer，在 ViT 的 FFN 层引入条件计算。分析视觉 token（patch）的路由模式——不同专家倾向处理不同空间位置或纹理特征的 patch。Batch Priority Routing（BPR）确保高优先级 token 不被丢弃。",
    "depth_hint": "可视化 V-MoE 各专家对图像不同区域 patch 的偏好热力图",
    "status": "pending"
  },
  {
    "id": 142,
    "title": "MoE 层的位置选择：全层 vs 交替层",
    "slug": "moe-layer-placement-strategy",
    "tags": [
      "底层原理",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：MoE 层的放置策略对性能影响显著——每层都用 MoE vs 每隔一层使用 MoE vs 仅在特定层使用。分析 Switch Transformer 的每隔一层策略、Mixtral 的全层策略、以及 ST-MoE 发现的最优放置模式。讨论 MoE 层密度与模型质量/效率的关系。",
    "depth_hint": "对比每 1/2/4 层使用 MoE 时的参数效率和预训练困惑度",
    "status": "pending"
  },
  {
    "id": 143,
    "title": "Auxiliary Loss 变体：从 Importance Loss 到 z-loss",
    "slug": "moe-auxiliary-loss-variants",
    "tags": [
      "底层原理",
      "MoE",
      "训练技巧"
    ],
    "brief": "核心要点：系统梳理 MoE 辅助损失的演进：Importance Loss（惩罚路由概率的变异系数 CV）、Load Loss（惩罚实际分配的不均衡）、二者联合使用、以及 ST-MoE 的 router z-loss。分析各损失函数的梯度行为和对路由器学习动态的不同影响。",
    "depth_hint": "推导各辅助损失对门控 logit 的梯度表达式，对比其正则化效果",
    "status": "pending"
  },
  {
    "id": 144,
    "title": "Megablocks：MoE 的高效 GPU 实现",
    "slug": "megablocks-efficient-moe-gpu",
    "tags": [
      "底层原理",
      "MoE",
      "系统实现"
    ],
    "brief": "核心要点：Megablocks 使用 block-sparse 矩阵运算替代传统 MoE 的 token-permutation + batched-matmul 实现，消除 padding 浪费。分析 dMoE（dropless MoE）如何在不丢弃 token 的前提下保持高 GPU 利用率，以及 block-sparse GEMM 的 Triton 内核设计。",
    "depth_hint": "对比 Megablocks 与 Tutel/FastMoE 在不同专家数下的训练吞吐量",
    "status": "pending"
  },
  {
    "id": 145,
    "title": "Token 丢弃 vs Token 不丢弃的 MoE 对比",
    "slug": "moe-token-drop-vs-dropless",
    "tags": [
      "底层原理",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：传统 MoE（Switch/GShard）使用容量因子丢弃溢出 token，而 dropless MoE（Megablocks/Expert Choice）通过不同机制避免丢弃。对比两种范式在训练稳定性、推理延迟可预测性、以及最终模型质量上的差异。分析 token 丢弃对长序列任务的特殊影响。",
    "depth_hint": "设计对照实验，测量 token 丢弃率 0%/5%/10% 对翻译和摘要任务 BLEU/ROUGE 的影响",
    "status": "pending"
  },
  {
    "id": 146,
    "title": "MoE 的 Upcycling：从 Dense 到稀疏的初始化",
    "slug": "moe-upcycling-dense-to-sparse",
    "tags": [
      "底层原理",
      "MoE",
      "训练技巧"
    ],
    "brief": "核心要点：MoE upcycling 将预训练好的 dense 模型转换为 MoE 模型——复制 FFN 层参数初始化多个专家，然后继续训练。分析 Sparse Upcycling 论文中的关键发现：初始化后路由器随机化的重要性、继续训练的 token 预算需求、以及与从头训练 MoE 的性能对比。",
    "depth_hint": "给出 upcycling T5-Base 为 8 专家 MoE 的训练曲线和计算量对比",
    "status": "pending"
  },
  {
    "id": 147,
    "title": "Sinkhorn 路由：最优传输视角的负载均衡",
    "slug": "sinkhorn-routing-optimal-transport",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：将 token-to-expert 分配建模为最优传输问题，使用 Sinkhorn 算法迭代求解双随机矩阵。与 Top-k 路由 + 辅助损失的方式不同，Sinkhorn 路由在约束满足的前提下最大化分配质量。分析 Sinkhorn 迭代次数与路由质量的关系，以及其计算开销。",
    "depth_hint": "推导 Sinkhorn 迭代的矩阵更新公式，给出 5/10/20 次迭代的收敛精度",
    "status": "pending"
  },
  {
    "id": 148,
    "title": "MoE 的扩展规律：稀疏模型的 Scaling Law",
    "slug": "moe-scaling-laws",
    "tags": [
      "底层原理",
      "MoE",
      "Scaling Law"
    ],
    "brief": "核心要点：MoE 的 scaling 行为不同于 dense 模型——增加专家数量的收益随规模递减。分析 Clark et al. 和 Switch Transformer 论文中的发现：专家数从 8→128 带来显著提升，但 128→512 收益趋缓。讨论 granularity（专家大小 vs 专家数量）对 scaling 的影响。",
    "depth_hint": "拟合专家数-困惑度的 power law 曲线，给出不同计算预算下的最优专家数",
    "status": "pending"
  },
  {
    "id": 149,
    "title": "Mixture-of-Depths：动态计算深度分配",
    "slug": "mixture-of-depths",
    "tags": [
      "底层原理",
      "MoE",
      "条件计算"
    ],
    "brief": "核心要点：Mixture-of-Depths（MoD）将 MoE 的路由思想从「选择哪个专家」推广到「是否经过该层」。每层通过路由器决定 token 是走完整计算还是跳过（残差直连），实现计算量的动态分配。分析 Top-k 容量控制和训练时的 straight-through 估计器设计。",
    "depth_hint": "对比 MoD 与标准 Transformer 在相同 FLOPs 下的性能，给出层跳过率统计",
    "status": "pending"
  },
  {
    "id": 150,
    "title": "MoE 与 MoA 的统一：专家粒度的设计空间",
    "slug": "moe-moa-unified-design-space",
    "tags": [
      "底层原理",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：将 Mixture of Experts（FFN 级路由）和 Mixture of Attention Heads（注意力头级路由）纳入统一框架——条件计算的粒度从整层、子层（FFN/Attn）、到单个头/专家。分析不同粒度下的路由开销、负载均衡难度和表达力，以及 MoE+MoA 联合使用的架构设计。",
    "depth_hint": "设计消融实验，对比 FFN-only MoE、Attn-only MoA、联合 MoE+MoA 的效果",
    "status": "pending"
  },
  {
    "id": 151,
    "title": "Tensor Parallel 的数学分解",
    "slug": "tensor-parallel-math",
    "tags": [
      "底层原理",
      "分布式训练",
      "模型并行"
    ],
    "brief": "核心要点：Tensor Parallel 将矩阵乘法 Y=XA 按列或行切分到多卡，列切分时每卡计算 Y_i=XA_i 无需通信，行切分需 AllReduce 聚合。分析 MLP 层的 Column-then-Row 切分策略，每层仅需 2 次 AllReduce，通信量为 2bsh，与隐藏维度无关。",
    "depth_hint": "推导列切分和行切分的通信量公式，画出 MLP 层的张量切分示意图",
    "status": "pending"
  },
  {
    "id": 152,
    "title": "Self-Attention 的 Tensor Parallel 切分",
    "slug": "tensor-parallel-attention",
    "tags": [
      "底层原理",
      "分布式训练",
      "注意力机制"
    ],
    "brief": "核心要点：多头注意力天然支持按 head 维度切分，每卡独立计算 h/p 个 head 的 QKV 投影和注意力，输出投影采用行切分后 AllReduce。GQA/MQA 场景下 KV head 数少于 TP 度时需广播或复制，分析不同注意力变体的 TP 效率差异。",
    "depth_hint": "对比 MHA/GQA/MQA 三种模式下 TP 切分的通信量和负载均衡",
    "status": "pending"
  },
  {
    "id": 153,
    "title": "流水线并行的微批次调度",
    "slug": "pipeline-parallel-microbatch",
    "tags": [
      "底层原理",
      "分布式训练",
      "流水线并行"
    ],
    "brief": "核心要点：Pipeline Parallel 将模型按层切分到不同设备，朴素实现 bubble 率为 (p-1)/m，其中 p 为流水线级数、m 为微批次数。GPipe 采用同步调度，所有微批次前向完成后统一反向；bubble 时间占比约 (p-1)/(m+p-1)，当 m≥4p 时 bubble 率低于 6%。",
    "depth_hint": "画出 GPipe 的时间线图，推导 bubble 率公式，给出不同 m/p 比值下的效率表格",
    "status": "pending"
  },
  {
    "id": 154,
    "title": "1F1B 调度：流水线并行的内存优化",
    "slug": "pipeline-1f1b-schedule",
    "tags": [
      "底层原理",
      "分布式训练",
      "流水线并行"
    ],
    "brief": "核心要点：PipeDream 的 1F1B（One Forward One Backward）调度在稳态阶段交替执行前向和反向，峰值激活内存从 GPipe 的 O(m) 降至 O(p)。分析 warmup 阶段的填充策略、权重版本管理问题，以及与同步训练语义的差异。",
    "depth_hint": "对比 GPipe 和 1F1B 的内存曲线，分析权重不一致对收敛的影响",
    "status": "pending"
  },
  {
    "id": 155,
    "title": "交错式流水线：Megatron-LM 的虚拟阶段",
    "slug": "interleaved-pipeline-stages",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "流水线并行"
    ],
    "brief": "核心要点：Megatron-LM 将每个设备分配多个非连续层（虚拟阶段），将 bubble 率从 (p-1)/m 降至 (p-1)/(vm)，v 为虚拟阶段数。代价是通信量增加 v 倍，因为每个微批次需要在设备间多次传输。当 v=L/p 时 bubble 趋近于零但通信开销最大。",
    "depth_hint": "推导 interleaved schedule 的 bubble 率和通信量，给出 v 的最优选择分析",
    "status": "pending"
  },
  {
    "id": 156,
    "title": "ZeRO Stage 1：优化器状态分片",
    "slug": "zero-stage1-optimizer-partition",
    "tags": [
      "底层原理",
      "ZeRO",
      "显存优化"
    ],
    "brief": "核心要点：Adam 优化器对每个参数维护 fp32 参数副本、一阶动量和二阶动量，总计 12 字节/参数。ZeRO Stage 1 将优化器状态按参数组均匀分片到 N 个 GPU，每卡仅存 12Ψ/N 字节。反向传播后通过 Reduce-Scatter 聚合梯度，各卡独立更新分片参数后 AllGather 同步。",
    "depth_hint": "计算 7B 模型在不同 GPU 数下的优化器内存，对比 DDP 基线",
    "status": "pending"
  },
  {
    "id": 157,
    "title": "ZeRO Stage 2：梯度分片的通信分析",
    "slug": "zero-stage2-gradient-partition",
    "tags": [
      "底层原理",
      "ZeRO",
      "显存优化"
    ],
    "brief": "核心要点：Stage 2 在 Stage 1 基础上将梯度也分片，每卡仅保留对应优化器分片的梯度，显存节省从 4Ψ+12Ψ/N 降至 2Ψ+(2+12)Ψ/N。关键在于反向传播时使用 Reduce-Scatter 替代 AllReduce，通信量保持 2Ψ 不变，与标准 DDP 等价。",
    "depth_hint": "推导 Stage 2 的通信量等价于 AllReduce，画出梯度分片的数据流",
    "status": "pending"
  },
  {
    "id": 158,
    "title": "ZeRO Stage 3：参数分片与通信开销",
    "slug": "zero-stage3-parameter-partition",
    "tags": [
      "底层原理",
      "ZeRO",
      "显存优化"
    ],
    "brief": "核心要点：Stage 3 将模型参数也分片，每卡仅存 (2+2+12)Ψ/N 字节。前向和反向时通过 AllGather 临时收集完整参数，用完即释放。通信量增至 3Ψ（前向 AllGather Ψ + 反向 AllGather Ψ + 梯度 Reduce-Scatter Ψ），比 DDP 多 50%。",
    "depth_hint": "推导三个 Stage 的显存和通信量公式，制表对比，分析适用场景",
    "status": "pending"
  },
  {
    "id": 159,
    "title": "ZeRO-Offload：GPU-CPU 混合训练",
    "slug": "zero-offload-gpu-cpu",
    "tags": [
      "底层原理",
      "ZeRO",
      "异构计算"
    ],
    "brief": "核心要点：ZeRO-Offload 将优化器状态和梯度计算卸载到 CPU，GPU 仅执行前向和反向。数据流设计为单向环路：GPU 计算梯度→PCIe 传至 CPU→CPU 执行 Adam 更新→更新后参数传回 GPU。CPU 计算与 GPU 计算重叠，PCIe 3.0 x16 带宽 16GB/s 成为瓶颈。",
    "depth_hint": "分析 PCIe 带宽对训练吞吐的影响，给出不同模型大小下的效率曲线",
    "status": "pending"
  },
  {
    "id": 160,
    "title": "ZeRO-Infinity：NVMe 极限扩展",
    "slug": "zero-infinity-nvme",
    "tags": [
      "底层原理",
      "ZeRO",
      "异构计算"
    ],
    "brief": "核心要点：ZeRO-Infinity 将 ZeRO-Offload 扩展到 NVMe SSD，支持万亿参数模型训练。采用 infinity offload engine 管理 GPU-CPU-NVMe 三级存储，通过预取和流水线化隐藏 I/O 延迟。NVMe 带宽 3-6 GB/s，需要精心设计 prefetch 策略使 I/O 与计算重叠。",
    "depth_hint": "分析三级存储的带宽层次，推导 prefetch window 大小的选择策略",
    "status": "pending"
  },
  {
    "id": 161,
    "title": "Megatron-LM 的 3D 并行架构",
    "slug": "megatron-3d-parallelism",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "分布式训练"
    ],
    "brief": "核心要点：Megatron-LM 组合 TP（张量并行）、PP（流水线并行）和 DP（数据并行）形成 3D 并行。在 DGX A100 集群上，TP 限制在单节点 8 卡（NVLink 600GB/s），PP 跨节点（IB 200Gb/s），DP 覆盖剩余维度。总 GPU 数 = tp × pp × dp，三个维度的映射决定通信效率。",
    "depth_hint": "给出 3D 并行在不同集群拓扑下的最优配置，分析通信瓶颈",
    "status": "pending"
  },
  {
    "id": 162,
    "title": "Megatron-LM 序列并行",
    "slug": "megatron-sequence-parallel",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "序列并行"
    ],
    "brief": "核心要点：Megatron-LM 的序列并行将 LayerNorm 和 Dropout 沿序列维度切分，与 Tensor Parallel 互补。TP 区域（Attention、MLP）按 hidden 维度切分，非 TP 区域按 sequence 维度切分，两者之间通过 AllGather/Reduce-Scatter 转换。激活内存从 O(sbh) 降至 O(sbh/t)。",
    "depth_hint": "画出序列并行与张量并行的切换点，推导激活内存节省量",
    "status": "pending"
  },
  {
    "id": 163,
    "title": "Tensor Parallel 中的通信原语",
    "slug": "tp-communication-primitives",
    "tags": [
      "底层原理",
      "分布式训练",
      "通信原语"
    ],
    "brief": "核心要点：TP 层的前向和反向分别需要不同的通信原语。Column Parallel 前向用 Identity+AllReduce（或 AllGather+split），反向则相反。分析 f 和 g 共轭算子的设计：前向 AllReduce 对应反向 Identity，前向 split 对应反向 AllGather。通信与计算的重叠策略。",
    "depth_hint": "推导 f/g 共轭算子的反向传播规则，对比 AllReduce 与 ReduceScatter+AllGather",
    "status": "pending"
  },
  {
    "id": 164,
    "title": "ZeRO 与模型并行的显存对比",
    "slug": "zero-vs-model-parallel-memory",
    "tags": [
      "底层原理",
      "ZeRO",
      "模型并行"
    ],
    "brief": "核心要点：以 GPT-3 175B 为例，纯 TP 需要 350GB 优化器状态（8 卡每卡 43.75GB），ZeRO-3 在 64 卡上每卡仅 33GB。但 ZeRO-3 通信量是 TP 的 1.5 倍。分析两种方案在不同网络带宽下的吞吐量交叉点，以及混合使用的最佳策略。",
    "depth_hint": "制表对比 TP、ZeRO Stage 1/2/3 在不同模型大小下的显存和通信量",
    "status": "pending"
  },
  {
    "id": 165,
    "title": "梯度累积与流水线并行的交互",
    "slug": "gradient-accumulation-pipeline",
    "tags": [
      "底层原理",
      "流水线并行",
      "训练技巧"
    ],
    "brief": "核心要点：流水线并行中微批次数 m 既影响 bubble 率又等效于梯度累积步数。全局批次大小 = dp × m × micro_batch_size。增大 m 降低 bubble 但增大等效 batch size，可能影响收敛。分析 m 的选择与学习率调度的关系，以及 gradient clipping 在 PP 中的实现细节。",
    "depth_hint": "推导 m 对有效学习率的影响，给出 bubble 率与收敛速度的权衡公式",
    "status": "pending"
  },
  {
    "id": 166,
    "title": "Megatron-LM 的激活重计算策略",
    "slug": "megatron-activation-recomputation",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "显存优化"
    ],
    "brief": "核心要点：Megatron-LM 提供 full 和 selective 两种激活重计算。Full 重计算每层仅保存输入激活，反向时重算所有中间值，显存降至 O(L·sbh) 但增加 33% 计算。Selective 仅重算占显存大但计算轻的部分（如 Softmax、Dropout），节省 60-70% 激活内存仅增加 5% 计算。",
    "depth_hint": "列出 Transformer 层各算子的显存/计算比，推导 selective 策略的最优选择",
    "status": "pending"
  },
  {
    "id": 167,
    "title": "PP 中的权重同步与一致性",
    "slug": "pipeline-weight-consistency",
    "tags": [
      "底层原理",
      "流水线并行",
      "分布式训练"
    ],
    "brief": "核心要点：异步流水线（PipeDream）中不同微批次使用不同版本权重，导致 weight stashing 问题。PipeDream-2BW 仅维护 2 个权重版本，在 bubble 期间同步。PipeDream-Flush 采用同步语义，牺牲部分吞吐保证收敛一致性。分析权重版本差异对梯度方差的定量影响。",
    "depth_hint": "推导异步权重更新的梯度偏差上界，对比同步和异步方案的收敛曲线",
    "status": "pending"
  },
  {
    "id": 168,
    "title": "跨节点 TP：NVLink 与 InfiniBand 的选择",
    "slug": "cross-node-tensor-parallel",
    "tags": [
      "底层原理",
      "分布式训练",
      "集群网络"
    ],
    "brief": "核心要点：TP 通信频率高（每层 2 次 AllReduce），延迟敏感。NVLink 带宽 600-900 GB/s、延迟 ~1μs，适合节点内 TP。IB HDR 200Gb/s（25GB/s）、延迟 ~1μs，跨节点 TP 吞吐下降 40-60%。分析 hidden_size 对通信计算比的影响，以及何时跨节点 TP 仍然合理。",
    "depth_hint": "计算不同 hidden_size 下 TP 的通信计算比，给出 NVLink vs IB 的效率对比",
    "status": "pending"
  },
  {
    "id": 169,
    "title": "ZeRO++ 的量化通信优化",
    "slug": "zero-plus-plus-quantized-comm",
    "tags": [
      "底层原理",
      "ZeRO",
      "通信优化"
    ],
    "brief": "核心要点：ZeRO++ 引入三项优化：qwZ（量化权重 AllGather，INT8 替代 FP16 减半通信）、qgZ（量化梯度 Reduce-Scatter）、hpZ（二级分区将跨节点通信替换为节点内通信）。INT8 量化对模型质量影响可控，loss 差异 <0.1%。总通信量降低 4 倍。",
    "depth_hint": "分析 INT8 量化误差的理论上界，实验对比量化前后的训练 loss 曲线",
    "status": "pending"
  },
  {
    "id": 170,
    "title": "FSDP 与 ZeRO Stage 3 的异同",
    "slug": "fsdp-vs-zero-stage3",
    "tags": [
      "底层原理",
      "ZeRO",
      "PyTorch"
    ],
    "brief": "核心要点：PyTorch FSDP 是 ZeRO Stage 3 的原生实现，但细节有差异。FSDP 以 FlatParameter 为单位分片，支持混合精度和 backward prefetch。与 DeepSpeed ZeRO-3 对比：FSDP 支持 torch.compile、不依赖外部库，但缺少 ZeRO-Offload 等高级功能。两者在 128 卡规模性能差距 <5%。",
    "depth_hint": "代码对比 FSDP 和 DeepSpeed ZeRO-3 的配置，benchmark 不同规模的吞吐量",
    "status": "pending"
  },
  {
    "id": 171,
    "title": "Megatron-LM 的分布式初始化",
    "slug": "megatron-distributed-init",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "分布式训练"
    ],
    "brief": "核心要点：Megatron-LM 初始化时创建多个进程组：tensor_model_parallel_group、pipeline_model_parallel_group、data_parallel_group。以 tp=4、pp=2、dp=4 共 32 卡为例，进程组的 rank 映射遵循 TP 连续、PP 其次、DP 最外层的规则。初始化顺序影响 NCCL 通信效率。",
    "depth_hint": "画出 32 卡的进程组映射矩阵，代码展示 initialize_model_parallel 的核心逻辑",
    "status": "pending"
  },
  {
    "id": 172,
    "title": "Tensor Parallel 的 Embedding 层处理",
    "slug": "tp-embedding-parallel",
    "tags": [
      "底层原理",
      "分布式训练",
      "模型并行"
    ],
    "brief": "核心要点：词表 Embedding 按 vocab 维度切分到 TP 组，每卡存 V/t 行。前向时各卡查表得到局部结果（非本卡词汇填零），AllReduce 合并。输出层（LM Head）与 Embedding 权重共享时，采用列切分后 AllGather 收集 logits。分析大词表（如 128K）下的内存和通信开销。",
    "depth_hint": "推导 Embedding TP 的通信量，对比列切分和行切分两种方案",
    "status": "pending"
  },
  {
    "id": 173,
    "title": "PP 中的负载均衡：层分配策略",
    "slug": "pipeline-layer-assignment",
    "tags": [
      "底层原理",
      "流水线并行",
      "性能优化"
    ],
    "brief": "核心要点：Transformer 各层计算量近似相等，但首尾阶段额外承担 Embedding 和 LM Head。Megatron-LM 默认均匀分层，可手动指定每级层数。不均匀分配需 profiling 各层延迟，使最慢阶段时间最小化。V 模型（首尾阶段少分层）比均匀分配提升 5-10% 吞吐。",
    "depth_hint": "建立流水线负载均衡的优化模型，给出不同模型结构的最优分层策略",
    "status": "pending"
  },
  {
    "id": 174,
    "title": "ZeRO-R：激活内存的分布式优化",
    "slug": "zero-r-activation-memory",
    "tags": [
      "底层原理",
      "ZeRO",
      "显存优化"
    ],
    "brief": "核心要点：ZeRO-R 优化残差内存（Residual Memory），包括激活内存分片、临时 buffer 定额管理和碎片优化。激活分片将检查点激活通过 AllGather 分布到 DP 组，每卡仅存 1/N。对于 batch_size=1 场景，可将激活 offload 到 CPU。与激活重计算正交，可组合使用。",
    "depth_hint": "推导激活分片的通信开销，对比单独使用重计算与组合使用的显存曲线",
    "status": "pending"
  },
  {
    "id": 175,
    "title": "Expert Parallel 与 Tensor Parallel 的组合",
    "slug": "expert-parallel-tensor-parallel",
    "tags": [
      "底层原理",
      "分布式训练",
      "MoE"
    ],
    "brief": "核心要点：大规模 MoE 模型中，EP 将不同专家分配到不同设备，TP 将单个专家内部切分。Mixtral 8x7B 的 8 个专家可 EP=8 各卡一个，或 EP=4+TP=2 每专家跨 2 卡。All-to-All 通信实现 token 到专家的路由，通信量为 2bsh·(ep-1)/ep。两种并行的通信模式不同，需要独立进程组。",
    "depth_hint": "分析 EP 和 TP 组合下的通信量公式，给出不同专家数和卡数的最优配置",
    "status": "pending"
  },
  {
    "id": 176,
    "title": "梯度通信压缩：从 FP16 到 1-bit",
    "slug": "gradient-compression-1bit",
    "tags": [
      "底层原理",
      "通信优化",
      "分布式训练"
    ],
    "brief": "核心要点：1-bit Adam/LAMB 将梯度压缩为符号位+误差补偿，通信量降至 1/16。压缩分两阶段：warmup 期用标准 AllReduce（约 15-20% 步数），之后切换为 1-bit 压缩。误差补偿保证收敛：本轮残差加入下轮梯度。在 256 卡上通信时间减少 5 倍，端到端加速 2-3 倍。",
    "depth_hint": "推导误差补偿的收敛保证，实验对比不同压缩率的训练曲线",
    "status": "pending"
  },
  {
    "id": 177,
    "title": "Context Parallel：超长序列的并行策略",
    "slug": "context-parallelism-long-seq",
    "tags": [
      "底层原理",
      "序列并行",
      "长上下文"
    ],
    "brief": "核心要点：Context Parallel（CP）将序列均匀切分到多卡，每卡处理 s/cp 长度的子序列。注意力计算需要 Ring Attention：Q 本地保留，KV 在环形拓扑上传递，每步计算一个 chunk 的注意力后传递 KV 到下一卡。通信量 O(bsh·cp)，可与 TP/PP/DP 正交组合，支持百万级序列。",
    "depth_hint": "画出 Ring Attention 的数据流图，推导 CP 与 TP 组合时的通信量",
    "status": "pending"
  },
  {
    "id": 178,
    "title": "异构集群训练：GPU 混合并行策略",
    "slug": "heterogeneous-cluster-training",
    "tags": [
      "底层原理",
      "分布式训练",
      "异构计算"
    ],
    "brief": "核心要点：实际集群常混合 A100/H100 或不同网络拓扑，均匀并行策略导致木桶效应。AMP（Adaptive Model Parallelism）根据设备计算能力和互联带宽动态调整 TP/PP/DP 维度。慢节点分配更少层或更小 micro-batch，通过异步流水线容忍延迟差异。吞吐比均匀策略提升 20-30%。",
    "depth_hint": "建立异构集群的性能模型，推导自适应并行度的优化目标函数",
    "status": "pending"
  },
  {
    "id": 179,
    "title": "Ulysses 与 Ring Attention 的对比",
    "slug": "ulysses-vs-ring-attention",
    "tags": [
      "底层原理",
      "序列并行",
      "长上下文"
    ],
    "brief": "核心要点：DeepSpeed-Ulysses 沿 head 维度切分 QKV，通过 All-to-All 通信重新分布序列维度，每卡执行完整注意力但仅处理部分 head。Ring Attention 沿序列维度切分，通信为 KV 环形传递。Ulysses 通信量 O(bsh) 与序列长度无关，但受限于 head 数；Ring 可扩展到任意序列长度。",
    "depth_hint": "推导两种方法的通信量和延迟公式，给出不同 seq_len/num_heads 下的选择建议",
    "status": "pending"
  },
  {
    "id": 180,
    "title": "DP 通信重叠：梯度 AllReduce 流水线化",
    "slug": "dp-communication-overlap",
    "tags": [
      "底层原理",
      "数据并行",
      "通信优化"
    ],
    "brief": "核心要点：标准 DDP 将反向传播产生的梯度分桶（bucket），每个 bucket 填满后立即启动 AllReduce，与后续层的反向计算重叠。桶大小（默认 25MB）影响重叠效率：过小增加通信次数，过大延迟启动时机。PyTorch DDP 的 gradient_as_bucket_view 优化避免梯度拷贝，节省 peak memory。",
    "depth_hint": "分析不同 bucket size 对通信重叠比例的影响，实验测量不同配置的端到端性能",
    "status": "pending"
  },
  {
    "id": 181,
    "title": "Scaling Law 的基本形式与拟合方法",
    "slug": "scaling-law-basic-formulation",
    "tags": [
      "底层原理",
      "扩展定律",
      "Scaling Law"
    ],
    "brief": "核心要点：Kaplan et al. (2020) 提出的幂律关系 L(N) ∝ N^{-α}，损失与参数量、数据量、计算量的三变量拟合。详解指数 α≈0.076（参数）和 α≈0.095（数据）的来源，以及 L(N,D) = L₀ + A/N^α + B/D^β 的联合公式推导过程。",
    "depth_hint": "需要复现三变量幂律拟合过程，给出 R² 拟合优度分析，对比不同拟合假设的差异",
    "status": "pending"
  },
  {
    "id": 182,
    "title": "Chinchilla 定律：计算最优的训练配比",
    "slug": "chinchilla-optimal-scaling",
    "tags": [
      "底层原理",
      "扩展定律",
      "Chinchilla"
    ],
    "brief": "核心要点：Hoffmann et al. (2022) 的核心结论——最优训练 token 数与参数量线性增长 D_opt ∝ N。对比 Kaplan 的 N^{0.74} 结论，分析三种拟合方法（固定N变D、固定FLOPs包络线、参数化损失函数）如何得到 a=0.50, b=0.50 的指数，以及 70B 参数对应 1.4T token 的具体数值。",
    "depth_hint": "推导 IsoFLOP 曲线上的最优点求解过程，复现表9的预测 vs 实际损失对比",
    "status": "pending"
  },
  {
    "id": 183,
    "title": "Scaling Law 中的不可约损失与熵下界",
    "slug": "irreducible-loss-entropy-bound",
    "tags": [
      "底层原理",
      "扩展定律",
      "信息论"
    ],
    "brief": "核心要点：幂律公式中的常数项 L₀ 代表数据分布的内在熵，是任何模型都无法突破的下界。分析自然语言的信息熵估计（Shannon 游戏约 1.0-1.3 bits/char），以及 L₀ 在不同数据集（C4、The Pile、RedPajama）上的差异如何反映数据质量。",
    "depth_hint": "需要对比不同数据集的 L₀ 估计值，分析数据去重对 L₀ 的影响量级",
    "status": "pending"
  },
  {
    "id": 184,
    "title": "Scaling Law 的下游任务迁移",
    "slug": "scaling-law-downstream-transfer",
    "tags": [
      "底层原理",
      "扩展定律",
      "迁移学习"
    ],
    "brief": "核心要点：预训练损失的幂律关系能否迁移到下游任务？分析 Downstream Scaling Law 的形式 Acc(L) 与 L(N,D) 的复合关系。Gadre et al. (2024) 发现不同任务的 scaling 斜率差异显著，部分任务存在 broken scaling（精度突然跃升），与 Emergent Abilities 的讨论直接相关。",
    "depth_hint": "给出 MMLU、HellaSwag 等基准的 scaling 曲线对比，标注 broken 区间",
    "status": "pending"
  },
  {
    "id": 185,
    "title": "计算最优之外：推理成本的 Scaling 权衡",
    "slug": "inference-aware-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "推理优化"
    ],
    "brief": "核心要点：Chinchilla 最优只考虑训练 FLOPs，但实际部署时推理成本与参数量 N 成正比。Sardana & Frankle (2023) 提出推理感知 Scaling Law，引入推理 token 总量 T_inf，最优模型应过训练（D >> D_chinchilla）以缩小 N。LLaMA 系列正是此策略的实践。",
    "depth_hint": "推导训练+推理总成本 C_total = 6ND + 2NT_inf 的最优解，对比 Chinchilla 与 LLaMA 的配比差异",
    "status": "pending"
  },
  {
    "id": 186,
    "title": "数据约束下的 Scaling Law 修正",
    "slug": "data-constrained-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "数据工程"
    ],
    "brief": "核心要点：Muennighoff et al. (2023) 研究数据重复对 Scaling Law 的影响。当唯一 token 数 U 不足时，重复 R 次后损失衰减变慢：有效数据量 D_eff = U(1 - e^{-R})。4 epoch 重复仅相当于约 2.8 倍数据，超过 16 epoch 后收益几乎为零。",
    "depth_hint": "需要给出 D_eff 公式推导和不同重复次数下的实际损失曲线数据",
    "status": "pending"
  },
  {
    "id": 187,
    "title": "Scaling Law 对模型宽度与深度的约束",
    "slug": "scaling-law-width-depth-ratio",
    "tags": [
      "底层原理",
      "扩展定律",
      "模型架构"
    ],
    "brief": "核心要点：参数量 N 相同时，宽度 d_model 与深度 n_layers 的最优比例是什么？Levine et al. (2020) 从表达力角度给出 d ∝ n^{2/3} 的理论最优，而实践中 GPT-3、LLaMA 的宽深比约为 128:1。分析过宽（过拟合）和过深（梯度退化）的具体机制。",
    "depth_hint": "对比 GPT-3 各档位的宽深比，结合 μP 参数化分析最优比例的实验验证",
    "status": "pending"
  },
  {
    "id": 188,
    "title": "涌现能力的原始定义与争议",
    "slug": "emergent-abilities-definition-debate",
    "tags": [
      "底层原理",
      "涌现能力",
      "评测方法"
    ],
    "brief": "核心要点：Wei et al. (2022) 定义涌现能力为'小模型中不存在、大模型中突然出现'的能力。但 Schaeffer et al. (2023) 指出这可能是评测指标的伪影——将连续的对数线性改进用非线性指标（如精确匹配）度量会产生假阶跃。两方观点的核心分歧在于'能力'的度量方式。",
    "depth_hint": "用具体数值复现 BIG-bench 上的涌现曲线，展示更换指标后阶跃消失的过程",
    "status": "pending"
  },
  {
    "id": 189,
    "title": "涌现与指标选择：精确匹配 vs 对数概率",
    "slug": "emergence-metric-artifact",
    "tags": [
      "底层原理",
      "涌现能力",
      "评测方法"
    ],
    "brief": "核心要点：Schaeffer et al. (2023) 的核心论证——多位数加法任务中，token 级准确率平滑上升，但序列级精确匹配呈阶跃。一个 5 位数加法需要所有 5 位正确，P(全对) = p^5，当 p 从 0.8→0.95 时精确匹配从 33%→77%，产生视觉上的'涌现'。这是统计伪影还是真实现象？",
    "depth_hint": "推导 p^k 的阈值效应公式，用模拟数据生成不同 k 值下的涌现假象图",
    "status": "pending"
  },
  {
    "id": 190,
    "title": "相变视角下的涌现能力分析",
    "slug": "emergence-phase-transition",
    "tags": [
      "底层原理",
      "涌现能力",
      "统计物理"
    ],
    "brief": "核心要点：将涌现类比为物理相变：模型规模 N 对应温度，能力对应序参量。Arora & Goyal (2023) 从统计力学角度分析，loss landscape 的对称性破缺可能导致真实的不连续相变。与 Ising 模型的临界指数类比，讨论涌现的普适类（universality class）是否存在。",
    "depth_hint": "给出二维 Ising 模型的相变类比图示，分析临界指数与模型规模指数的对应关系",
    "status": "pending"
  },
  {
    "id": 191,
    "title": "链式思维的涌现：为什么小模型做不到",
    "slug": "chain-of-thought-emergence",
    "tags": [
      "底层原理",
      "涌现能力",
      "链式思维"
    ],
    "brief": "核心要点：CoT 推理能力在约 100B 参数时涌现（Wei et al., 2022）。小模型的 CoT 反而降低性能，因为中间步骤的错误会累积。分析 CoT 涌现的两个必要条件：(1) 单步推理准确率超过阈值 p > 1/k；(2) 上下文窗口中的工作记忆容量足够保持多步状态。",
    "depth_hint": "推导多步推理中错误累积的概率模型 P(n步全对) = p^n，分析临界 p 值",
    "status": "pending"
  },
  {
    "id": 192,
    "title": "Grokking：过拟合之后的延迟涌现",
    "slug": "grokking-delayed-generalization",
    "tags": [
      "底层原理",
      "涌现能力",
      "泛化理论"
    ],
    "brief": "核心要点：Power et al. (2022) 发现模型在训练集上过拟合很久之后，测试集性能突然跃升，称为 Grokking。在模运算任务中，训练损失在 10³ 步到达 0，但测试精度在 10⁵ 步才突然从随机涨到 95%+。与涌现能力的联系：都是非线性的能力获取过程。",
    "depth_hint": "复现模运算任务的训练曲线，分析权重范数与正则化强度对 grokking 时间点的影响",
    "status": "pending"
  },
  {
    "id": 193,
    "title": "U 形涌现：逆向 Scaling 现象",
    "slug": "inverse-scaling-u-shaped-emergence",
    "tags": [
      "底层原理",
      "涌现能力",
      "模型评测"
    ],
    "brief": "核心要点：Inverse Scaling Prize 发现部分任务（如 Hindsight Neglect、Memo Trap）中大模型表现反而更差。McKenzie et al. (2023) 分析原因：大模型更倾向于模仿训练数据中的虚假相关，但超大模型（>100B）会再次纠正，形成 U 形曲线。这对涌现的定义提出了修正。",
    "depth_hint": "给出具体任务的 U 形 Scaling 曲线数据，分析模仿学习与推理能力的竞争机制",
    "status": "pending"
  },
  {
    "id": 194,
    "title": "In-Context Learning 的信息论解释",
    "slug": "in-context-learning-information-theory",
    "tags": [
      "底层原理",
      "上下文学习",
      "信息论"
    ],
    "brief": "核心要点：ICL 可以用互信息 I(Y;X|C) 来理解——上下文 C（示例）提供了 X→Y 映射的信息，降低了预测 Y 的不确定性。Xie et al. (2022) 证明 Transformer 在 ICL 中隐式执行贝叶斯推断，上下文示例用于推断隐含的生成概念 θ，等价于 p(y|x) = ∫p(y|x,θ)p(θ|C)dθ。",
    "depth_hint": "推导贝叶斯推断公式在 ICL 中的对应关系，用 HMM 生成数据验证理论",
    "status": "pending"
  },
  {
    "id": 195,
    "title": "Transformer 的隐式梯度下降：ICL 即学习",
    "slug": "icl-implicit-gradient-descent",
    "tags": [
      "底层原理",
      "上下文学习",
      "优化理论"
    ],
    "brief": "核心要点：Von Oswald et al. (2023) 和 Akyürek et al. (2023) 证明线性 Attention 层的前向传播等价于一步梯度下降。给定上下文示例 {(xᵢ, yᵢ)}，Self-Attention 计算出的输出等价于在示例上做最小二乘回归的梯度更新 W ← W - η∇L。多层 Transformer 对应多步梯度下降。",
    "depth_hint": "推导线性 Attention 与梯度下降的等价关系，给出权重矩阵的闭式解对比",
    "status": "pending"
  },
  {
    "id": 196,
    "title": "ICL 中的任务识别与任务学习的分离",
    "slug": "icl-task-recognition-vs-learning",
    "tags": [
      "底层原理",
      "上下文学习",
      "机制分析"
    ],
    "brief": "核心要点：Pan et al. (2023) 将 ICL 分解为两个独立机制：任务识别（从示例中识别出'这是情感分类'）和任务学习（学习输入到输出的映射）。实验证据：(1) 随机标签仍能维持部分性能（任务识别起作用）；(2) 更多示例主要提升任务学习而非识别。两个机制在不同层中实现。",
    "depth_hint": "设计随机标签实验对比正常标签，量化两种机制的贡献比例",
    "status": "pending"
  },
  {
    "id": 197,
    "title": "Induction Head：ICL 的电路级机制",
    "slug": "induction-head-icl-circuit",
    "tags": [
      "底层原理",
      "上下文学习",
      "机械可解释性"
    ],
    "brief": "核心要点：Olsson et al. (2022) 发现 ICL 依赖一种特定的注意力电路——归纳头（Induction Head）。两层电路：第一层的 previous-token head 将信息从位置 i 复制到 i+1，第二层的 induction head 在当前位置匹配相同前缀并复制后续 token。这个电路在训练中突然形成，对应 loss 的阶跃下降。",
    "depth_hint": "给出 Induction Head 的 QKV 参数结构分析，复现其在训练中形成的时间点",
    "status": "pending"
  },
  {
    "id": 198,
    "title": "ICL 的示例选择策略与性能影响",
    "slug": "icl-example-selection-strategies",
    "tags": [
      "底层原理",
      "上下文学习",
      "提示工程"
    ],
    "brief": "核心要点：ICL 性能对示例选择高度敏感。Liu et al. (2022) 发现与测试输入语义最近的示例效果最好（kNN 检索），但顺序也很重要——最相关的示例放在最后（近因效应）可提升 10%+。此外，示例的多样性与相关性存在权衡，Coverage-based 选择优于纯相似度。",
    "depth_hint": "对比随机/kNN/多样性三种选择策略的定量实验结果，分析示例顺序的影响",
    "status": "pending"
  },
  {
    "id": 199,
    "title": "ICL 中的标签空间与格式效应",
    "slug": "icl-label-space-format-effect",
    "tags": [
      "底层原理",
      "上下文学习",
      "提示工程"
    ],
    "brief": "核心要点：Min et al. (2022) 发现 ICL 中标签是否正确影响不大，但标签空间（使用哪些标签词）、输入分布和格式影响显著。将'positive/negative'换成'foo/bar'会大幅降低性能，因为模型依赖预训练中标签词的语义先验。这揭示了 ICL 更多是'格式学习'而非'任务学习'。",
    "depth_hint": "复现标签替换实验的定量结果，分析不同标签词的困惑度与 ICL 性能的相关性",
    "status": "pending"
  },
  {
    "id": 200,
    "title": "Many-Shot ICL：突破少样本的限制",
    "slug": "many-shot-icl-long-context",
    "tags": [
      "底层原理",
      "上下文学习",
      "长上下文"
    ],
    "brief": "核心要点：Agarwal et al. (2024) 利用长上下文窗口（100K+）进行数百到数千个示例的 ICL。实验表明 Many-Shot ICL 在多个任务上逼近甚至超过微调性能。但存在边际递减效应：前 100 个示例贡献最大，之后收益按 O(log n) 衰减。同时发现 Reinforced ICL（用模型生成的 rationale 替代人工示例）可进一步提升。",
    "depth_hint": "给出示例数量 vs 性能的对数曲线，分析不同任务的饱和点",
    "status": "pending"
  },
  {
    "id": 201,
    "title": "Scaling Law 的理论基础：神经网络逼近论",
    "slug": "scaling-law-approximation-theory",
    "tags": [
      "底层原理",
      "扩展定律",
      "理论基础"
    ],
    "brief": "核心要点：Scaling Law 的幂律指数 α 与目标函数的内在维度有关。Sharma & Kaplan (2022) 从神经网络逼近论推导：如果数据流形的内在维度为 d，则 L(N) ∝ N^{-4/d}。自然语言的有效维度约 50-100，对应 α ≈ 0.04-0.08，与实验观测一致。",
    "depth_hint": "推导逼近论中的幂律指数与流形维度的关系式，对比不同数据集的维度估计",
    "status": "pending"
  },
  {
    "id": 202,
    "title": "Broken Neural Scaling Law：S 形修正",
    "slug": "broken-neural-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "经验公式"
    ],
    "brief": "核心要点：Caballero et al. (2023) 提出 BNSL 公式，用分段幂律替代单一幂律：在某个临界点 N_c 处斜率发生变化，形成 S 形曲线。这能更好地拟合下游任务的 scaling 行为，解释为什么某些任务出现'涌现'——实际上是幂律斜率的突变而非阶跃。BNSL 在 100+ 任务上的拟合 R² > 0.99。",
    "depth_hint": "给出 BNSL 的数学形式和参数拟合方法，对比标准幂律在 break point 附近的拟合误差",
    "status": "pending"
  },
  {
    "id": 203,
    "title": "Scaling Law 的多模态扩展",
    "slug": "multimodal-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "多模态"
    ],
    "brief": "核心要点：Aghajanyan et al. (2023) 研究文本+图像联合训练的 Scaling Law。多模态训练的有效计算量不等于各模态之和——图文交叉注意力带来的增益遵循 L(N,D_t,D_i) 的三变量幂律，且图像 token 的'兑换比'约为 1 图像 token ≈ 0.3 文本 token。模态比例也存在最优配比。",
    "depth_hint": "给出多模态幂律的联合公式，分析图文比例 vs 损失的等高线图数据",
    "status": "pending"
  },
  {
    "id": 204,
    "title": "训练稳定性的 Scaling：大模型为何更难训练",
    "slug": "training-instability-scaling",
    "tags": [
      "底层原理",
      "扩展定律",
      "训练稳定性"
    ],
    "brief": "核心要点：随着模型规模增大，训练不稳定性（loss spike）的频率和幅度增加。Chowdhery et al. (2022, PaLM) 记录了 540B 模型中约 20 次 loss spike。原因分析：(1) 梯度范数的方差与 N 正相关；(2) Attention logits 的量级随 d_model 增长为 O(√d)；(3) 学习率的最优值随 N 下降。μP 参数化可部分解决。",
    "depth_hint": "给出 PaLM 训练中 loss spike 的时间分布数据，分析 Attention logits 缩放的数学推导",
    "status": "pending"
  },
  {
    "id": 205,
    "title": "μP：从小模型预测大模型的最优超参数",
    "slug": "mup-maximal-update-parameterization",
    "tags": [
      "底层原理",
      "扩展定律",
      "超参数迁移"
    ],
    "brief": "核心要点：Yang et al. (2022) 提出 Maximal Update Parameterization (μP)，通过特定的初始化和学习率缩放规则，使得小模型的最优超参数可以直接迁移到大模型。核心思想：让每层的激活更新量 Δh/h 在不同宽度下保持恒定。具体规则：lr ∝ 1/width（输出层），lr 恒定（隐藏层），init ∝ 1/√width。",
    "depth_hint": "推导 μP 的缩放规则表，对比标准参数化（SP）下超参数迁移的失败案例",
    "status": "pending"
  },
  {
    "id": 206,
    "title": "ICL 的注意力模式：从浅层匹配到深层推理",
    "slug": "icl-attention-pattern-analysis",
    "tags": [
      "底层原理",
      "上下文学习",
      "注意力机制"
    ],
    "brief": "核心要点：Transformer 不同层在 ICL 中承担不同角色。浅层（1-8层）进行词汇和格式匹配，中层（9-20层）进行语义对齐，深层（21+层）进行任务特定的推理。Bansal et al. (2023) 通过逐层冻结实验量化各层贡献：移除中层对 ICL 性能影响最大（下降 30%+），而移除浅层影响较小。",
    "depth_hint": "给出逐层消融实验的定量数据，可视化不同层的注意力模式差异",
    "status": "pending"
  },
  {
    "id": 207,
    "title": "Transformer 的上下文窗口与 ICL 容量",
    "slug": "context-window-icl-capacity",
    "tags": [
      "底层原理",
      "上下文学习",
      "模型容量"
    ],
    "brief": "核心要点：ICL 的有效示例数量受限于上下文窗口中的'工作记忆'容量，而非窗口长度本身。Bertsch et al. (2024) 发现即使窗口足够长，模型对超过一定数量示例后的信息利用效率急剧下降。有效容量约为 O(d_model/d_head) 个独立'概念槽'，与注意力头数成正比。",
    "depth_hint": "分析有效容量与模型维度的关系式，给出不同规模模型的实测容量对比",
    "status": "pending"
  },
  {
    "id": 208,
    "title": "涌现能力的可预测性：Scaling Law 的视角",
    "slug": "predictability-of-emergence",
    "tags": [
      "底层原理",
      "涌现能力",
      "扩展定律"
    ],
    "brief": "核心要点：涌现能力是否真的不可预测？Wei et al. (2024) 从 Scaling Law 角度分析：如果使用连续指标（如 Brier Score）替代离散指标，多数'涌现'可以被小模型的 scaling 趋势预测。但仍存在少数任务（如 BigBench 中的 navigate）的 scaling 行为确实非光滑，可能涉及内部表示的相变。",
    "depth_hint": "对比 Brier Score 与精确匹配的 scaling 曲线差异，标注无法被平滑拟合的异常任务",
    "status": "pending"
  },
  {
    "id": 209,
    "title": "重复 Token 的 Scaling 效应：数据质量 vs 数量",
    "slug": "token-repetition-scaling-effect",
    "tags": [
      "底层原理",
      "扩展定律",
      "数据工程"
    ],
    "brief": "核心要点：Hernandez et al. (2022) 发现重复数据对 Scaling Law 的破坏效应可以量化——100 次重复的数据等效于仅约 3.4 次的新数据（有效倍率 ≈ 0.034）。更关键的是，重复不均匀分布在不同 domain 中，高重复 domain（如 Common Crawl 中的模板页面）会系统性偏移模型的能力分布。",
    "depth_hint": "给出重复次数 vs 有效数据量的定量关系曲线，分析不同 domain 的重复率分布",
    "status": "pending"
  },
  {
    "id": 210,
    "title": "ICL 与微调的统一理论框架",
    "slug": "icl-finetuning-unified-framework",
    "tags": [
      "底层原理",
      "上下文学习",
      "微调"
    ],
    "brief": "核心要点：Dai et al. (2023) 从对偶优化角度证明 ICL 和微调是同一优化问题的两种求解方式。ICL 通过 Attention 在前向传播中动态构建'虚拟梯度'，微调通过反向传播更新参数。两者收敛到相似但不完全相同的解：ICL 受限于 Attention 的表达力，相当于一阶优化；微调可以达到高阶最优。",
    "depth_hint": "推导 ICL 的虚拟梯度公式与标准 SGD 梯度的等价条件，给出两者性能差距的上界",
    "status": "pending"
  },
  {
    "id": 211,
    "title": "Constitutional AI 的核心机制",
    "slug": "constitutional-ai-core-mechanism",
    "tags": [
      "模型解析",
      "Claude",
      "Constitutional AI"
    ],
    "brief": "核心要点：Constitutional AI (CAI) 通过两阶段训练实现自我对齐——先用 LLM 自身按宪法原则修订有害回答（SL-CAI），再对修订后的数据做 RLHF（RL-CAI）。相比纯 RLHF，CAI 将人类反馈从逐条标注转为原则制定，标注成本降低数个数量级。",
    "depth_hint": "需拆解 critique-revision 循环的具体 prompt 结构，对比 CAI 与标准 RLHF 的 helpfulness/harmlessness 得分",
    "status": "pending"
  },
  {
    "id": 212,
    "title": "RLHF 与 RLAIF 的对齐效果对比",
    "slug": "rlhf-vs-rlaif-alignment",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：RLAIF 用 AI 生成的偏好标签替代人类标注训练奖励模型。Anthropic 实验表明 RLAIF 在 helpfulness 上与 RLHF 表现相当（胜率约 50%），但在 harmlessness 上 RLAIF 的 AI 反馈更一致。关键在于 chain-of-thought 提示显著提升 AI 标注质量。",
    "depth_hint": "需给出 RLAIF 与 RLHF 的人类评估胜率数据，分析 CoT 对标注准确率的提升幅度",
    "status": "pending"
  },
  {
    "id": 213,
    "title": "Claude 的 HHH 目标函数解析",
    "slug": "claude-hhh-objective",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：Helpful、Honest、Harmless 三目标之间存在张力——过度追求 harmless 导致拒绝合理请求（过度拒绝），过度 helpful 又可能输出有害内容。Claude 通过 Constitutional AI 的原则层级来平衡三者，其中 harmlessness 在涉及严重伤害时具有最高优先级。",
    "depth_hint": "需分析三目标冲突的具体场景分类，给出 Anthropic 论文中的 Pareto 前沿示意",
    "status": "pending"
  },
  {
    "id": 214,
    "title": "Claude 的 Critique-Revision 循环",
    "slug": "claude-critique-revision-loop",
    "tags": [
      "模型解析",
      "Claude",
      "Constitutional AI"
    ],
    "brief": "核心要点：CAI 的 SL 阶段通过多轮 critique-revision 循环提升回答质量。每轮随机采样一条宪法原则，让模型自我批评并修订。实验显示 4 轮修订后 harmlessness 评分提升约 30%，但超过 4 轮后边际收益递减且 helpfulness 开始下降。",
    "depth_hint": "需给出多轮修订的 prompt 模板结构，展示不同轮次的质量变化曲线",
    "status": "pending"
  },
  {
    "id": 215,
    "title": "Claude 宪法原则的设计与演化",
    "slug": "claude-constitution-principles-design",
    "tags": [
      "模型解析",
      "Claude",
      "Constitutional AI"
    ],
    "brief": "核心要点：Claude 的宪法原则从最初 16 条基础规则（涵盖 UN 人权宣言、非歧视、真实性等）逐步扩展。原则设计需平衡覆盖度与一致性——过于宽泛导致模型行为不可预测，过于具体则无法泛化。原则之间的优先级通过训练数据中的出现频率隐式编码。",
    "depth_hint": "需列举关键宪法原则的原文及分类，分析原则冲突时的裁决逻辑",
    "status": "pending"
  },
  {
    "id": 216,
    "title": "Claude 1 到 Claude 3 的架构演进",
    "slug": "claude-architecture-evolution",
    "tags": [
      "模型解析",
      "Claude",
      "架构"
    ],
    "brief": "核心要点：Claude 系列从 52B 参数的 Claude 1 演进到多模态的 Claude 3 家族（Haiku/Sonnet/Opus）。关键变化包括：上下文窗口从 9K→200K、引入视觉能力、从纯 RLHF 转向 CAI+RLHF 混合训练。Claude 3 Opus 在 MMLU 上达到 86.8%，首次超越 GPT-4。",
    "depth_hint": "需整理各版本的参数规模、上下文长度、基准得分对比表",
    "status": "pending"
  },
  {
    "id": 217,
    "title": "Claude 的长上下文处理机制",
    "slug": "claude-long-context-mechanism",
    "tags": [
      "模型解析",
      "Claude",
      "注意力机制"
    ],
    "brief": "核心要点：Claude 3 支持 200K token 上下文窗口，在 Needle-in-a-Haystack 测试中准确率超过 99%。长上下文处理的关键技术包括位置编码外推（RoPE 的 NTK-aware 缩放）、注意力稀疏化，以及训练时逐步扩展上下文长度的阶段式策略。",
    "depth_hint": "需分析 RoPE 缩放的数学形式，给出 NIAH 测试在不同文档长度下的准确率数据",
    "status": "pending"
  },
  {
    "id": 218,
    "title": "Claude 3 的多模态融合架构",
    "slug": "claude3-multimodal-architecture",
    "tags": [
      "模型解析",
      "Claude",
      "多模态"
    ],
    "brief": "核心要点：Claude 3 通过视觉编码器（ViT 变体）将图像转化为 token 序列，与文本 token 共享 Transformer 的注意力层。图像分辨率自适应分片，最大支持约 1568×1568 像素。在 MMMU 基准上 Claude 3 Opus 达到 59.4%，显示出强视觉推理能力。",
    "depth_hint": "需分析图像 token 化的分辨率-token 数映射关系，对比主流多模态架构方案",
    "status": "pending"
  },
  {
    "id": 219,
    "title": "Claude 的偏好模型训练细节",
    "slug": "claude-preference-model-training",
    "tags": [
      "模型解析",
      "Claude",
      "RLHF"
    ],
    "brief": "核心要点：Claude 的奖励模型（PM）基于 Bradley-Terry 模型，使用人类偏好对和 AI 生成偏好对联合训练。PM 的损失函数为 -log σ(r_w - r_l)，其中 r 为标量奖励。关键挑战是奖励黑客（reward hacking）：策略模型学会利用 PM 漏洞获得高分但质量下降。",
    "depth_hint": "需给出 Bradley-Terry 模型的概率公式，分析奖励黑客的典型表现和缓解策略",
    "status": "pending"
  },
  {
    "id": 220,
    "title": "Claude 的 PPO 训练稳定性优化",
    "slug": "claude-ppo-training-stability",
    "tags": [
      "模型解析",
      "Claude",
      "RLHF"
    ],
    "brief": "核心要点：Claude 使用 PPO 算法进行 RL 微调，关键超参数包括 clip ratio ε=0.2、KL 惩罚系数 β。训练中常见的不稳定现象包括奖励崩塌、KL 散度爆炸、模式坍缩。Anthropic 采用自适应 KL 控制和早停策略来维持训练稳定性。",
    "depth_hint": "需给出 PPO 的 clipped objective 公式，分析 KL 惩罚项的动态调节机制",
    "status": "pending"
  },
  {
    "id": 221,
    "title": "Claude 的 Red Teaming 方法论",
    "slug": "claude-red-teaming-methodology",
    "tags": [
      "模型解析",
      "Claude",
      "安全"
    ],
    "brief": "核心要点：Anthropic 采用人类红队和自动化红队结合的方式测试 Claude 的安全边界。自动化红队使用另一个 LLM 生成攻击 prompt，按毒性分类器得分筛选有效攻击。实验显示 CAI 训练后的模型在红队攻击成功率上降低约 50%，但仍存在越狱漏洞。",
    "depth_hint": "需分类红队攻击的类型层级，给出攻击成功率随模型规模和训练方法的变化数据",
    "status": "pending"
  },
  {
    "id": 222,
    "title": "Claude 的过度拒绝问题与缓解",
    "slug": "claude-overrefusal-mitigation",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：过度安全对齐导致 Claude 拒绝合理请求（如医学咨询、创意写作中的冲突场景）。过度拒绝率在早期版本中约 15-20%。缓解方法包括：细化宪法原则、增加 borderline 场景的训练数据、引入 helpfulness 与 harmlessness 的显式权衡机制。",
    "depth_hint": "需定义过度拒绝的评估标准，分析不同版本的拒绝率变化趋势",
    "status": "pending"
  },
  {
    "id": 223,
    "title": "Claude 的 System Prompt 机制",
    "slug": "claude-system-prompt-mechanism",
    "tags": [
      "模型解析",
      "Claude",
      "推理"
    ],
    "brief": "核心要点：System prompt 在 Claude 的注意力机制中占据特殊位置——它作为前缀 context 影响后续所有生成。Claude 对 system prompt 的遵循度通过专门的指令跟随训练强化，包括多轮对话中的一致性保持。system prompt 与用户指令冲突时，Claude 倾向于遵循 system prompt。",
    "depth_hint": "需分析 system prompt 在注意力层中的影响模式，给出指令冲突场景的处理优先级",
    "status": "pending"
  },
  {
    "id": 224,
    "title": "Constitutional AI 的数学形式化",
    "slug": "constitutional-ai-formalization",
    "tags": [
      "模型解析",
      "Claude",
      "Constitutional AI"
    ],
    "brief": "核心要点：CAI 可形式化为约束优化问题：max E[r_helpful] s.t. E[r_harmful] < ε，其中 r 为对应奖励函数。SL-CAI 阶段等价于带拒绝采样的蒸馏，RL-CAI 阶段等价于在宪法约束下的 PPO。Lagrange 对偶性连接了约束优化与 KL 正则化的 RL 目标。",
    "depth_hint": "需推导约束优化到 Lagrange 形式的变换，给出 KL 正则化系数的物理含义",
    "status": "pending"
  },
  {
    "id": 225,
    "title": "Claude 的 Token 采样策略",
    "slug": "claude-token-sampling-strategy",
    "tags": [
      "模型解析",
      "Claude",
      "推理"
    ],
    "brief": "核心要点：Claude 支持 temperature、top-p、top-k 等采样参数。temperature=0 时使用贪心解码，temperature=1 时为原始分布采样。Claude 的默认 temperature 约 0.7，在创造性与一致性之间取得平衡。nucleus sampling (top-p=0.9) 通过动态截断词表避免长尾噪声。",
    "depth_hint": "需给出 softmax temperature 的数学定义，分析不同采样策略对输出多样性和质量的影响",
    "status": "pending"
  },
  {
    "id": 226,
    "title": "Claude 3.5 Sonnet 的效率优化",
    "slug": "claude-35-sonnet-efficiency",
    "tags": [
      "模型解析",
      "Claude",
      "模型压缩"
    ],
    "brief": "核心要点：Claude 3.5 Sonnet 在性能上接近 Claude 3 Opus，但推理速度快 2 倍、成本降低 80%。效率提升来源于：更优的模型架构（可能采用 GQA 或 MQA 减少 KV cache）、更高效的训练数据配比、以及推理端的投机解码等加速技术。",
    "depth_hint": "需对比 MHA/GQA/MQA 的计算复杂度差异，分析 KV cache 的内存节省比例",
    "status": "pending"
  },
  {
    "id": 227,
    "title": "Claude 的多轮对话状态管理",
    "slug": "claude-multi-turn-state-management",
    "tags": [
      "模型解析",
      "Claude",
      "推理"
    ],
    "brief": "核心要点：Claude 在多轮对话中通过 KV cache 复用实现状态延续。随对话轮次增加，早期轮次的信息在注意力分布中逐渐稀释（attention sink 现象）。Claude 通过位置编码和训练策略缓解远距离信息丢失，但实测显示超过 20 轮后指令遵循度下降约 10%。",
    "depth_hint": "需分析注意力分布随对话轮次的变化模式，给出 KV cache 的内存增长公式",
    "status": "pending"
  },
  {
    "id": 228,
    "title": "Anthropic 的可解释性研究：特征字典",
    "slug": "anthropic-interpretability-feature-dictionary",
    "tags": [
      "模型解析",
      "Claude",
      "可解释性"
    ],
    "brief": "核心要点：Anthropic 使用稀疏自编码器（SAE）从 Claude 3 Sonnet 的 MLP 层中提取出数百万个可解释特征。每个特征对应一个语义概念（如'Golden Gate Bridge'），激活值反映概念的相关度。SAE 的训练目标是最小化重建误差同时保持稀疏性：L = ||x - D·z||² + λ||z||₁。",
    "depth_hint": "需给出 SAE 的训练目标公式，展示典型特征的激活模式和语义标签",
    "status": "pending"
  },
  {
    "id": 229,
    "title": "Claude 中的 Superposition 现象",
    "slug": "claude-superposition-phenomenon",
    "tags": [
      "模型解析",
      "Claude",
      "可解释性"
    ],
    "brief": "核心要点：Superposition 指神经网络用 N 维空间表示远超 N 个特征的现象。Anthropic 的 toy model 实验表明，当特征稀疏度 S < 1/N 时，模型倾向于将特征以近似正交的方向编码，牺牲少量干扰换取更大的表示容量。这是 Claude 内部表示的核心组织原则。",
    "depth_hint": "需推导 superposition 的容量-干扰权衡公式，给出不同稀疏度下的最优表示策略",
    "status": "pending"
  },
  {
    "id": 230,
    "title": "Claude 的特征引导：激活工程",
    "slug": "claude-activation-engineering",
    "tags": [
      "模型解析",
      "Claude",
      "可解释性"
    ],
    "brief": "核心要点：通过 SAE 提取的特征可用于干预 Claude 的行为——放大'Golden Gate Bridge'特征使模型在所有回答中提及金门大桥。这种激活工程提供了比 prompt 更精细的行为控制。技术实现是在推理时对残差流添加特征方向的偏移向量 x' = x + α·f。",
    "depth_hint": "需给出激活干预的向量运算公式，分析干预强度 α 与行为变化的非线性关系",
    "status": "pending"
  },
  {
    "id": 231,
    "title": "Claude 的安全分层架构",
    "slug": "claude-safety-layered-architecture",
    "tags": [
      "模型解析",
      "Claude",
      "安全"
    ],
    "brief": "核心要点：Claude 的安全系统分为四层：预训练数据过滤、Constitutional AI 对齐训练、system prompt 级约束、输出过滤器。每层防御不同类型的攻击——数据过滤防训练污染，CAI 防基本有害请求，system prompt 防角色突破，输出过滤防漏网之鱼。",
    "depth_hint": "需绘制四层防御的覆盖范围矩阵，分析每层的典型失效模式",
    "status": "pending"
  },
  {
    "id": 232,
    "title": "Claude 的越狱攻击分类与防御",
    "slug": "claude-jailbreak-taxonomy-defense",
    "tags": [
      "模型解析",
      "Claude",
      "安全"
    ],
    "brief": "核心要点：针对 Claude 的越狱攻击可分为：角色扮演型（DAN）、编码绕过型（Base64/ROT13）、渐进诱导型（多轮渐进突破）、逻辑悖论型（制造伦理困境）。CAI 对直接攻击防御率>95%，但对多轮渐进攻击和编码绕过仍有约 5-10% 的成功率。",
    "depth_hint": "需分类各攻击类型的机制和示例模式，给出不同防御版本的攻击成功率对比",
    "status": "pending"
  },
  {
    "id": 233,
    "title": "Claude 的不确定性表达机制",
    "slug": "claude-uncertainty-expression",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：Claude 通过 CAI 中'诚实性'原则训练来表达认知不确定性，包括承认知识边界、区分事实与推测、给出置信度修饰语。内部机制上，模型的 logit 分布熵与其输出中不确定性表达的相关性约 0.3-0.5，表明模型对自身不确定性有一定感知能力。",
    "depth_hint": "需分析 logit 熵与不确定性表达的相关性实验，给出校准曲线数据",
    "status": "pending"
  },
  {
    "id": 234,
    "title": "Claude 的指令层级与优先级",
    "slug": "claude-instruction-hierarchy",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：Claude 的指令遵循存在隐式层级：宪法原则 > 安全训练 > system prompt > 用户指令。当低层级指令与高层级冲突时，模型倾向于拒绝低层级指令。这种层级通过训练中的数据配比和奖励信号强度编码，而非硬编码规则实现。",
    "depth_hint": "需分析各指令层级的冲突解决实验结果，给出不同冲突场景的遵循率数据",
    "status": "pending"
  },
  {
    "id": 235,
    "title": "Anthropic 的 Scaling 安全研究",
    "slug": "anthropic-scaling-safety-research",
    "tags": [
      "模型解析",
      "Claude",
      "安全"
    ],
    "brief": "核心要点：Anthropic 发现模型能力与安全性的 Scaling 并非同步——更大的模型更擅长遵循有害指令（能力提升），但 CAI 训练后的安全对齐也更有效（可控性提升）。关键发现是对齐税（alignment tax）随模型规模增大而降低：大模型同时做到安全和有用更容易。",
    "depth_hint": "需给出对齐税的定义公式，分析不同模型规模下 helpfulness-harmlessness 的 Pareto 曲线",
    "status": "pending"
  },
  {
    "id": 236,
    "title": "Claude 的 Prompt Engineering 原理",
    "slug": "claude-prompt-engineering-principles",
    "tags": [
      "模型解析",
      "Claude",
      "推理"
    ],
    "brief": "核心要点：Claude 的 prompt 敏感性源于自回归 Transformer 的条件生成本质：P(output|prompt) 高度依赖 prompt 的 token 序列。有效 prompt 的关键原则包括：明确角色设定、结构化输出格式、XML 标签分段、思维链引导。Claude 对 prompt 格式的敏感性低于 GPT-4。",
    "depth_hint": "需对比不同 prompt 格式对 Claude 输出质量的影响实验数据，分析 XML 标签的作用机制",
    "status": "pending"
  },
  {
    "id": 237,
    "title": "Claude 的 Chain-of-Thought 推理",
    "slug": "claude-chain-of-thought-reasoning",
    "tags": [
      "模型解析",
      "Claude",
      "推理"
    ],
    "brief": "核心要点：Claude 的 CoT 能力通过 RLHF 和 CAI 训练强化——模型学会在回答复杂问题前展开中间推理步骤。CoT 在数学推理上提升约 20-30%（GSM8K），在逻辑推理上提升约 15%。Claude 3.5 Sonnet 在 extended thinking 模式下进一步支持更长的推理链。",
    "depth_hint": "需对比有无 CoT 在多个基准上的性能差异，分析推理链长度与准确率的关系",
    "status": "pending"
  },
  {
    "id": 238,
    "title": "Claude 的工具使用能力训练",
    "slug": "claude-tool-use-training",
    "tags": [
      "模型解析",
      "Claude",
      "智能体"
    ],
    "brief": "核心要点：Claude 的工具使用（function calling）通过专门的微调数据训练，使模型学会将自然语言请求映射到结构化函数调用。训练数据包含工具定义 schema、调用示例和多步编排场景。Claude 3.5 在工具使用基准上达到 90%+ 准确率，支持并行调用和嵌套调用。",
    "depth_hint": "需分析工具调用的 JSON schema 解析机制，给出工具选择准确率的基准测试数据",
    "status": "pending"
  },
  {
    "id": 239,
    "title": "Claude 的知识蒸馏与模型家族",
    "slug": "claude-knowledge-distillation-family",
    "tags": [
      "模型解析",
      "Claude",
      "模型压缩"
    ],
    "brief": "核心要点：Claude 3 家族（Haiku/Sonnet/Opus）的三个规模档位可能通过知识蒸馏关联——大模型的输出分布作为小模型的软标签进行训练。蒸馏的关键是 temperature scaling：KL(P_teacher^(1/T) || P_student^(1/T))，T>1 时软标签包含更多暗知识。",
    "depth_hint": "需给出知识蒸馏的损失函数和 temperature 参数的作用推导，对比蒸馏前后的性能差异",
    "status": "pending"
  },
  {
    "id": 240,
    "title": "Claude 的幻觉检测与缓解",
    "slug": "claude-hallucination-detection-mitigation",
    "tags": [
      "模型解析",
      "Claude",
      "对齐"
    ],
    "brief": "核心要点：Claude 通过 CAI 中的诚实性原则减少幻觉——训练模型在不确定时说'我不确定'而非编造答案。幻觉类型分为事实性幻觉（错误事实）和忠实性幻觉（与输入矛盾）。Claude 3 在 TruthfulQA 上的得分约 85%，高于多数竞品，但长文本生成中幻觉率仍约 5-8%。",
    "depth_hint": "需分类幻觉类型和检测方法，给出 TruthfulQA 和长文本幻觉率的实验数据",
    "status": "pending"
  },
  {
    "id": 241,
    "title": "Gemini 1.0 的 MoE 架构解析",
    "slug": "gemini-1-moe-architecture",
    "tags": [
      "模型解析",
      "Gemini",
      "MoE"
    ],
    "brief": "核心要点：Gemini 1.0 采用稀疏混合专家架构，通过 Top-K 路由机制在推理时仅激活部分参数。分析其专家数量、路由策略、负载均衡损失函数设计，以及与 dense Transformer 在相同 FLOPs 下的性能对比",
    "depth_hint": "需包含路由函数公式、负载均衡辅助损失推导、专家利用率分布图",
    "status": "pending"
  },
  {
    "id": 242,
    "title": "Gemini 的原生多模态训练范式",
    "slug": "gemini-native-multimodal-training",
    "tags": [
      "模型解析",
      "Gemini",
      "多模态"
    ],
    "brief": "核心要点：Gemini 从预训练阶段即联合训练文本、图像、音频、视频，区别于 LLaVA 等后接适配器的方案。分析其统一 tokenizer 设计、跨模态注意力机制、多模态数据配比策略及其对涌现能力的影响",
    "depth_hint": "需对比 native multimodal 与 adapter-based 方案的损失曲线差异、数据配比实验",
    "status": "pending"
  },
  {
    "id": 243,
    "title": "Gemini 的视觉编码器：从 ViT 到统一表征",
    "slug": "gemini-visual-encoder-vit",
    "tags": [
      "模型解析",
      "Gemini",
      "视觉编码"
    ],
    "brief": "核心要点：Gemini 使用定制化 ViT 变体作为视觉编码器，支持动态分辨率输入。分析 patch embedding 策略、位置编码方案（2D RoPE 扩展）、视觉 token 与文本 token 的对齐机制及分辨率-性能权衡",
    "depth_hint": "需包含 2D 位置编码公式、不同分辨率下的 benchmark 数据、token 数量计算",
    "status": "pending"
  },
  {
    "id": 244,
    "title": "Gemini 1.5 的百万级上下文窗口实现",
    "slug": "gemini-1-5-million-context-window",
    "tags": [
      "模型解析",
      "Gemini",
      "长上下文"
    ],
    "brief": "核心要点：Gemini 1.5 Pro 支持 1M token 上下文，实际测试可达 10M。分析其基于 RingAttention 思想的分布式注意力计算、KV cache 压缩策略、长序列位置编码外推方法及 Needle-in-a-Haystack 测试表现",
    "depth_hint": "需包含注意力计算的内存复杂度公式、NIAH 不同深度的召回率数据",
    "status": "pending"
  },
  {
    "id": 245,
    "title": "Gemini 1.5 Pro 的 MoE 效率设计",
    "slug": "gemini-1-5-pro-moe-efficiency",
    "tags": [
      "模型解析",
      "Gemini",
      "MoE"
    ],
    "brief": "核心要点：Gemini 1.5 Pro 在总参数量级显著提升的同时保持推理成本可控。分析其 MoE 层的专家粒度选择、共享专家设计、激活参数比例优化，以及与 Mixtral 8x7B 在效率指标上的对比",
    "depth_hint": "需包含激活参数/总参数比、每 token FLOPs 计算、吞吐量基准对比",
    "status": "pending"
  },
  {
    "id": 246,
    "title": "Ring Attention：长序列分布式训练核心",
    "slug": "ring-attention-distributed-long-context",
    "tags": [
      "模型解析",
      "长上下文",
      "分布式训练"
    ],
    "brief": "核心要点：Ring Attention 将序列分片到多设备并通过环形通信重叠计算与传输，实现近线性序列长度扩展。分析其 blockwise attention 分解、通信隐藏策略、因果掩码处理及与 sequence parallelism 的关系",
    "depth_hint": "需包含通信复杂度分析、block 大小选择对效率的影响公式、设备间同步图",
    "status": "pending"
  },
  {
    "id": 247,
    "title": "Gemini 的音频理解架构",
    "slug": "gemini-audio-understanding-architecture",
    "tags": [
      "模型解析",
      "Gemini",
      "音频"
    ],
    "brief": "核心要点：Gemini 原生支持音频输入，可处理语音、音乐和环境声。分析其音频 tokenizer（基于 USM/Whisper 风格编码器）、音频-文本交叉注意力设计、采样率与 token 粒度的权衡及 ASR 任务表现",
    "depth_hint": "需包含音频特征提取流程图、mel-spectrogram 到 token 的映射参数、WER 对比数据",
    "status": "pending"
  },
  {
    "id": 248,
    "title": "Gemini 的视频理解：时序建模机制",
    "slug": "gemini-video-temporal-modeling",
    "tags": [
      "模型解析",
      "Gemini",
      "视频理解"
    ],
    "brief": "核心要点：Gemini 通过帧采样与时序注意力实现长视频理解，支持小时级视频输入。分析其关键帧选择策略、时序 token 压缩（temporal pooling）、视频 QA 中的因果推理机制及与 VideoLLaMA 的性能对比",
    "depth_hint": "需包含帧采样率与 token 数关系式、时序 pooling 具体方案、视频 benchmark 数据",
    "status": "pending"
  },
  {
    "id": 249,
    "title": "多模态位置编码：从 1D 到 N-D 的扩展",
    "slug": "multimodal-positional-encoding-nd",
    "tags": [
      "模型解析",
      "多模态",
      "位置编码"
    ],
    "brief": "核心要点：多模态模型需要处理文本（1D）、图像（2D）、视频（3D）的位置信息。系统分析 RoPE 的多维扩展、跨模态位置编码对齐方案、频率基数选择对不同模态分辨率的影响",
    "depth_hint": "需包含 N-D RoPE 数学推导、频率参数与分辨率关系公式、消融实验结论",
    "status": "pending"
  },
  {
    "id": 250,
    "title": "KV Cache 压缩：长上下文推理的核心优化",
    "slug": "kv-cache-compression-long-context",
    "tags": [
      "模型解析",
      "长上下文",
      "推理优化"
    ],
    "brief": "核心要点：百万级上下文推理中 KV cache 占用可达数百 GB。分析 GQA、MQA、Sliding Window、H2O（Heavy Hitter Oracle）、SnapKV 等压缩方案的原理、压缩比与精度损失权衡",
    "depth_hint": "需包含各方案内存占用公式、压缩比-perplexity 曲线、Gemini 可能采用的组合策略",
    "status": "pending"
  },
  {
    "id": 251,
    "title": "Gemini 2.0 Flash 的推理效率突破",
    "slug": "gemini-2-flash-inference-efficiency",
    "tags": [
      "模型解析",
      "Gemini",
      "推理优化"
    ],
    "brief": "核心要点：Gemini 2.0 Flash 在多模态能力持平 1.5 Pro 的同时大幅降低延迟和成本。分析其可能采用的知识蒸馏策略、架构简化方案、量化友好设计及 speculative decoding 集成",
    "depth_hint": "需包含 Flash 与 Pro 的延迟/吞吐量对比数据、蒸馏损失函数设计",
    "status": "pending"
  },
  {
    "id": 252,
    "title": "Gemini 的跨模态注意力融合策略",
    "slug": "gemini-cross-modal-attention-fusion",
    "tags": [
      "模型解析",
      "Gemini",
      "注意力机制"
    ],
    "brief": "核心要点：多模态融合有 early fusion、late fusion、cross-attention 等方案。分析 Gemini 的 early fusion 设计选择、不同模态 token 在自注意力中的交互模式、模态间信息流动的层级分布",
    "depth_hint": "需包含注意力权重的跨模态分布可视化描述、各融合方案的计算复杂度对比",
    "status": "pending"
  },
  {
    "id": 253,
    "title": "长上下文中的注意力稀疏性",
    "slug": "attention-sparsity-long-context",
    "tags": [
      "模型解析",
      "长上下文",
      "注意力机制"
    ],
    "brief": "核心要点：超长序列中注意力分布高度稀疏，大部分 token 对间注意力权重接近零。分析 attention sink 现象、局部注意力模式、稀疏注意力的理论保证及 Gemini 1.5 利用稀疏性提速的可能方案",
    "depth_hint": "需包含注意力熵的统计分析、稀疏度与序列长度关系、StreamingLLM 对比",
    "status": "pending"
  },
  {
    "id": 254,
    "title": "Gemini 的多模态安全对齐",
    "slug": "gemini-multimodal-safety-alignment",
    "tags": [
      "模型解析",
      "Gemini",
      "安全"
    ],
    "brief": "核心要点：多模态模型面临图像/音频注入攻击等新安全挑战。分析 Gemini 的多模态 RLHF、视觉安全分类器、跨模态对抗样本防御、以及 Google DeepMind 的 Red Teaming 框架",
    "depth_hint": "需包含多模态攻击分类体系、视觉对抗扰动检测方法、安全评估 benchmark 数据",
    "status": "pending"
  },
  {
    "id": 255,
    "title": "Gemini 的数学推理能力：多模态解题",
    "slug": "gemini-math-reasoning-multimodal",
    "tags": [
      "模型解析",
      "Gemini",
      "数学推理"
    ],
    "brief": "核心要点：Gemini 可直接理解数学公式图片和几何图形并进行推理。分析其视觉数学 OCR 精度、符号接地机制、CoT 在多模态数学中的应用及 MathVista 等 benchmark 表现",
    "depth_hint": "需包含 MathVista/MATH 分项得分对比、视觉符号到 LaTeX 的转换准确率",
    "status": "pending"
  },
  {
    "id": 256,
    "title": "Gemini 的代码生成与执行架构",
    "slug": "gemini-code-generation-execution",
    "tags": [
      "模型解析",
      "Gemini",
      "代码生成"
    ],
    "brief": "核心要点：Gemini 2.0 支持原生代码执行（code execution tool），可在推理过程中运行 Python。分析其沙箱设计、代码-推理交替机制、与 AlphaCode 2 的关系及 HumanEval/MBPP 表现",
    "depth_hint": "需包含代码执行流程架构图、tool-use 格式规范、编程 benchmark 得分对比",
    "status": "pending"
  },
  {
    "id": 257,
    "title": "Infini-Attention：无限上下文注意力",
    "slug": "infini-attention-infinite-context",
    "tags": [
      "模型解析",
      "长上下文",
      "注意力机制"
    ],
    "brief": "核心要点：Google 提出的 Infini-Attention 将压缩记忆与标准注意力结合，在固定内存下处理无限长序列。分析其 compressive memory 更新规则、linear attention 作为检索机制、与 segment-level recurrence 的融合",
    "depth_hint": "需包含 memory update 数学公式、门控融合机制推导、passkey retrieval 实验数据",
    "status": "pending"
  },
  {
    "id": 258,
    "title": "Gemini 的 Grounding 机制：搜索增强生成",
    "slug": "gemini-grounding-search-augmented",
    "tags": [
      "模型解析",
      "Gemini",
      "RAG"
    ],
    "brief": "核心要点：Gemini 原生集成 Google Search grounding，可在推理中调用搜索引擎获取实时信息。分析其 grounding 触发条件、检索结果融合机制、引用归因设计及与传统 RAG 的架构差异",
    "depth_hint": "需包含 grounding API 调用流程、置信度阈值设计、事实性评估数据",
    "status": "pending"
  },
  {
    "id": 259,
    "title": "GQA 在长上下文模型中的关键作用",
    "slug": "gqa-grouped-query-attention-long-context",
    "tags": [
      "模型解析",
      "长上下文",
      "注意力机制"
    ],
    "brief": "核心要点：Grouped Query Attention 通过共享 KV head 将 KV cache 压缩为 MHA 的 1/N。分析 GQA 的数学等价性、head 分组策略对质量的影响、从 MHA 到 GQA 的 uptrain 方法及在 Gemini 中的应用",
    "depth_hint": "需包含 GQA 注意力公式、不同分组数的 perplexity 对比、KV cache 节省量计算",
    "status": "pending"
  },
  {
    "id": 260,
    "title": "Gemini 的多模态 Tokenizer 设计",
    "slug": "gemini-multimodal-tokenizer-design",
    "tags": [
      "模型解析",
      "Gemini",
      "Tokenizer"
    ],
    "brief": "核心要点：Gemini 需要将文本、图像、音频、视频统一编码为 token 序列。分析其 SentencePiece 文本分词、视觉 patch tokenization、音频帧分词及跨模态特殊 token 设计",
    "depth_hint": "需包含各模态的 token 数量计算公式、词表大小与覆盖率、特殊 token 列表",
    "status": "pending"
  },
  {
    "id": 261,
    "title": "YaRN：位置编码外推的频率调制",
    "slug": "yarn-positional-encoding-extrapolation",
    "tags": [
      "模型解析",
      "长上下文",
      "位置编码"
    ],
    "brief": "核心要点：YaRN 通过分频段调节 RoPE 频率实现高效的上下文窗口扩展，无需全量重训练。分析其 NTK-aware 插值策略、attention scale 修正、temperature 参数设计及与 NTK-ALiBi 的对比",
    "depth_hint": "需包含频率分段公式、scale factor 推导、不同扩展倍数的 perplexity 对比",
    "status": "pending"
  },
  {
    "id": 262,
    "title": "Gemini 与 GPT-4V 的多模态能力对比",
    "slug": "gemini-vs-gpt4v-multimodal-comparison",
    "tags": [
      "模型解析",
      "Gemini",
      "多模态"
    ],
    "brief": "核心要点：系统对比 Gemini Ultra/Pro 与 GPT-4V 在视觉理解、文档分析、多模态推理等任务上的表现差异。分析架构设计差异（native 与 adapter）如何影响各项能力及各自优势领域",
    "depth_hint": "需包含 MMMU/MMBench/DocVQA 分项得分对比表、架构差异对性能影响分析",
    "status": "pending"
  },
  {
    "id": 263,
    "title": "Gemini 的 Function Calling 实现机制",
    "slug": "gemini-function-calling-mechanism",
    "tags": [
      "模型解析",
      "Gemini",
      "工具使用"
    ],
    "brief": "核心要点：Gemini 的 function calling 支持声明式工具定义与并行调用。分析其 schema 解析机制、参数提取准确率、multi-turn tool use 的状态管理及与 OpenAI function calling 的协议差异",
    "depth_hint": "需包含 function declaration JSON 格式、调用流程时序图、BFCL benchmark 数据",
    "status": "pending"
  },
  {
    "id": 264,
    "title": "长上下文评估方法论：超越 NIAH",
    "slug": "long-context-evaluation-beyond-niah",
    "tags": [
      "模型解析",
      "长上下文",
      "评估方法"
    ],
    "brief": "核心要点：Needle-in-a-Haystack 过于简单，无法区分真正的长程理解能力。分析 RULER、LongBench、InfiniteBench 等评估框架的设计理念、任务分类及 Gemini 1.5 在各维度的表现",
    "depth_hint": "需包含各 benchmark 的任务分类表、Gemini 1.5 Pro 在不同长度区间的得分曲线",
    "status": "pending"
  },
  {
    "id": 265,
    "title": "Gemini 的 Thinking 模式：显式推理链",
    "slug": "gemini-thinking-mode-reasoning",
    "tags": [
      "模型解析",
      "Gemini",
      "推理"
    ],
    "brief": "核心要点：Gemini 2.0 Flash Thinking 在输出前生成内部推理链。分析其 thinking token 的生成机制、推理预算控制、与 OpenAI o1 的技术路线差异及在数学/编程任务上的推理增益",
    "depth_hint": "需包含 thinking token 长度与准确率关系、推理任务 benchmark 对比、token 效率分析",
    "status": "pending"
  },
  {
    "id": 266,
    "title": "多模态对比学习：CLIP 到 SigLIP 演进",
    "slug": "multimodal-contrastive-learning-clip-siglip",
    "tags": [
      "模型解析",
      "多模态",
      "对比学习"
    ],
    "brief": "核心要点：SigLIP 用 sigmoid 损失替代 CLIP 的 softmax 对比损失，解除了对全局 batch 通信的依赖。分析其损失函数改进、训练效率提升、在 Gemini/PaLI 视觉编码器中的应用",
    "depth_hint": "需包含 CLIP 与 SigLIP 损失函数公式对比、batch size 影响实验、zero-shot 性能对比",
    "status": "pending"
  },
  {
    "id": 267,
    "title": "Gemini 的上下文缓存：成本优化机制",
    "slug": "gemini-context-caching-cost-optimization",
    "tags": [
      "模型解析",
      "Gemini",
      "推理优化"
    ],
    "brief": "核心要点：Gemini API 支持 context caching，将长 prefix 的 KV cache 持久化以降低重复计算成本。分析其缓存粒度、TTL 策略、prefix matching 算法及与 prompt caching 的技术差异",
    "depth_hint": "需包含缓存命中率与成本节省计算公式、缓存存储开销分析、适用场景决策树",
    "status": "pending"
  },
  {
    "id": 268,
    "title": "Gemini 的多模态嵌入空间分析",
    "slug": "gemini-multimodal-embedding-space",
    "tags": [
      "模型解析",
      "Gemini",
      "表征学习"
    ],
    "brief": "核心要点：Gemini 的不同模态在统一 embedding 空间中的分布与对齐质量直接影响跨模态理解。分析模态间的语义对齐度量、模态 gap 现象、层级表征的模态特异性变化",
    "depth_hint": "需包含 CKA 相似度分析方法、模态 gap 的量化指标、不同层的表征分布可视化描述",
    "status": "pending"
  },
  {
    "id": 269,
    "title": "Gemini 的 Sparse Attention 与 Flash Attention 融合",
    "slug": "gemini-sparse-flash-attention",
    "tags": [
      "模型解析",
      "Gemini",
      "注意力机制"
    ],
    "brief": "核心要点：长上下文模型需要同时利用稀疏注意力降低复杂度和 FlashAttention 优化 IO。分析 blockwise sparse attention 与 FlashAttention-2 的融合方案、稀疏模式选择（local+global）及在 TPU 上的适配",
    "depth_hint": "需包含 IO 复杂度分析公式、SRAM/HBM 数据流图、不同稀疏比例的速度-质量曲线",
    "status": "pending"
  },
  {
    "id": 270,
    "title": "Gemini 的多模态 RLHF 训练流程",
    "slug": "gemini-multimodal-rlhf-training",
    "tags": [
      "模型解析",
      "Gemini",
      "RLHF"
    ],
    "brief": "核心要点：多模态模型的 RLHF 需要处理图文混合输出的偏好标注。分析 Gemini 的多模态奖励模型设计、视觉安全约束的 reward shaping、多目标优化（有用性 vs 安全性）的 Pareto 策略",
    "depth_hint": "需包含多模态 reward model 架构、多目标损失函数、安全-有用性 Pareto 前沿图描述",
    "status": "pending"
  },
  {
    "id": 271,
    "title": "DeepSeek-V1 的架构设计与训练策略",
    "slug": "deepseek-v1-architecture",
    "tags": [
      "模型解析",
      "DeepSeek",
      "预训练"
    ],
    "brief": "核心要点：DeepSeek-V1（67B）的Dense架构选择，采用Pre-Norm + RMSNorm + SwiGLU组合，训练数据2T tokens的配比策略，与LLaMA 2架构的关键差异（GQA头数、FFN维度比），以及在代码和数学基准上的表现分析",
    "depth_hint": "需包含与LLaMA 2-70B的架构参数对比表，训练超参数设置，loss曲线分析",
    "status": "pending"
  },
  {
    "id": 272,
    "title": "DeepSeek-V2 的 MLA 注意力机制",
    "slug": "deepseek-v2-multi-latent-attention",
    "tags": [
      "模型解析",
      "DeepSeek",
      "注意力机制"
    ],
    "brief": "核心要点：Multi-head Latent Attention将KV缓存压缩至原始MHA的5-13%，通过低秩联合压缩将K和V投影到低维潜在空间（d_c=512），再解压恢复多头表示，相比GQA/MQA在保持性能的同时大幅降低推理显存",
    "depth_hint": "需推导KV压缩的矩阵分解公式，对比MHA/GQA/MQA/MLA的缓存量计算",
    "status": "pending"
  },
  {
    "id": 273,
    "title": "DeepSeek-V2 的 DeepSeekMoE 架构",
    "slug": "deepseek-v2-moe-architecture",
    "tags": [
      "模型解析",
      "DeepSeek",
      "MoE"
    ],
    "brief": "核心要点：DeepSeekMoE采用细粒度专家分割（160个路由专家+2个共享专家），每token激活6个路由专家，总参数236B但激活仅21B。共享专家捕获通用知识，路由专家实现专业化分工，相比GShard的粗粒度MoE提升2-6%",
    "depth_hint": "需包含专家数量与粒度的消融实验数据，负载均衡loss的具体公式",
    "status": "pending"
  },
  {
    "id": 274,
    "title": "DeepSeek-V3 的无辅助损失负载均衡",
    "slug": "deepseek-v3-auxiliary-loss-free-balancing",
    "tags": [
      "模型解析",
      "DeepSeek",
      "MoE"
    ],
    "brief": "核心要点：V3提出auxiliary-loss-free负载均衡策略，用动态偏置项b_i替代传统辅助损失函数。当专家负载过高时自动降低偏置、过低时提升，避免辅助损失与主损失的梯度冲突。实验表明该方法在保持均衡的同时提升0.3-0.5%的下游性能",
    "depth_hint": "需推导偏置更新规则，对比传统负载均衡loss的梯度干扰问题",
    "status": "pending"
  },
  {
    "id": 275,
    "title": "DeepSeek-V3 的 Multi-Token Prediction",
    "slug": "deepseek-v3-multi-token-prediction",
    "tags": [
      "模型解析",
      "DeepSeek",
      "训练方法"
    ],
    "brief": "核心要点：V3引入MTP模块，每个位置同时预测后续D个token（D=2），使用D个顺序连接的Transformer层作为预测头，共享主模型embedding。MTP作为训练信号密度增强手段，训练完成后可丢弃额外头，也可用于speculative decoding加速推理",
    "depth_hint": "需包含MTP loss的加权公式，speculative decoding的接受率数据",
    "status": "pending"
  },
  {
    "id": 276,
    "title": "DeepSeek-V3 的 FP8 混合精度训练",
    "slug": "deepseek-v3-fp8-mixed-precision",
    "tags": [
      "模型解析",
      "DeepSeek",
      "训练优化"
    ],
    "brief": "核心要点：V3在671B参数规模首次成功应用FP8训练，采用细粒度量化（tile-wise scaling）而非tensor-wise，将GEMM操作在E4M3格式下完成，累加器使用FP32保持精度。FP8训练使显存降低约40%，训练成本仅557万美元（2048 H800，约2个月）",
    "depth_hint": "需对比FP8/BF16训练的loss曲线差异，tile-wise量化的分块策略细节",
    "status": "pending"
  },
  {
    "id": 277,
    "title": "DeepSeek-R1 的强化学习推理训练",
    "slug": "deepseek-r1-reinforcement-learning",
    "tags": [
      "模型解析",
      "DeepSeek",
      "强化学习"
    ],
    "brief": "核心要点：R1采用纯RL训练路线（GRPO算法），不依赖监督微调的思维链数据。奖励模型仅基于结果正确性（数学验证/代码测试），模型自发涌现出反思、验证、回溯等推理行为。R1在AIME 2024达到79.8%，MATH-500达到97.3%",
    "depth_hint": "需包含GRPO的目标函数推导，与PPO的关键差异对比",
    "status": "pending"
  },
  {
    "id": 278,
    "title": "DeepSeek-R1-Zero 的推理涌现现象",
    "slug": "deepseek-r1-zero-emergent-reasoning",
    "tags": [
      "模型解析",
      "DeepSeek",
      "推理能力"
    ],
    "brief": "核心要点：R1-Zero直接在基础模型上做RL，无SFT冷启动，观察到推理行为的自然涌现：思维链长度从数百token增长到数千token，出现自我验证和回溯修正。但也存在可读性差、语言混杂等问题，揭示了RL训练中reasoning的本质特征",
    "depth_hint": "需展示训练过程中思维链长度变化曲线，涌现行为的具体case分析",
    "status": "pending"
  },
  {
    "id": 279,
    "title": "GRPO：DeepSeek 的群组相对策略优化",
    "slug": "grpo-group-relative-policy-optimization",
    "tags": [
      "模型解析",
      "DeepSeek",
      "强化学习"
    ],
    "brief": "核心要点：GRPO移除PPO中的Critic模型，对同一问题采样G个回答组成群组，用组内相对奖励排名替代价值函数估计baseline。优势函数A_i=(r_i-mean)/std，大幅减少训练显存（省去与Policy同等规模的Critic），训练吞吐提升约50%",
    "depth_hint": "需完整推导GRPO的目标函数，与PPO/REINFORCE/RLOO的梯度估计方差对比",
    "status": "pending"
  },
  {
    "id": 280,
    "title": "DeepSeek-R1 的蒸馏策略与小模型推理",
    "slug": "deepseek-r1-distillation-small-models",
    "tags": [
      "模型解析",
      "DeepSeek",
      "知识蒸馏"
    ],
    "brief": "核心要点：R1将671B模型的长思维链能力蒸馏到1.5B-70B的Qwen和LLaMA系列。蒸馏数据约80万条R1生成的推理轨迹，仅做SFT即获得显著推理提升。R1-Distill-Qwen-32B在AIME上达72.6%，超越OpenAI o1-mini，证明小模型可习得长链推理",
    "depth_hint": "需包含各蒸馏模型的基准对比表，蒸馏数据筛选标准",
    "status": "pending"
  },
  {
    "id": 281,
    "title": "MoE 路由机制：从 Top-K 到专家选择",
    "slug": "moe-routing-topk-to-expert-choice",
    "tags": [
      "模型解析",
      "MoE",
      "路由策略"
    ],
    "brief": "核心要点：MoE路由从Token-Choice（每token选K个专家）演进到Expert-Choice（每专家选固定数token）。Token-Choice存在负载不均和token丢弃问题，Expert-Choice通过转置选择矩阵实现完美均衡，但引入因果性问题（未来token信息泄露），需要特殊处理",
    "depth_hint": "需对比两种路由的数学公式，讨论Expert-Choice在自回归中的因果mask方案",
    "status": "pending"
  },
  {
    "id": 282,
    "title": "MoE 的专家崩塌与负载均衡",
    "slug": "moe-expert-collapse-load-balancing",
    "tags": [
      "模型解析",
      "MoE",
      "训练稳定性"
    ],
    "brief": "核心要点：MoE训练中路由器易收敛到仅使用少数专家的退化状态（专家崩塌）。解决方案包括：辅助均衡损失（L_balance）、随机路由噪声（Noisy Top-K）、专家容量因子（capacity factor）、Z-loss正则化。各方法在均衡效果与模型性能间存在权衡",
    "depth_hint": "需推导L_balance和Z-loss的具体公式，展示崩塌时的专家利用率分布",
    "status": "pending"
  },
  {
    "id": 283,
    "title": "MoE 中共享专家的设计原理",
    "slug": "moe-shared-expert-design",
    "tags": [
      "模型解析",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：DeepSeekMoE引入共享专家（Shared Expert）捕获跨领域通用知识，避免路由专家重复学习公共特征。共享专家始终激活，与路由专家的输出相加。实验表明2个共享专家+128个路由专家优于纯160路由专家配置，通用知识与专业知识的解耦提升参数效率",
    "depth_hint": "需包含有无共享专家的消融实验数据，专家激活模式的可视化分析",
    "status": "pending"
  },
  {
    "id": 284,
    "title": "MoE 的细粒度专家分割策略",
    "slug": "moe-fine-grained-expert-segmentation",
    "tags": [
      "模型解析",
      "MoE",
      "架构设计"
    ],
    "brief": "核心要点：DeepSeekMoE将传统粗粒度专家（如16个大专家）拆分为更多细粒度专家（如256个小专家），每个专家FFN维度缩小但数量增多，激活组合更灵活。在总参数和激活参数不变的情况下，细粒度分割使专家组合数从C(16,2)增至C(256,8)，表达能力指数级提升",
    "depth_hint": "需计算不同粒度下的组合数对比，展示专家利用率分布变化",
    "status": "pending"
  },
  {
    "id": 285,
    "title": "DeepSeek-V2 的 RoPE 解耦设计",
    "slug": "deepseek-v2-rope-decoupled-design",
    "tags": [
      "模型解析",
      "DeepSeek",
      "位置编码"
    ],
    "brief": "核心要点：MLA中的低秩压缩与RoPE的位置相关性存在冲突——RoPE作用于Q/K后无法被压缩矩阵吸收。V2的解决方案是将Q/K各分出一小部分维度（d_r=64）专门携带RoPE，其余维度走压缩路径，两部分拼接后做注意力，兼顾位置感知与缓存压缩",
    "depth_hint": "需推导RoPE与低秩压缩不兼容的数学原因，解耦后的注意力计算公式",
    "status": "pending"
  },
  {
    "id": 286,
    "title": "DeepSeek-V3 的 DualPipe 并行策略",
    "slug": "deepseek-v3-dualpipe-parallelism",
    "tags": [
      "模型解析",
      "DeepSeek",
      "分布式训练"
    ],
    "brief": "核心要点：V3提出DualPipe流水线并行，将前向和反向计算与All-to-All通信双向重叠。通过将micro-batch从两端同时注入流水线，计算与通信的重叠率接近100%，pipeline bubble降至接近零。在2048卡H800集群上实现单卡180 TFLOPS的MFU",
    "depth_hint": "需画出DualPipe的时序调度图，对比1F1B/ZeroBubble的bubble率",
    "status": "pending"
  },
  {
    "id": 287,
    "title": "DeepSeek Coder 的代码预训练策略",
    "slug": "deepseek-coder-pretraining-strategy",
    "tags": [
      "模型解析",
      "DeepSeek",
      "代码生成"
    ],
    "brief": "核心要点：DeepSeek-Coder在2T tokens上训练（87%代码+13%自然语言），采用repo-level代码组织和FIM（Fill-in-the-Middle）训练。仓库级上下文通过依赖拓扑排序文件顺序，FIM以PSM模式（Prefix-Suffix-Middle）训练，使模型具备代码补全和跨文件理解能力",
    "depth_hint": "需包含FIM的三种模式对比，仓库级拓扑排序的具体算法",
    "status": "pending"
  },
  {
    "id": 288,
    "title": "DeepSeek-Math 的数学推理训练",
    "slug": "deepseek-math-reasoning-training",
    "tags": [
      "模型解析",
      "DeepSeek",
      "数学推理"
    ],
    "brief": "核心要点：DeepSeek-Math从Common Crawl中提取120B高质量数学语料，使用fastText分类器筛选。RL阶段采用GRPO在MATH和GSM8K上训练，引入过程奖励和结果奖励的混合信号。7B模型在MATH上达51.7%（超越GPT-4早期），验证了数据质量>数据规模的假设",
    "depth_hint": "需描述数学语料筛选pipeline，展示过程奖励vs结果奖励的消融实验",
    "status": "pending"
  },
  {
    "id": 289,
    "title": "MoE 的 All-to-All 通信优化",
    "slug": "moe-all-to-all-communication-optimization",
    "tags": [
      "模型解析",
      "MoE",
      "分布式系统"
    ],
    "brief": "核心要点：MoE的Expert Parallelism需要All-to-All通信将token分发到对应专家所在的GPU。通信量为O(B×d×E/N)，成为MoE扩展瓶颈。优化手段包括：分层All-to-All（节点内NVLINK+节点间IB）、通信-计算重叠、token压缩传输（FP8量化）",
    "depth_hint": "需计算不同规模下的通信量，DeepSeek-V3的节点内外通信分层策略",
    "status": "pending"
  },
  {
    "id": 290,
    "title": "DeepSeek-V3 的训练数据配方",
    "slug": "deepseek-v3-training-data-recipe",
    "tags": [
      "模型解析",
      "DeepSeek",
      "预训练"
    ],
    "brief": "核心要点：V3预训练14.8T tokens，两阶段数据配方：第一阶段以通用文本为主，第二阶段提升数学、代码、STEM占比并引入长文本（最长128K）。数据质量控制包括去重（MinHash+精确匹配）、毒性过滤、质量分类器打分，开源数据占比约60%",
    "depth_hint": "需包含两阶段数据配比的具体比例，质量筛选pipeline的各阶段保留率",
    "status": "pending"
  },
  {
    "id": 291,
    "title": "推理强化训练中的奖励设计",
    "slug": "reasoning-rl-reward-design",
    "tags": [
      "模型解析",
      "强化学习",
      "推理训练"
    ],
    "brief": "核心要点：推理RL的奖励设计分为结果奖励（ORM）和过程奖励（PRM）。ORM仅看最终答案正确性，实现简单但奖励稀疏；PRM对每步推理打分，信号密集但标注成本高。R1证明纯ORM配合GRPO即可涌现推理，而PRM+MCTS路线（如AlphaProof）在数学竞赛更强",
    "depth_hint": "需对比ORM/PRM的数学形式，讨论奖励hack现象及其缓解",
    "status": "pending"
  },
  {
    "id": 292,
    "title": "MoE 推理加速：专家卸载与预取",
    "slug": "moe-inference-expert-offloading-prefetch",
    "tags": [
      "模型解析",
      "MoE",
      "推理优化"
    ],
    "brief": "核心要点：MoE模型总参数量大但每token仅激活少数专家，推理时可将不活跃专家卸载到CPU/NVMe。关键是路由预测——利用前一层路由结果预测下一层激活专家并预取到GPU。DeepSeek-V3的671B参数在单节点8卡上可通过专家缓存+预取实现可用推理速度",
    "depth_hint": "需包含专家缓存命中率与推理速度的关系曲线，预取策略的延迟隐藏分析",
    "status": "pending"
  },
  {
    "id": 293,
    "title": "DeepSeek-R1 的冷启动数据构建",
    "slug": "deepseek-r1-cold-start-data",
    "tags": [
      "模型解析",
      "DeepSeek",
      "数据工程"
    ],
    "brief": "核心要点：R1（非Zero版本）使用数千条高质量长思维链数据做SFT冷启动，再接RL训练。冷启动数据包含详细推理步骤、自我反思和验证，格式为<think>...</think>标签包裹。冷启动解决了R1-Zero的可读性和语言混杂问题，同时保留了RL涌现推理的优势",
    "depth_hint": "需描述冷启动数据的具体格式规范，与R1-Zero在可读性指标上的对比",
    "status": "pending"
  },
  {
    "id": 294,
    "title": "Switch Transformer 到 V-MoE 的演进",
    "slug": "switch-transformer-to-vmoe-evolution",
    "tags": [
      "模型解析",
      "MoE",
      "架构演进"
    ],
    "brief": "核心要点：Switch Transformer（2021）首次将MoE扩展到Top-1路由，简化实现但牺牲表达力。V-MoE（2022）将MoE引入ViT并提出优先路由（priority routing），按token重要性分配专家容量。从GShard的Top-2到Switch的Top-1再到V-MoE的自适应，路由策略持续演进",
    "depth_hint": "需对比三者的路由公式和容量因子设计，展示训练稳定性差异",
    "status": "pending"
  },
  {
    "id": 295,
    "title": "Mixtral 8x7B 与 DeepSeek MoE 对比",
    "slug": "mixtral-vs-deepseek-moe-comparison",
    "tags": [
      "模型解析",
      "MoE",
      "模型对比"
    ],
    "brief": "核心要点：Mixtral采用标准Top-2路由+8个粗粒度专家（46.7B总参/12.9B激活），DeepSeekMoE采用Top-6路由+160细粒度专家+2共享专家。在相近激活参数下，DeepSeekMoE通过细粒度分割和共享专家获得更好的专家利用效率，但通信开销更大",
    "depth_hint": "需列出两者的完整架构参数对比表，在MMLU/GSM8K等基准的性能对比",
    "status": "pending"
  },
  {
    "id": 296,
    "title": "推理模型的测试时计算扩展",
    "slug": "test-time-compute-scaling-reasoning",
    "tags": [
      "模型解析",
      "推理训练",
      "Scaling Law"
    ],
    "brief": "核心要点：推理模型通过增加测试时计算（更长思维链、多次采样）提升性能，形成test-time compute scaling law。R1在AIME上pass@1=79.8%，但maj@64可达90%+，揭示推理任务中测试时计算的scaling指数约0.3-0.5。与预训练scaling law形成互补的两条曲线",
    "depth_hint": "需画出test-time compute的scaling曲线，对比预训练FLOPs与推理FLOPs的边际收益",
    "status": "pending"
  },
  {
    "id": 297,
    "title": "DeepSeek-V3 的长上下文扩展训练",
    "slug": "deepseek-v3-long-context-extension",
    "tags": [
      "模型解析",
      "DeepSeek",
      "长上下文"
    ],
    "brief": "核心要点：V3通过两阶段扩展上下文至128K：第一阶段用YaRN调整RoPE基频从10K到100K（在4K→32K数据上训练），第二阶段在32K→128K数据上继续训练。MLA中仅对解耦的RoPE维度（d_r=64）做频率调整，压缩维度不受影响，简化了长上下文适配",
    "depth_hint": "需包含YaRN频率调整的具体参数，Needle-in-a-Haystack在128K的准确率",
    "status": "pending"
  },
  {
    "id": 298,
    "title": "DeepSeek-R1 的拒绝采样与多阶段训练",
    "slug": "deepseek-r1-rejection-sampling-pipeline",
    "tags": [
      "模型解析",
      "DeepSeek",
      "训练流程"
    ],
    "brief": "核心要点：R1的完整训练流水线：SFT冷启动→RL推理训练→拒绝采样生成80万条数据→全场景SFT→全场景RL。拒绝采样从RL checkpoint生成多个回答，按正确性和格式筛选，将推理能力扩展到写作、翻译等非推理任务，实现通用能力与推理能力的统一",
    "depth_hint": "需详细描述四阶段pipeline，拒绝采样的筛选标准和通过率",
    "status": "pending"
  },
  {
    "id": 299,
    "title": "MoE 模型的 Upcycling 初始化策略",
    "slug": "moe-upcycling-initialization",
    "tags": [
      "模型解析",
      "MoE",
      "训练策略"
    ],
    "brief": "核心要点：Upcycling将已训练的Dense模型转换为MoE：复制FFN层N次作为N个专家的初始化，路由器随机初始化。相比从头训练MoE，Upcycling节省50-70%训练计算量。关键问题是初始路由崩塌（所有专家相同导致梯度相同），需加入噪声或差异化初始化打破对称性",
    "depth_hint": "需对比Upcycling与从头训练的loss收敛曲线，对称性打破的具体方法",
    "status": "pending"
  },
  {
    "id": 300,
    "title": "Codex 的代码生成训练流程",
    "slug": "codex-code-generation-training",
    "tags": [
      "模型解析",
      "代码模型",
      "GPT"
    ],
    "brief": "核心要点：OpenAI Codex 基于 GPT-3 12B 参数微调，使用 159GB GitHub 代码数据。分析其从自然语言到代码的微调策略、Docstring-to-Code 的训练目标设计，以及 pass@k 评估指标的统计方法（k=1 时 HumanEval 达 28.8%）",
    "depth_hint": "需包含 pass@k 的无偏估计公式推导、温度参数对采样多样性的影响实验数据",
    "status": "pending"
  },
  {
    "id": 301,
    "title": "CodeLlama 的长上下文填充训练",
    "slug": "codellama-long-context-infilling",
    "tags": [
      "模型解析",
      "代码模型",
      "Llama"
    ],
    "brief": "核心要点：CodeLlama 基于 Llama 2 进行 500B token 代码续训，支持 100K 上下文。解析其长上下文微调（LCFT）阶段将 RoPE 基频从 10000 修改为 1000000 的外推策略，以及 Fill-in-the-Middle（FIM）训练中 PSM/SPM 两种格式的效果对比",
    "depth_hint": "需包含 RoPE 频率修改的数学推导、FIM 训练中 PSM 与 SPM 的 pass@1 对比数据",
    "status": "pending"
  },
  {
    "id": 302,
    "title": "DeepSeek-Coder-V2 的 MoE 代码模型设计",
    "slug": "deepseek-coder-v2-moe-architecture",
    "tags": [
      "模型解析",
      "代码模型",
      "DeepSeek"
    ],
    "brief": "核心要点：DeepSeek-Coder-V2 采用 236B 总参数/21B 激活参数的 MoE 架构，支持 338 种编程语言和 128K 上下文。分析其从 Dense 到 MoE 的代码模型演进、额外 6T token 的续训策略，以及在 HumanEval/MBPP 等基准上超越 GPT-4 Turbo 的关键因素",
    "depth_hint": "需包含与 CodeLlama-70B、GPT-4 Turbo 的多基准对比表、MoE 专家在代码任务中的激活模式分析",
    "status": "pending"
  },
  {
    "id": 303,
    "title": "StarCoder 的多语言代码预训练",
    "slug": "starcoder-multilingual-pretraining",
    "tags": [
      "模型解析",
      "代码模型",
      "开源模型"
    ],
    "brief": "核心要点：StarCoder 15.5B 参数，在 The Stack v1 的 80+ 编程语言上训练 1T token。分析其数据去重与 PII 过滤流程、Multi-Query Attention 的推理加速效果（约 1.5x），以及 Fill-in-the-Middle 训练对代码补全任务的增益",
    "depth_hint": "需包含 The Stack 数据处理流水线细节、MQA vs MHA 的推理吞吐对比数据",
    "status": "pending"
  },
  {
    "id": 304,
    "title": "StarCoder2 的分阶段训练策略",
    "slug": "starcoder2-staged-training",
    "tags": [
      "模型解析",
      "代码模型",
      "开源模型"
    ],
    "brief": "核心要点：StarCoder2 提供 3B/7B/15B 三个规模，在 The Stack v2（3.3-4.3T token）上训练。分析其分阶段数据配方（代码→高质量代码→指令数据）、Grouped-Query Attention 的引入，以及 15B 模型在 HumanEval 上达到 46.3% 的训练细节",
    "depth_hint": "需包含三阶段训练的数据配比表、不同规模模型的 scaling 效果对比",
    "status": "pending"
  },
  {
    "id": 305,
    "title": "CodeGemma 的代码与数学联合训练",
    "slug": "codegemma-code-math-joint-training",
    "tags": [
      "模型解析",
      "代码模型",
      "Gemma"
    ],
    "brief": "核心要点：CodeGemma 基于 Gemma 2B/7B 进行 500B token 代码续训，其中 80% 为代码、20% 为自然语言与数学数据。分析代码与数学数据联合训练对逻辑推理能力的协同增益，以及 FIM 训练中 token 级别填充 vs 行级别填充的效果差异",
    "depth_hint": "需包含数学数据占比对 GSM8K 得分的消融实验、FIM 粒度对比数据",
    "status": "pending"
  },
  {
    "id": 306,
    "title": "WizardCoder 的 Evol-Instruct 代码指令进化",
    "slug": "wizardcoder-evol-instruct",
    "tags": [
      "模型解析",
      "代码模型",
      "指令微调"
    ],
    "brief": "核心要点：WizardCoder 将 Evol-Instruct 方法适配到代码领域，通过增加约束、深化需求、并发需求、替换算法、增加推理步骤五种进化算子，从 20K 种子指令进化出 78K 代码指令。15B 模型在 HumanEval 达 57.3%，超越闭源模型",
    "depth_hint": "需包含五种进化算子的具体示例、进化轮次对指令质量的影响曲线",
    "status": "pending"
  },
  {
    "id": 307,
    "title": "SantaCoder 的 FIM 训练目标设计",
    "slug": "santacoder-fim-training-objective",
    "tags": [
      "模型解析",
      "代码模型",
      "训练方法"
    ],
    "brief": "核心要点：SantaCoder 1.1B 参数在 Python/Java/JavaScript 上训练，系统研究了 Fill-in-the-Middle 训练。对比 Prefix-Suffix-Middle（PSM）与 Suffix-Prefix-Middle（SPM）两种格式，发现 SPM 在 single-line infilling 上更优。FIM rate 从 0% 到 100% 的消融实验显示 50% 为最优",
    "depth_hint": "需包含 PSM/SPM 的 token 排列公式、FIM rate 消融曲线及最优值分析",
    "status": "pending"
  },
  {
    "id": 308,
    "title": "AlphaCode 的大规模采样与过滤策略",
    "slug": "alphacode-sampling-filtering",
    "tags": [
      "模型解析",
      "代码模型",
      "竞赛编程"
    ],
    "brief": "核心要点：AlphaCode 在 Codeforces 竞赛中达到 top 54% 水平。核心策略：每题生成百万级代码样本，通过测试用例过滤、聚类去重将候选解压缩到 10 个提交。分析其 41B 参数 Encoder-Decoder 架构选择、GOLD 训练目标（离线强化学习）、以及基于执行结果的聚类方法",
    "depth_hint": "需包含 GOLD 损失函数公式、采样-过滤-聚类流水线的数量级变化数据",
    "status": "pending"
  },
  {
    "id": 309,
    "title": "AlphaCode 2 的多模型协作代码生成",
    "slug": "alphacode2-multi-model-collaboration",
    "tags": [
      "模型解析",
      "代码模型",
      "Gemini"
    ],
    "brief": "核心要点：AlphaCode 2 基于 Gemini Pro 微调，在 Codeforces 达到 top 15% 水平。分析其两阶段策略：先用微调模型大规模采样生成代码，再用 Gemini 评分模型对候选解排序。评分模型的 token-level 似然评分 vs 解的正确性之间的对齐机制",
    "depth_hint": "需包含与 AlphaCode 1 的性能对比表、评分模型训练数据构造方法",
    "status": "pending"
  },
  {
    "id": 310,
    "title": "CodeT 的双重执行验证机制",
    "slug": "codet-dual-execution-verification",
    "tags": [
      "模型解析",
      "代码模型",
      "测试生成"
    ],
    "brief": "核心要点：CodeT 同时生成代码解和测试用例，通过交叉执行进行双重验证。对于每个代码解，统计其通过的测试用例数量进行排序，HumanEval pass@1 从 65.8% 提升至 85.2%。分析 Dual Execution Agreement 机制的理论基础及其与 self-consistency 的区别",
    "depth_hint": "需包含排序评分函数的形式化定义、不同采样数量下的性能增益曲线",
    "status": "pending"
  },
  {
    "id": 311,
    "title": "Qwen2.5-Coder 的大规模代码数据工程",
    "slug": "qwen25-coder-data-engineering",
    "tags": [
      "模型解析",
      "代码模型",
      "Qwen"
    ],
    "brief": "核心要点：Qwen2.5-Coder 在 5.5T token 代码数据上训练，覆盖 92 种编程语言。分析其多阶段数据清洗流水线（文件级去重→仓库级去重→质量过滤→依赖排序）、代码与文本的最优混合比例实验（代码占比 70%），以及 32B 模型在 HumanEval+ 上达 73.2% 的关键因素",
    "depth_hint": "需包含数据清洗各阶段的过滤率统计、代码/文本混合比例消融实验数据",
    "status": "pending"
  },
  {
    "id": 312,
    "title": "代码模型的仓库级上下文建模",
    "slug": "repo-level-context-modeling",
    "tags": [
      "模型解析",
      "代码模型",
      "长上下文"
    ],
    "brief": "核心要点：对比 RepoCoder、RepoFusion、CrossCodeEval 等仓库级代码补全方法。分析如何从跨文件依赖图中检索相关上下文、BM25 vs Dense Retrieval 在代码检索中的效果差异，以及 Repository-level FIM 训练如何提升跨文件补全准确率（相对提升 15-30%）",
    "depth_hint": "需包含跨文件依赖图构建算法、检索增强 vs 长上下文两种方案的对比实验",
    "status": "pending"
  },
  {
    "id": 313,
    "title": "代码模型的 Tokenizer 设计差异",
    "slug": "code-model-tokenizer-design",
    "tags": [
      "模型解析",
      "代码模型",
      "分词器"
    ],
    "brief": "核心要点：对比 Codex（GPT BPE）、CodeLlama（SentencePiece）、StarCoder（Hugging Face BPE）的分词策略差异。分析空白符编码（空格 vs 制表符压缩）、编程语言特殊 token 设计、词表大小对代码任务的影响（32K vs 49K vs 100K），以及多语言代码分词的公平性问题",
    "depth_hint": "需包含不同 Tokenizer 在 Python/C++/Rust 上的 token 效率对比表",
    "status": "pending"
  },
  {
    "id": 314,
    "title": "BLOOM 的多语言训练数据平衡策略",
    "slug": "bloom-multilingual-data-balance",
    "tags": [
      "模型解析",
      "多语言模型",
      "训练策略"
    ],
    "brief": "核心要点：BLOOM 176B 参数，在 ROOTS 语料库（46 种自然语言 + 13 种编程语言，1.6T token）上训练。分析其语言采样策略：高资源语言降采样（α=0.3 的温度采样），低资源语言上采样，以及这种平衡对各语言性能的非线性影响",
    "depth_hint": "需包含温度采样公式 p_i^(1/α)/Σp_j^(1/α)、不同 α 值下各语言困惑度变化",
    "status": "pending"
  },
  {
    "id": 315,
    "title": "mGPT 的跨语言迁移涌现",
    "slug": "mgpt-crosslingual-transfer-emergence",
    "tags": [
      "模型解析",
      "多语言模型",
      "涌现能力"
    ],
    "brief": "核心要点：mGPT 1.3B 在 60 种语言上训练，发现 few-shot 跨语言迁移的涌现现象：模型未经对齐训练即可将英语 few-shot 示例的推理模式迁移到低资源语言。分析共享子词重叠率、语系距离与迁移效果的相关性，以及「意外跨语言泛化」的成因假说",
    "depth_hint": "需包含跨语言迁移矩阵（源语言→目标语言准确率）、子词重叠率与迁移效果的回归分析",
    "status": "pending"
  },
  {
    "id": 316,
    "title": "Aya 模型的多语言指令微调",
    "slug": "aya-multilingual-instruction-tuning",
    "tags": [
      "模型解析",
      "多语言模型",
      "指令微调"
    ],
    "brief": "核心要点：Aya 模型覆盖 101 种语言，使用人工标注的 Aya Dataset（204K 条跨语言指令）微调。分析其数据收集的社区驱动模式、模板化翻译 vs 原生标注的质量差异，以及在低资源语言上相对 mT5/BLOOMZ 最高 25% 的性能提升",
    "depth_hint": "需包含各语言数据量分布图、翻译数据 vs 原生数据的人工评估对比",
    "status": "pending"
  },
  {
    "id": 317,
    "title": "NLLB 的低资源语言翻译架构",
    "slug": "nllb-low-resource-translation",
    "tags": [
      "模型解析",
      "多语言模型",
      "机器翻译"
    ],
    "brief": "核心要点：Meta NLLB-200 支持 200 种语言互译，使用 54.5B 参数 MoE 架构。分析其 Conditional Computation（语言路由的 MoE）设计、LASER3 跨语言嵌入用于平行语料挖掘、以及针对极低资源语言（<1M 句对）的回译数据增强策略",
    "depth_hint": "需包含 MoE 路由的语言聚类可视化、回译增强对低资源语言 BLEU 的提升数据",
    "status": "pending"
  },
  {
    "id": 318,
    "title": "多语言 Tokenizer 的词表分配公平性",
    "slug": "multilingual-tokenizer-vocabulary-fairness",
    "tags": [
      "模型解析",
      "多语言模型",
      "分词器"
    ],
    "brief": "核心要点：多语言模型的 BPE/SentencePiece 词表在不同语言间存在严重不公平：同义句在中文需要的 token 数量是英文的 1.5-3 倍。分析 XLM-R、mT5、BLOOM 的词表分配策略差异，以及 token 效率不均衡对模型性能和推理成本的影响",
    "depth_hint": "需包含多语言 fertility 指标定义及对比表、词表大小与公平性的 trade-off 曲线",
    "status": "pending"
  },
  {
    "id": 319,
    "title": "GLM-4 的中英双语对齐训练",
    "slug": "glm4-bilingual-alignment",
    "tags": [
      "模型解析",
      "多语言模型",
      "ChatGLM"
    ],
    "brief": "核心要点：GLM-4 在中英双语场景下实现接近 GPT-4 的性能。分析其预训练数据中中英比例的动态调整策略、中英文 SFT 数据的交叉对齐方法、以及如何在 RLHF 阶段保持双语能力不退化的正则化技巧",
    "depth_hint": "需包含中英基准（MMLU/C-Eval）的交叉对比表、双语 RLHF 的奖励模型设计",
    "status": "pending"
  },
  {
    "id": 320,
    "title": "Minerva 的数学推理预训练",
    "slug": "minerva-math-pretraining",
    "tags": [
      "模型解析",
      "数学推理",
      "PaLM"
    ],
    "brief": "核心要点：Google Minerva 基于 PaLM 540B 在 38.5B token 数学数据（arXiv + 网页数学内容）上续训。分析其 LaTeX 公式归一化处理、数学特有 token 的引入，以及在 MATH 基准上达到 50.3% 的多数投票（maj@k）策略",
    "depth_hint": "需包含 maj@k 评估公式、不同 k 值下准确率变化曲线、数学数据预处理流水线",
    "status": "pending"
  },
  {
    "id": 321,
    "title": "Llemma 的开源数学语言模型",
    "slug": "llemma-open-math-language-model",
    "tags": [
      "模型解析",
      "数学推理",
      "开源模型"
    ],
    "brief": "核心要点：Llemma 基于 CodeLlama 在 Proof-Pile-2（55B token 数学数据）上续训，7B/34B 模型开源。分析其训练数据构成（数学论文 + 数学代码 + 数学网页）、代码预训练对数学推理的正迁移效应，以及在 MATH 上无需微调即超越 Minerva 540B 的原因",
    "depth_hint": "需包含 Proof-Pile-2 的数据来源配比、代码续训 vs 直接数学续训的消融实验",
    "status": "pending"
  },
  {
    "id": 322,
    "title": "数学推理中的思维链格式设计",
    "slug": "math-reasoning-cot-format-design",
    "tags": [
      "模型解析",
      "数学推理",
      "提示工程"
    ],
    "brief": "核心要点：对比数学推理中不同 CoT 格式的效果：自然语言推理、代码形式推理（PoT/PAL）、LaTeX 形式推理、结构化 JSON 推理。分析 Program-of-Thought 将准确率从 58.6% 提升至 71.3% 的机制，以及代码执行作为外部验证器的可靠性分析",
    "depth_hint": "需包含四种 CoT 格式的具体示例对比、各格式在 GSM8K/MATH 上的准确率对比表",
    "status": "pending"
  },
  {
    "id": 323,
    "title": "数学推理的过程奖励模型 PRM",
    "slug": "math-process-reward-model",
    "tags": [
      "模型解析",
      "数学推理",
      "奖励模型"
    ],
    "brief": "核心要点：OpenAI 提出过程奖励模型（PRM）对数学推理的每个步骤进行评分，对比结果奖励模型（ORM）仅评判最终答案。PRM800K 数据集包含 75K 道题、800K 步骤级标注。分析 PRM 在 best-of-N 采样中将 MATH 准确率从 72.4% 提升至 78.2% 的机制",
    "depth_hint": "需包含 PRM vs ORM 的形式化定义、步骤级标注的一致性分析、best-of-N 的最优 N 值",
    "status": "pending"
  },
  {
    "id": 324,
    "title": "MetaMath 的数学数据增强方法",
    "slug": "metamath-data-augmentation",
    "tags": [
      "模型解析",
      "数学推理",
      "数据增强"
    ],
    "brief": "核心要点：MetaMath 通过四种数据增强策略（答案增强、问题重述、FOBAR 反向推理、Self-Verification）将 GSM8K 和 MATH 数据从 15K 扩展到 395K。7B 模型在 GSM8K 达 66.5%、MATH 达 19.8%，分析各增强策略的独立贡献和互补效应",
    "depth_hint": "需包含四种增强策略的具体示例、各策略的消融实验贡献度数据",
    "status": "pending"
  },
  {
    "id": 325,
    "title": "InternLM-Math 的统一数学推理框架",
    "slug": "internlm-math-unified-framework",
    "tags": [
      "模型解析",
      "数学推理",
      "InternLM"
    ],
    "brief": "核心要点：InternLM-Math 将代码解释器、形式化证明（Lean 4）和自然语言推理统一在单一模型中。分析其 Chain-of-Thought 与 Program-of-Thought 的动态切换机制、Lean 4 形式化验证作为奖励信号的 RLHF 训练，以及在 MiniF2F 形式化数学基准上的表现",
    "depth_hint": "需包含三种推理模式的路由判断逻辑、Lean 4 验证正确率与推理准确率的关联分析",
    "status": "pending"
  },
  {
    "id": 326,
    "title": "NuminaMath 的竞赛数学数据构建",
    "slug": "numinamath-competition-data",
    "tags": [
      "模型解析",
      "数学推理",
      "数据集"
    ],
    "brief": "核心要点：NuminaMath 在 AIMO 竞赛中获胜，核心是高质量竞赛数学数据集构建。包含来自 AMC/AIME/IMO 等竞赛的 860K 题目，每题配有 CoT 和 Tool-Integrated Reasoning 两种解法。分析竞赛数据的难度分级策略和课程学习训练方案",
    "depth_hint": "需包含数据来源与难度分布统计表、课程学习的阶段划分与性能增长曲线",
    "status": "pending"
  },
  {
    "id": 327,
    "title": "数学推理中的自一致性解码",
    "slug": "math-self-consistency-decoding",
    "tags": [
      "模型解析",
      "数学推理",
      "解码策略"
    ],
    "brief": "核心要点：Self-Consistency 在数学推理中通过多次采样（温度 0.7）取多数投票，GSM8K 准确率从 CoT 的 58% 提升至 74%。分析投票数量 vs 准确率的对数关系、加权投票（基于序列概率）vs 等权投票的差异，以及与 beam search 的本质区别",
    "depth_hint": "需包含 self-consistency 的概率论推导、采样数-准确率曲线的数学拟合",
    "status": "pending"
  },
  {
    "id": 328,
    "title": "代码模型的指令微调对齐方法对比",
    "slug": "code-model-instruction-alignment",
    "tags": [
      "模型解析",
      "代码模型",
      "对齐"
    ],
    "brief": "核心要点：对比代码模型的三种对齐路径：纯 SFT（CodeLlama-Instruct）、Evol-Instruct（WizardCoder）、RLHF/DPO（DeepSeek-Coder-Instruct）。分析各路径在代码正确性、指令遵循、安全性三个维度的 trade-off，以及 DPO 在代码领域相对 RLHF 的数据效率优势",
    "depth_hint": "需包含三种方法的训练流程对比图、多维度评估雷达图数据",
    "status": "pending"
  },
  {
    "id": 329,
    "title": "数学推理的形式化验证：Lean 4 集成",
    "slug": "math-formal-verification-lean4",
    "tags": [
      "模型解析",
      "数学推理",
      "形式化验证"
    ],
    "brief": "核心要点：将大模型的数学推理与 Lean 4 形式化证明系统结合，实现可验证的数学推理。分析 LeanDojo 的 tactic 预测方法、autoformalization（自然语言→形式化语言）的难点，以及 MiniF2F 基准上当前最优的 41.2% 证明成功率的瓶颈分析",
    "depth_hint": "需包含 Lean 4 tactic 生成的 beam search 策略、autoformalization 的错误类型分类与占比",
    "status": "pending"
  },
  {
    "id": 330,
    "title": "ReAct 框架的推理-行动交替机制",
    "slug": "react-reasoning-acting-mechanism",
    "tags": [
      "智能体",
      "ReAct",
      "推理框架"
    ],
    "brief": "核心要点：解析 ReAct 框架中 Thought-Action-Observation 三元循环的设计原理，对比纯推理（CoT）和纯行动（Act-only）基线，ReAct 在 HotpotQA 上将幻觉率降低约 20%，在 ALFWorld 任务成功率提升 34%",
    "depth_hint": "需包含 ReAct prompt 模板结构、三种模式的消融实验数据、失败案例分析",
    "status": "pending"
  },
  {
    "id": 331,
    "title": "思维链提示的涌现与规模阈值",
    "slug": "cot-emergence-scale-threshold",
    "tags": [
      "智能体",
      "思维链",
      "涌现能力"
    ],
    "brief": "核心要点：分析 Chain-of-Thought 在不同模型规模下的涌现行为，Wei et al. 实验表明 CoT 在约 100B 参数以上模型才显著生效，小模型使用 CoT 反而导致性能下降，探讨涌现的可能机制与 U 形曲线现象",
    "depth_hint": "需包含 PaLM 不同规模的 GSM8K 准确率对比、涌现阈值的定量分析",
    "status": "pending"
  },
  {
    "id": 332,
    "title": "自洽性解码的多路径投票机制",
    "slug": "self-consistency-majority-voting",
    "tags": [
      "智能体",
      "自洽性",
      "解码策略"
    ],
    "brief": "核心要点：Self-Consistency 通过采样多条推理路径并对最终答案进行边际化投票，替代贪心解码。在 GSM8K 上将 CoT 准确率从 56.5% 提升至 74.4%（PaLM 540B），分析采样数量与准确率的对数线性关系",
    "depth_hint": "需包含边际化公式 P(a)=ΣP(a,r_i)、采样数-准确率曲线、温度参数影响",
    "status": "pending"
  },
  {
    "id": 333,
    "title": "Tree of Thoughts 的搜索策略设计",
    "slug": "tot-search-strategy-bfs-dfs",
    "tags": [
      "智能体",
      "ToT",
      "搜索算法"
    ],
    "brief": "核心要点：ToT 将推理建模为树搜索问题，支持 BFS 和 DFS 两种搜索策略。在 Game of 24 任务中成功率从 CoT 的 4% 提升至 74%，分析不同任务特征下 BFS vs DFS 的选择依据与剪枝条件设计",
    "depth_hint": "需包含 ToT 搜索伪代码、Game of 24 和创意写作任务的实验对比、评估函数设计",
    "status": "pending"
  },
  {
    "id": 334,
    "title": "CoT 中的忠实推理与不忠实推理",
    "slug": "cot-faithful-vs-unfaithful-reasoning",
    "tags": [
      "智能体",
      "思维链",
      "可解释性"
    ],
    "brief": "核心要点：研究表明 LLM 生成的思维链不一定反映其真实推理过程。Turpin et al. 发现模型在 CoT 中会编造合理化解释而非暴露真实偏见，Lanham et al. 通过 early answering 实验量化了推理忠实度约 50-80%",
    "depth_hint": "需包含忠实性测试方法（截断实验、扰动实验）、偏见注入实验数据",
    "status": "pending"
  },
  {
    "id": 335,
    "title": "ReAct 与工具调用的统一接口设计",
    "slug": "react-tool-calling-interface",
    "tags": [
      "智能体",
      "ReAct",
      "工具调用"
    ],
    "brief": "核心要点：将 ReAct 的 Action 步骤与外部工具 API 对接，实现搜索引擎、计算器、代码执行器等工具的统一调用。对比 Toolformer、Gorilla 等工具学习方法，分析 ReAct 在多工具协调场景下的优势与路由决策机制",
    "depth_hint": "需包含工具调用 prompt 格式、多工具路由逻辑、错误恢复策略",
    "status": "pending"
  },
  {
    "id": 336,
    "title": "Zero-shot CoT 的触发词工程",
    "slug": "zero-shot-cot-trigger-engineering",
    "tags": [
      "智能体",
      "思维链",
      "提示工程"
    ],
    "brief": "核心要点：Kojima et al. 发现仅添加'Let's think step by step'即可在零样本下激活推理能力。系统对比不同触发词（step by step、think carefully、break down）在 MultiArith、GSM8K 等基准上的效果差异，最优触发词提升可达 20%+",
    "depth_hint": "需包含不同触发词的准确率对比表、两阶段提示流程、失败模式分析",
    "status": "pending"
  },
  {
    "id": 337,
    "title": "自洽性在代码生成中的应用",
    "slug": "self-consistency-code-generation",
    "tags": [
      "智能体",
      "自洽性",
      "代码生成"
    ],
    "brief": "核心要点：将 Self-Consistency 的多路径投票思想应用于代码生成，通过执行测试用例替代答案比对实现功能等价性判断。CodeT 等方法结合双重执行验证，在 HumanEval 上 pass@1 从 47% 提升至 65%",
    "depth_hint": "需包含代码自洽性的等价判定方法、采样-执行-投票流水线、与 pass@k 的关系",
    "status": "pending"
  },
  {
    "id": 338,
    "title": "ToT 的启发式评估函数设计",
    "slug": "tot-heuristic-evaluation-function",
    "tags": [
      "智能体",
      "ToT",
      "启发式搜索"
    ],
    "brief": "核心要点：ToT 中节点评估函数决定搜索质量，可用 LLM 自身作为评估器（value prompt）或外部验证器。分析投票式评估、打分式评估、分类式评估三种范式的准确率-成本权衡，及评估函数校准问题",
    "depth_hint": "需包含三种评估 prompt 模板、评估准确率实验、与 MCTS 中 value network 的类比",
    "status": "pending"
  },
  {
    "id": 339,
    "title": "CoT 的自动生成与 Auto-CoT",
    "slug": "auto-cot-automatic-chain-generation",
    "tags": [
      "智能体",
      "思维链",
      "自动化"
    ],
    "brief": "核心要点：Auto-CoT 通过聚类问题集并让 LLM 为每个簇自动生成示范推理链，消除人工编写 few-shot 示例的成本。采用多样性采样 + Zero-shot CoT 构建示范集，在 10 个推理基准上平均仅落后人工 CoT 1-2%",
    "depth_hint": "需包含聚类算法选择、示范选择策略、与 Manual-CoT 的逐任务对比数据",
    "status": "pending"
  },
  {
    "id": 340,
    "title": "ReAct 中的观察信息压缩策略",
    "slug": "react-observation-compression",
    "tags": [
      "智能体",
      "ReAct",
      "上下文管理"
    ],
    "brief": "核心要点：ReAct 循环中 Observation 累积会快速耗尽上下文窗口。分析摘要压缩、选择性保留、滑动窗口三种压缩策略对长链任务的影响，实验表明智能压缩可将最大可执行步数从 5-6 步扩展至 15+ 步",
    "depth_hint": "需包含不同压缩策略的信息保留率、长链任务成功率对比、token 消耗分析",
    "status": "pending"
  },
  {
    "id": 341,
    "title": "自洽性的采样效率优化",
    "slug": "self-consistency-sampling-efficiency",
    "tags": [
      "智能体",
      "自洽性",
      "推理优化"
    ],
    "brief": "核心要点：标准 Self-Consistency 需要 40+ 次采样才趋于稳定，计算开销大。分析自适应采样（early stopping）、分层采样、重要性加权等优化方法，可在保持 95% 准确率的前提下减少 60-70% 的采样次数",
    "depth_hint": "需包含采样数-准确率收敛曲线、自适应停止的置信度阈值设计、成本对比",
    "status": "pending"
  },
  {
    "id": 342,
    "title": "ToT 与蒙特卡洛树搜索的结合",
    "slug": "tot-mcts-integration",
    "tags": [
      "智能体",
      "ToT",
      "MCTS"
    ],
    "brief": "核心要点：将 MCTS 的 UCB 选择策略引入 ToT，实现探索-利用平衡。RAP（Reasoning via Planning）框架用 LLM 同时作为世界模型和推理引擎，在 Blocksworld 规划任务上超越 CoT 约 30%，分析 UCB 常数 c 的调参影响",
    "depth_hint": "需包含 UCB1 公式、MCTS 四阶段流程、RAP 框架架构图、rollout 策略",
    "status": "pending"
  },
  {
    "id": 343,
    "title": "思维链的长度与推理深度关系",
    "slug": "cot-length-reasoning-depth",
    "tags": [
      "智能体",
      "思维链",
      "推理能力"
    ],
    "brief": "核心要点：更长的 CoT 不一定带来更好的推理结果。实验表明存在最优链长度，过短缺乏推理步骤，过长引入错误累积。分析 token 级别的错误传播概率模型，单步错误率 p 下 n 步链的整体准确率约为 (1-p)^n",
    "depth_hint": "需包含链长-准确率曲线、错误累积概率模型、不同难度任务的最优链长",
    "status": "pending"
  },
  {
    "id": 344,
    "title": "ReAct Agent 的错误恢复与重试机制",
    "slug": "react-error-recovery-retry",
    "tags": [
      "智能体",
      "ReAct",
      "容错设计"
    ],
    "brief": "核心要点：ReAct Agent 在工具调用失败或返回无效结果时的恢复策略。分析反思式重试（Reflexion）、回溯重规划、降级策略三种机制，Reflexion 通过语言反馈累积经验，在 AlfWorld 上成功率从 75% 提升至 97%",
    "depth_hint": "需包含 Reflexion 的记忆更新机制、重试次数上限策略、失败分类与对应恢复方案",
    "status": "pending"
  },
  {
    "id": 345,
    "title": "多步推理中的 CoT 分解策略",
    "slug": "cot-decomposition-strategies",
    "tags": [
      "智能体",
      "思维链",
      "问题分解"
    ],
    "brief": "核心要点：对比 Least-to-Most Prompting、Decomposed Prompting、Successive Prompting 三种将复杂问题分解为子问题的 CoT 变体。Least-to-Most 在 SCAN 组合泛化任务上将准确率从 16% 提升至 99.7%",
    "depth_hint": "需包含三种分解方法的 prompt 结构、子问题依赖图构建、组合泛化实验数据",
    "status": "pending"
  },
  {
    "id": 346,
    "title": "自洽性与验证器的对比与融合",
    "slug": "self-consistency-vs-verifier",
    "tags": [
      "智能体",
      "自洽性",
      "验证器"
    ],
    "brief": "核心要点：Self-Consistency 的多数投票与训练专用验证器（ORM/PRM）是两种互补的答案选择策略。分析 SC 的无需训练优势与验证器的细粒度判断能力，融合方案（SC + PRM 重排序）在 GSM8K 上可再提升 3-5%",
    "depth_hint": "需包含 ORM vs PRM 对比、SC+验证器融合流程、不同采样数下的增益曲线",
    "status": "pending"
  },
  {
    "id": 347,
    "title": "GoT 思维图：超越树结构的推理拓扑",
    "slug": "graph-of-thoughts-reasoning-topology",
    "tags": [
      "智能体",
      "ToT",
      "思维图"
    ],
    "brief": "核心要点：Graph of Thoughts 将 ToT 的树结构扩展为有向无环图，允许思维节点的合并与聚合操作。在排序任务中，GoT 比 ToT 减少约 31% 的 LLM 调用次数，同时支持并行分支的信息融合",
    "depth_hint": "需包含 GoT 的图操作定义（生成/聚合/精炼）、与 CoT/ToT 的拓扑对比、排序实验数据",
    "status": "pending"
  },
  {
    "id": 348,
    "title": "CoT 提示中的示例选择策略",
    "slug": "cot-example-selection-strategy",
    "tags": [
      "智能体",
      "思维链",
      "示例选择"
    ],
    "brief": "核心要点：Few-shot CoT 的性能高度依赖示例选择。对比随机选择、相似度检索（kNN）、多样性采样、难度递增四种策略，复杂度自适应选择（Complex CoT）在 GSM8K 上比随机选择提升 5-8%",
    "depth_hint": "需包含各策略的选择算法、embedding 相似度计算、复杂度评估指标",
    "status": "pending"
  },
  {
    "id": 349,
    "title": "ReAct 在多模态场景中的扩展",
    "slug": "react-multimodal-extension",
    "tags": [
      "智能体",
      "ReAct",
      "多模态"
    ],
    "brief": "核心要点：将 ReAct 的推理-行动循环扩展到视觉-语言任务，Action 包含图像理解、UI 操作、视觉搜索等。分析 VisualWebArena、SeeAct 等框架中视觉 Observation 的处理方式，以及视觉 grounding 对决策准确率的影响",
    "depth_hint": "需包含多模态 ReAct 的 prompt 结构、视觉 observation 编码方式、Web 导航任务数据",
    "status": "pending"
  },
  {
    "id": 350,
    "title": "思维链蒸馏：小模型的推理能力迁移",
    "slug": "cot-distillation-small-models",
    "tags": [
      "智能体",
      "思维链",
      "知识蒸馏"
    ],
    "brief": "核心要点：通过大模型生成 CoT 标注数据，微调小模型获得推理能力。Magister et al. 证明 CoT 蒸馏可让 0.3B 模型在算术任务上接近 540B 模型的 CoT 性能。分析蒸馏数据质量、规模与目标模型大小的交互关系",
    "depth_hint": "需包含蒸馏数据构建流程、不同规模学生模型的性能曲线、与直接微调的对比",
    "status": "pending"
  },
  {
    "id": 351,
    "title": "自洽性在多语言推理中的表现差异",
    "slug": "self-consistency-multilingual-reasoning",
    "tags": [
      "智能体",
      "自洽性",
      "多语言"
    ],
    "brief": "核心要点：Self-Consistency 在不同语言上的增益不均匀，英语推理路径一致性高于低资源语言。分析 MGSM 基准上 SC 在 10 种语言的增益差异（英语 +15% vs 泰语 +8%），及跨语言混合采样对一致性的提升效果",
    "depth_hint": "需包含 MGSM 多语言 SC 增益对比表、语言特征与一致性的相关性分析",
    "status": "pending"
  },
  {
    "id": 352,
    "title": "ToT 的并行化与延迟优化",
    "slug": "tot-parallelization-latency",
    "tags": [
      "智能体",
      "ToT",
      "推理加速"
    ],
    "brief": "核心要点：ToT 的串行树搜索导致推理延迟远高于 CoT。分析批量节点扩展、异步评估、投机式搜索等并行化策略，实验表明 BFS 宽度为 5 时可通过并行评估将延迟降低 3-4 倍，同时保持搜索质量",
    "depth_hint": "需包含串行 vs 并行 ToT 的延迟对比、GPU 利用率分析、批处理大小的权衡",
    "status": "pending"
  },
  {
    "id": 353,
    "title": "CoT 在逻辑推理中的系统性失败",
    "slug": "cot-logical-reasoning-failures",
    "tags": [
      "智能体",
      "思维链",
      "推理局限"
    ],
    "brief": "核心要点：CoT 在形式逻辑、反事实推理、否定推理等场景存在系统性失败模式。分析 LLM 在三段论推理中的信念偏差（准确率仅 60-70%）、否定词处理错误、以及 CoT 如何放大而非纠正这些偏差",
    "depth_hint": "需包含逻辑推理错误分类、具体失败案例、与人类认知偏差的类比分析",
    "status": "pending"
  },
  {
    "id": 354,
    "title": "ReAct 与 Plan-and-Execute 的架构对比",
    "slug": "react-vs-plan-and-execute",
    "tags": [
      "智能体",
      "ReAct",
      "规划架构"
    ],
    "brief": "核心要点：ReAct 的逐步推理-执行与 Plan-and-Execute 的先规划后执行是两种主流 Agent 架构。对比两者在任务复杂度、错误恢复、token 效率上的权衡，混合架构（ADaPT）在长链任务上比纯 ReAct 成功率提升 20%",
    "depth_hint": "需包含两种架构的流程图、不同任务复杂度下的成功率对比、ADaPT 的递归分解机制",
    "status": "pending"
  },
  {
    "id": 355,
    "title": "Universal Self-Consistency 的无标签校验",
    "slug": "universal-self-consistency",
    "tags": [
      "智能体",
      "自洽性",
      "开放式推理"
    ],
    "brief": "核心要点：标准 SC 仅适用于有固定答案的任务，Universal SC 通过 LLM 选择而非精确匹配实现开放式任务的一致性校验。在摘要、翻译等生成任务上，USC 比 greedy decoding 的人类评分提升 10-15%",
    "depth_hint": "需包含 USC 的选择 prompt 设计、开放式任务的一致性度量方法、与 MBR 解码的对比",
    "status": "pending"
  },
  {
    "id": 356,
    "title": "ToT 在数学证明搜索中的应用",
    "slug": "tot-mathematical-proof-search",
    "tags": [
      "智能体",
      "ToT",
      "数学证明"
    ],
    "brief": "核心要点：将 ToT 框架应用于数学定理证明的策略搜索，每个分支代表一种证明策略（直接证明/反证法/归纳法）。分析在 miniF2F 基准上 ToT 比线性 CoT 提升约 8-12%，以及与 Lean 4 验证器的反馈循环",
    "depth_hint": "需包含证明搜索树结构示例、策略选择的评估函数、形式化验证反馈机制",
    "status": "pending"
  },
  {
    "id": 357,
    "title": "CoT 的对抗鲁棒性分析",
    "slug": "cot-adversarial-robustness",
    "tags": [
      "智能体",
      "思维链",
      "对抗攻击"
    ],
    "brief": "核心要点：研究对抗性 prompt 注入对 CoT 推理的干扰效果。添加无关信息可使 CoT 准确率下降 30%+，而误导性中间步骤的注入更为致命。分析防御策略：推理步骤验证、自对弈过滤、对抗训练",
    "depth_hint": "需包含对抗扰动类型分类、不同攻击下的准确率下降数据、防御方法有效性对比",
    "status": "pending"
  },
  {
    "id": 358,
    "title": "Reflexion：语言反馈驱动的 Agent 学习",
    "slug": "reflexion-language-feedback-learning",
    "tags": [
      "智能体",
      "ReAct",
      "自我反思"
    ],
    "brief": "核心要点：Reflexion 在 ReAct 基础上增加语言化反思记忆，Agent 将失败经验转化为自然语言经验存入长期记忆。在 HumanEval 代码生成上从 67% 提升至 91%（pass@1），分析反思质量与记忆管理的关键设计",
    "depth_hint": "需包含 Reflexion 的三组件架构（Actor/Evaluator/Self-Reflection）、记忆格式、多任务消融实验",
    "status": "pending"
  },
  {
    "id": 359,
    "title": "思维链与程序化推理的互补机制",
    "slug": "cot-program-aided-reasoning",
    "tags": [
      "智能体",
      "思维链",
      "程序推理"
    ],
    "brief": "核心要点：PAL（Program-Aided Language models）将 CoT 的自然语言推理与程序执行结合，LLM 生成推理代码而非直接计算。在 GSM8K 上 PAL 比纯 CoT 提升约 10%，分析自然语言推理与形式化执行各自的适用边界",
    "depth_hint": "需包含 PAL 的 prompt 格式、代码生成推理示例、与 PoT 的方法对比",
    "status": "pending"
  },
  {
    "id": 360,
    "title": "Function Calling 的协议设计与类型约束",
    "slug": "function-calling-protocol-design",
    "tags": [
      "智能体",
      "工具调用",
      "Function Calling"
    ],
    "brief": "核心要点：拆解 OpenAI、Anthropic、Gemini 三家 Function Calling 协议的 JSON Schema 定义差异，分析参数类型约束（enum、oneOf、$ref）对调用准确率的影响，GPT-4 在嵌套对象参数上的错误率约 12-18%，扁平化参数设计可降至 3-5%",
    "depth_hint": "需要包含三家 API 的 schema 对比表、嵌套 vs 扁平参数的准确率实验数据",
    "status": "pending"
  },
  {
    "id": 361,
    "title": "并行函数调用的调度与依赖分析",
    "slug": "parallel-function-calling-scheduling",
    "tags": [
      "智能体",
      "工具调用",
      "并行执行"
    ],
    "brief": "核心要点：分析 GPT-4-turbo 的 parallel_tool_calls 机制，模型如何识别独立调用并批量发出。构建调用依赖图的拓扑排序算法，实测并行调用相比串行可减少 40-60% 的端到端延迟，但存在状态竞争问题需要幂等性保证",
    "depth_hint": "需要包含依赖图构建代码、并行 vs 串行延迟对比数据、幂等性设计模式",
    "status": "pending"
  },
  {
    "id": 362,
    "title": "工具调用的强制模式与自由选择对比",
    "slug": "tool-choice-forced-vs-auto",
    "tags": [
      "智能体",
      "工具调用",
      "提示工程"
    ],
    "brief": "核心要点：对比 tool_choice 的 auto/required/none/指定函数四种模式对调用行为的影响。auto 模式下模型可能跳过必要调用，required 模式下可能产生幻觉调用。实测在多工具场景（>10 tools）中，精确指定可将准确率从 72% 提升至 94%",
    "depth_hint": "需要包含四种模式的行为对比表、不同工具数量下的准确率曲线",
    "status": "pending"
  },
  {
    "id": 363,
    "title": "工具描述的 Prompt 工程优化",
    "slug": "tool-description-prompt-engineering",
    "tags": [
      "智能体",
      "工具调用",
      "提示工程"
    ],
    "brief": "核心要点：工具的 name、description、parameter description 三层文本如何影响模型选择和参数填充。实验表明描述中加入使用条件（when to use）可减少 30% 的误调用，加入参数示例可减少 25% 的参数格式错误，过长描述（>500 token）反而降低性能",
    "depth_hint": "需要包含 A/B 实验设计、不同描述策略的调用准确率数据",
    "status": "pending"
  },
  {
    "id": 364,
    "title": "结构化输出与 Function Calling 的统一",
    "slug": "structured-output-function-calling-unification",
    "tags": [
      "智能体",
      "工具调用",
      "结构化输出"
    ],
    "brief": "核心要点：OpenAI Structured Outputs 通过受限解码（Constrained Decoding）保证 JSON Schema 100% 符合率，其底层与 Function Calling 共享同一套 CFG 引导生成机制。对比 strict: true 模式与普通模式在 schema 复杂度、首 token 延迟（+0.5-1s）上的差异",
    "depth_hint": "需要包含 CFG 受限解码原理图、strict 模式的延迟开销测量",
    "status": "pending"
  },
  {
    "id": 365,
    "title": "多轮工具调用的上下文管理",
    "slug": "multi-turn-tool-calling-context",
    "tags": [
      "智能体",
      "工具调用",
      "上下文管理"
    ],
    "brief": "核心要点：多轮对话中工具调用结果如何回传（tool role message），上下文窗口中工具结果的 token 占比分析。当工具返回超过 4K token 时，后续调用准确率下降约 20%。摘要压缩、选择性保留、滑动窗口三种上下文管理策略的效果对比",
    "depth_hint": "需要包含消息序列结构图、工具结果长度与后续准确率的关系曲线",
    "status": "pending"
  },
  {
    "id": 366,
    "title": "Agent 代码沙箱的隔离与安全机制",
    "slug": "agent-code-sandbox-isolation",
    "tags": [
      "智能体",
      "代码执行",
      "安全"
    ],
    "brief": "核心要点：分析 OpenAI Code Interpreter、Claude Analysis、E2B 三种代码沙箱的隔离架构。gVisor/microVM/容器三层沙箱的安全边界，文件系统隔离、网络隔离、系统调用白名单的具体实现。代码执行的超时机制（通常 30-120s）和资源限制（内存 512MB-2GB）",
    "depth_hint": "需要包含三种沙箱的架构对比图、安全边界层次表、资源限制参数",
    "status": "pending"
  },
  {
    "id": 367,
    "title": "代码执行反馈的自动修复循环",
    "slug": "code-execution-auto-fix-loop",
    "tags": [
      "智能体",
      "代码执行",
      "自动修复"
    ],
    "brief": "核心要点：代码执行 Agent 在获取错误输出后的自动修复机制设计。错误分类（语法/运行时/逻辑）对修复策略的影响，实测 GPT-4 在 3 次重试内的修复成功率：语法错误 95%、运行时错误 78%、逻辑错误 42%。最大重试次数与成本的权衡",
    "depth_hint": "需要包含修复循环的状态机设计、三类错误的修复成功率数据",
    "status": "pending"
  },
  {
    "id": 368,
    "title": "代码生成的多语言运行时适配",
    "slug": "code-generation-multi-runtime",
    "tags": [
      "智能体",
      "代码执行",
      "运行时"
    ],
    "brief": "核心要点：Agent 在 Python/JavaScript/Bash 多运行时间切换的决策逻辑。Python 适合数据处理和科学计算，JS 适合 DOM 操作和 API 调用，Bash 适合文件操作和系统管理。不同运行时的启动延迟（Python ~200ms、Node ~100ms）和包管理策略差异",
    "depth_hint": "需要包含运行时选择决策树、各语言的启动延迟和适用场景对比表",
    "status": "pending"
  },
  {
    "id": 369,
    "title": "代码执行中的状态持久化设计",
    "slug": "code-execution-state-persistence",
    "tags": [
      "智能体",
      "代码执行",
      "状态管理"
    ],
    "brief": "核心要点：Jupyter 内核式的有状态执行 vs 每次重启的无状态执行对比。有状态模式支持变量跨 cell 复用，但存在隐式依赖和内存泄漏风险。OpenAI Code Interpreter 的会话保持机制分析，文件系统作为持久化层的 /mnt/data 设计",
    "depth_hint": "需要包含有状态 vs 无状态的优缺点对比表、会话生命周期管理代码",
    "status": "pending"
  },
  {
    "id": 370,
    "title": "Playwright 驱动的浏览器 Agent 架构",
    "slug": "playwright-browser-agent-architecture",
    "tags": [
      "智能体",
      "浏览器控制",
      "Playwright"
    ],
    "brief": "核心要点：基于 Playwright 的浏览器 Agent 架构设计，包括页面状态提取（DOM 快照 vs 截图 vs Accessibility Tree）、动作空间定义（click/type/scroll/navigate）、动作执行与验证。Accessibility Tree 相比原始 HTML 可减少 90% 的 token 消耗",
    "depth_hint": "需要包含架构图、三种状态提取方式的 token 消耗对比、动作空间定义代码",
    "status": "pending"
  },
  {
    "id": 371,
    "title": "网页元素定位的语义标注方法",
    "slug": "web-element-semantic-annotation",
    "tags": [
      "智能体",
      "浏览器控制",
      "元素定位"
    ],
    "brief": "核心要点：浏览器 Agent 如何将 DOM 元素映射为可操作目标。Set-of-Mark（SoM）标注法在页面截图上叠加数字标签，使多模态模型可直接引用元素。对比 CSS 选择器、XPath、坐标点击、SoM 标注四种定位方式的鲁棒性和准确率差异",
    "depth_hint": "需要包含 SoM 标注的实现代码、四种定位方式的准确率对比实验",
    "status": "pending"
  },
  {
    "id": 372,
    "title": "浏览器 Agent 的视觉感知与 DOM 感知对比",
    "slug": "browser-agent-vision-vs-dom",
    "tags": [
      "智能体",
      "浏览器控制",
      "多模态"
    ],
    "brief": "核心要点：纯视觉（截图+多模态模型）vs 纯 DOM（HTML/Accessibility Tree+文本模型）vs 混合模式三种浏览器感知方案。WebArena 基准上纯视觉准确率约 14%，纯 DOM 约 19%，混合模式可达 25%。视觉模式在动态渲染页面上优势明显，DOM 模式在表格数据提取上更可靠",
    "depth_hint": "需要包含 WebArena 各方案的详细评分、典型失败 case 分析",
    "status": "pending"
  },
  {
    "id": 373,
    "title": "WebArena 基准的任务设计与评估方法",
    "slug": "webarena-benchmark-design",
    "tags": [
      "智能体",
      "浏览器控制",
      "评测"
    ],
    "brief": "核心要点：WebArena 包含 812 个跨 5 个真实网站（Reddit/GitLab/购物/CMS/地图）的长程浏览器任务。评估采用基于 URL/页面内容/程序化验证的多维度判定。GPT-4V 端到端成功率约 14.4%，人类约 78.2%，差距主要来自长程规划和错误恢复能力不足",
    "depth_hint": "需要包含 WebArena 的任务类别分布、各模型的成功率对比表",
    "status": "pending"
  },
  {
    "id": 374,
    "title": "浏览器 Agent 的动作空间设计与约束",
    "slug": "browser-agent-action-space-design",
    "tags": [
      "智能体",
      "浏览器控制",
      "动作空间"
    ],
    "brief": "核心要点：浏览器 Agent 的动作原语设计：低级（click坐标/type文本/press按键/scroll方向）vs 高级（fill表单/select下拉/upload文件/drag拖拽）。动作空间过大（>50种）导致模型选择困难，实测精简至 15-20 种核心动作可提升任务成功率约 8%",
    "depth_hint": "需要包含动作空间层次结构表、精简前后的成功率对比",
    "status": "pending"
  },
  {
    "id": 375,
    "title": "Claude Computer Use 的屏幕交互协议",
    "slug": "claude-computer-use-protocol",
    "tags": [
      "智能体",
      "计算机使用",
      "Computer Use"
    ],
    "brief": "核心要点：Anthropic Computer Use API 的三个工具（computer、text_editor、bash）协议设计。computer 工具通过坐标系（screenshot → coordinate → action）实现屏幕交互，分辨率建议 1280x800，缩放因子影响点击精度。连续截图的 token 成本约 1500 token/张",
    "depth_hint": "需要包含三个工具的 API schema、分辨率与精度的关系、token 成本估算",
    "status": "pending"
  },
  {
    "id": 376,
    "title": "计算机使用中的坐标预测精度分析",
    "slug": "computer-use-coordinate-prediction",
    "tags": [
      "智能体",
      "计算机使用",
      "坐标预测"
    ],
    "brief": "核心要点：多模态模型从截图预测点击坐标的精度分析。Claude 3.5 Sonnet 在 OSWorld 基准上的坐标偏差中位数约 15-30px，对小按钮（<20px）的点击失败率约 35%。分辨率降低、UI 元素密集、深色主题是三大精度下降因素",
    "depth_hint": "需要包含坐标偏差分布图、不同 UI 元素尺寸的点击成功率数据",
    "status": "pending"
  },
  {
    "id": 377,
    "title": "OSWorld 基准的跨平台桌面 Agent 评测",
    "slug": "osworld-benchmark-desktop-agent",
    "tags": [
      "智能体",
      "计算机使用",
      "评测"
    ],
    "brief": "核心要点：OSWorld 提供 369 个跨 Ubuntu/Windows/macOS 的真实桌面任务评测。任务涵盖办公软件、文件管理、系统设置、多应用协作四大类。GPT-4V 成功率约 12.2%，Claude 3.5 Sonnet 约 22.0%，人类约 72.4%。主要瓶颈在长程任务的规划退化",
    "depth_hint": "需要包含 OSWorld 任务类别分布、各模型各类别的成功率热力图",
    "status": "pending"
  },
  {
    "id": 378,
    "title": "Computer Use 的安全边界与权限控制",
    "slug": "computer-use-security-boundaries",
    "tags": [
      "智能体",
      "计算机使用",
      "安全"
    ],
    "brief": "核心要点：计算机使用场景的安全风险分类：Prompt Injection 通过屏幕内容注入指令、权限越界（sudo/管理员操作）、数据泄露（截图含敏感信息）。Anthropic 的安全建议：专用虚拟机、最小权限用户、网络隔离、Human-in-the-loop 审批高危操作",
    "depth_hint": "需要包含安全威胁分类表、防护措施的实施架构图",
    "status": "pending"
  },
  {
    "id": 379,
    "title": "屏幕截图的 Token 优化与压缩策略",
    "slug": "screenshot-token-optimization",
    "tags": [
      "智能体",
      "计算机使用",
      "效率优化"
    ],
    "brief": "核心要点：连续屏幕截图的 token 消耗是 Computer Use 的主要成本瓶颈。单张 1280x800 截图约 1500 token，10 步交互即消耗 15K token。裁剪关注区域（减少 60% token）、降低分辨率（减少 40%）、差分截图（仅传变化区域）三种优化策略的效果对比",
    "depth_hint": "需要包含三种优化策略的 token 节省量和精度损失数据",
    "status": "pending"
  },
  {
    "id": 380,
    "title": "MCP 协议的工具发现与注册机制",
    "slug": "mcp-tool-discovery-registration",
    "tags": [
      "智能体",
      "工具调用",
      "MCP"
    ],
    "brief": "核心要点：Model Context Protocol 的 tools/list 和 tools/call 两阶段交互设计。Server 通过 JSON-RPC 2.0 注册工具的 name/description/inputSchema，Client 动态发现并调用。对比静态工具定义 vs MCP 动态发现在扩展性和延迟上的差异",
    "depth_hint": "需要包含 MCP 消息交互序列图、工具注册的 JSON-RPC 示例",
    "status": "pending"
  },
  {
    "id": 381,
    "title": "工具调用的错误处理与回退策略",
    "slug": "tool-calling-error-handling-fallback",
    "tags": [
      "智能体",
      "工具调用",
      "容错"
    ],
    "brief": "核心要点：工具调用失败的五种类型：参数校验失败、API 超时、权限不足、资源不存在、服务端内部错误。每种错误的最佳回退策略不同：重试（超时）、参数修正（校验失败）、替代工具（权限不足）、用户确认（资源不存在）。错误信息的结构化回传格式设计",
    "depth_hint": "需要包含错误分类决策树、回退策略代码实现、错误消息格式规范",
    "status": "pending"
  },
  {
    "id": 382,
    "title": "工具选择的意图识别与消歧",
    "slug": "tool-selection-intent-disambiguation",
    "tags": [
      "智能体",
      "工具调用",
      "意图理解"
    ],
    "brief": "核心要点：当工具数量 >20 时，模型的工具选择准确率显著下降。原因是工具功能重叠导致歧义（如 search_web vs browse_url）。通过工具分组（按领域聚类）、两阶段选择（先选类别再选工具）、负样例描述（when NOT to use）三种方法可将 50 工具场景的选择准确率从 68% 提升至 89%",
    "depth_hint": "需要包含工具聚类算法、两阶段选择的 prompt 模板、消歧实验数据",
    "status": "pending"
  },
  {
    "id": 383,
    "title": "代码执行 Agent 的包管理策略",
    "slug": "code-agent-package-management",
    "tags": [
      "智能体",
      "代码执行",
      "包管理"
    ],
    "brief": "核心要点：代码沙箱中的 Python 包安装策略：预装常用包（numpy/pandas/matplotlib 等约 200MB）vs 按需安装（pip install 延迟 2-10s）vs 白名单限制。E2B 的模板系统支持自定义预装环境，Open Interpreter 的 --safe-mode 限制危险包（os/subprocess/socket）",
    "depth_hint": "需要包含主流沙箱的预装包列表对比、按需安装的延迟测量数据",
    "status": "pending"
  },
  {
    "id": 384,
    "title": "代码执行结果的可视化渲染管线",
    "slug": "code-execution-visualization-pipeline",
    "tags": [
      "智能体",
      "代码执行",
      "可视化"
    ],
    "brief": "核心要点：代码执行 Agent 生成可视化的完整管线：matplotlib/plotly 图表生成 → 图像序列化（base64/文件路径）→ 多模态模型解读 → 自然语言描述。实测 GPT-4V 对 matplotlib 图表的数据点读取误差约 5-15%，趋势判断准确率约 85%",
    "depth_hint": "需要包含渲染管线架构图、图表理解准确率的分类评估数据",
    "status": "pending"
  },
  {
    "id": 385,
    "title": "浏览器 Agent 的多标签页协同控制",
    "slug": "browser-agent-multi-tab-coordination",
    "tags": [
      "智能体",
      "浏览器控制",
      "多标签页"
    ],
    "brief": "核心要点：多标签页场景（信息对比、跨站表单填写、多步验证）的 Agent 控制策略。标签页上下文切换的 token 开销（每次切换需重新截图/提取 DOM 约 500-1500 token），页面状态缓存与失效机制，并行加载 vs 串行操作的权衡",
    "depth_hint": "需要包含多标签页状态管理的数据结构、上下文切换的 token 成本分析",
    "status": "pending"
  },
  {
    "id": 386,
    "title": "浏览器 Agent 的表单自动填写机制",
    "slug": "browser-agent-form-filling",
    "tags": [
      "智能体",
      "浏览器控制",
      "表单处理"
    ],
    "brief": "核心要点：复杂表单（多步骤、条件字段、验证码）的自动化填写策略。字段类型识别（text/select/radio/checkbox/file）与对应操作映射，动态表单（字段根据前序输入变化）的处理，实测在 50 个真实注册表单上的完整填写成功率约 62%，失败主要集中在验证码和自定义组件",
    "depth_hint": "需要包含字段类型映射表、表单填写流程的状态机设计",
    "status": "pending"
  },
  {
    "id": 387,
    "title": "GUI Agent 的 Grounding 对齐训练",
    "slug": "gui-agent-grounding-training",
    "tags": [
      "智能体",
      "计算机使用",
      "视觉定位"
    ],
    "brief": "核心要点：GUI Grounding 是将自然语言指令映射到屏幕坐标的能力。CogAgent（18B）通过高分辨率交叉注意力模块实现 1120x1120 输入，在 ScreenSpot 基准上文本定位准确率 76.1%、图标定位 67.0%。对比 SeeClick、UGround 等方法的训练数据构建策略",
    "depth_hint": "需要包含 CogAgent 的架构图、ScreenSpot 各方法准确率对比表",
    "status": "pending"
  },
  {
    "id": 388,
    "title": "计算机使用中的 GUI 导航规划算法",
    "slug": "computer-use-gui-navigation-planning",
    "tags": [
      "智能体",
      "计算机使用",
      "规划"
    ],
    "brief": "核心要点：桌面 Agent 在多步任务中的规划策略：全局规划（预先分解所有步骤）vs 局部规划（每步观察后决策）vs 混合规划（粗粒度全局+细粒度局部）。全局规划在 OSWorld 上成功率约 8%（规划偏差累积），混合规划约 22%。AppAgent 的 UI 文档探索预学习方法",
    "depth_hint": "需要包含三种规划策略的形式化定义、OSWorld 上的消融实验数据",
    "status": "pending"
  },
  {
    "id": 389,
    "title": "工具调用的流式响应与中间状态展示",
    "slug": "tool-calling-streaming-intermediate-state",
    "tags": [
      "智能体",
      "工具调用",
      "用户体验"
    ],
    "brief": "核心要点：长时间工具调用（数据库查询 >5s、API 调用 >10s）的用户体验优化。Streaming 模式下工具调用的三阶段展示：调用发起（参数预览）→ 执行中（进度指示）→ 结果返回（摘要展示）。Anthropic 的 tool_use streaming event 设计与 OpenAI 的 function_call delta 对比",
    "depth_hint": "需要包含两家 streaming 事件格式对比、前端状态机实现代码",
    "status": "pending"
  },
  {
    "id": 390,
    "title": "Agent 短期记忆的滑动窗口与摘要压缩",
    "slug": "agent-short-term-memory-sliding-window",
    "tags": [
      "智能体",
      "记忆系统",
      "上下文管理"
    ],
    "brief": "核心要点：Agent 短期记忆面临上下文窗口限制，主流方案包括滑动窗口截断、递归摘要、Token 级压缩三种策略。递归摘要可将 128K Token 对话压缩至 2K 以内，信息保留率约 85%。分析各方案在多轮对话中的信息损失曲线与延迟开销。",
    "depth_hint": "需包含信息保留率的量化对比实验、摘要压缩的 Prompt 模板设计、Token 消耗公式推导",
    "status": "pending"
  },
  {
    "id": 391,
    "title": "长期记忆的存储架构：KV 缓存与外部持久化",
    "slug": "agent-long-term-memory-storage-architecture",
    "tags": [
      "智能体",
      "记忆系统",
      "存储架构"
    ],
    "brief": "核心要点：Agent 长期记忆需要跨会话持久化，核心挑战是检索效率与存储成本的平衡。对比 KV 缓存复用、向量数据库存储、结构化数据库存储三种方案，分析各自的写入延迟（KV 缓存 <10ms，向量库 50-200ms）、检索精度和扩展性瓶颈。",
    "depth_hint": "需包含三种方案的延迟与吞吐量基准测试数据、存储容量与检索精度的权衡曲线",
    "status": "pending"
  },
  {
    "id": 392,
    "title": "语义记忆的 Embedding 表示与检索策略",
    "slug": "semantic-memory-embedding-retrieval",
    "tags": [
      "智能体",
      "语义记忆",
      "Embedding"
    ],
    "brief": "核心要点：语义记忆将经验抽象为向量表示，关键在于 Embedding 模型选择与检索策略设计。text-embedding-3-large 在语义相似度任务上 NDCG@10 达 0.62，但对时序关系捕获不足。混合检索（向量 + BM25）可提升 Recall@10 约 15%。",
    "depth_hint": "需包含不同 Embedding 模型的维度与性能对比表、混合检索的加权融合公式",
    "status": "pending"
  },
  {
    "id": 393,
    "title": "MemGPT 的分层记忆管理机制",
    "slug": "memgpt-hierarchical-memory-management",
    "tags": [
      "智能体",
      "记忆系统",
      "MemGPT"
    ],
    "brief": "核心要点：MemGPT 借鉴操作系统虚拟内存思想，将 LLM 上下文窗口视为主存，外部存储视为磁盘，通过自主页面换入/换出实现无限上下文。核心机制包括 main context（工作记忆）、recall storage（对话历史）、archival storage（长期知识）三级结构。",
    "depth_hint": "需包含三级存储的数据流转状态机、换页触发条件的判定逻辑、与原生长上下文模型的对比实验",
    "status": "pending"
  },
  {
    "id": 394,
    "title": "对话历史的增量摘要与遗忘曲线建模",
    "slug": "conversation-summary-forgetting-curve",
    "tags": [
      "智能体",
      "记忆系统",
      "信息压缩"
    ],
    "brief": "核心要点：模拟人类遗忘曲线（R = e^{-t/S}）对 Agent 对话历史进行衰减加权，近期信息保留完整细节，远期信息仅保留摘要。实验表明基于遗忘曲线的记忆管理在 20 轮以上对话中，任务完成率比固定窗口高 12%。",
    "depth_hint": "需包含遗忘曲线公式参数调优、增量摘要的链式 Prompt 设计、多轮对话的消融实验",
    "status": "pending"
  },
  {
    "id": 395,
    "title": "向量数据库选型：Pinecone vs Milvus vs Chroma",
    "slug": "vector-database-comparison-pinecone-milvus-chroma",
    "tags": [
      "智能体",
      "向量数据库",
      "技术选型"
    ],
    "brief": "核心要点：三款主流向量数据库在 Agent 场景下的核心差异——Pinecone 托管型延迟 P99 <50ms 但成本高；Milvus 支持十亿级向量但运维复杂；Chroma 轻量嵌入式适合原型但缺乏分布式能力。从 QPS、索引构建时间、Recall@10 三个维度量化对比。",
    "depth_hint": "需包含百万级向量的基准测试数据表、不同索引类型（HNSW/IVF/PQ）的性能对比",
    "status": "pending"
  },
  {
    "id": 396,
    "title": "HNSW 索引在 Agent 记忆检索中的调参实践",
    "slug": "hnsw-index-tuning-agent-memory",
    "tags": [
      "智能体",
      "向量数据库",
      "HNSW"
    ],
    "brief": "核心要点：HNSW 是 Agent 记忆检索最常用的 ANN 索引，核心参数 M（邻居数）和 efConstruction 直接影响检索质量与构建速度。M=16 时 Recall@10 约 0.95，M=64 时达 0.99 但内存增加 4 倍。给出不同数据规模下的最优参数配置。",
    "depth_hint": "需包含 M 和 ef 参数的 Recall-延迟曲线图数据、内存占用公式、百万级数据的实测结果",
    "status": "pending"
  },
  {
    "id": 397,
    "title": "记忆检索的语义路由与多索引分发",
    "slug": "memory-retrieval-semantic-routing",
    "tags": [
      "智能体",
      "记忆系统",
      "检索优化"
    ],
    "brief": "核心要点：Agent 拥有多类记忆（事实、对话、技能）时，需要语义路由层判断查询应分发到哪个索引。基于查询意图分类器（准确率 >92%）将请求路由到对应记忆库，避免全库检索带来的噪声。路由错误率每降 1%，任务完成率提升约 2%。",
    "depth_hint": "需包含路由分类器的训练方案、多索引查询的融合排序算法、路由错误的级联影响分析",
    "status": "pending"
  },
  {
    "id": 398,
    "title": "RAG 管线中的 Chunk 策略对 Agent 表现的影响",
    "slug": "rag-chunking-strategy-agent-performance",
    "tags": [
      "智能体",
      "RAG",
      "知识管理"
    ],
    "brief": "核心要点：文档分块策略直接影响 Agent 的知识检索质量。固定长度分块（512 Token）简单但语义割裂；递归字符分块保留段落结构；语义分块基于 Embedding 相似度断句，Recall@5 比固定分块高 18%，但处理速度降低 3 倍。",
    "depth_hint": "需包含三种分块策略的 Recall/Precision 对比表、不同 chunk_size 的消融实验、语义分块的阈值设定",
    "status": "pending"
  },
  {
    "id": 399,
    "title": "Agent 情景记忆的时序索引与因果检索",
    "slug": "episodic-memory-temporal-causal-retrieval",
    "tags": [
      "智能体",
      "情景记忆",
      "时序推理"
    ],
    "brief": "核心要点：情景记忆记录 Agent 的具体经历序列，需要支持时序查询（'上次失败是什么时候'）和因果查询（'为什么选择了方案 A'）。在纯向量检索基础上叠加时间戳过滤和因果链标注，可将时序相关查询的准确率从 45% 提升至 78%。",
    "depth_hint": "需包含时序索引的数据结构设计、因果链的图存储方案、与纯向量检索的对比实验",
    "status": "pending"
  },
  {
    "id": 400,
    "title": "知识图谱与向量检索的混合记忆架构",
    "slug": "knowledge-graph-vector-hybrid-memory",
    "tags": [
      "智能体",
      "知识图谱",
      "混合检索"
    ],
    "brief": "核心要点：向量检索擅长语义匹配但缺乏结构化推理，知识图谱擅长关系推理但覆盖度有限。混合架构先用向量检索召回候选，再用知识图谱做关系约束过滤，在 HotpotQA 上 F1 比纯向量方案高 8.3%，比纯图谱方案高 12.1%。",
    "depth_hint": "需包含混合检索的流水线架构图、图谱三元组与向量的对齐方法、多跳推理的精度数据",
    "status": "pending"
  },
  {
    "id": 401,
    "title": "Agent 工作记忆的容量约束与优先级淘汰",
    "slug": "working-memory-capacity-eviction-policy",
    "tags": [
      "智能体",
      "记忆系统",
      "资源管理"
    ],
    "brief": "核心要点：工作记忆容量受上下文窗口限制，需要淘汰策略决定保留哪些信息。对比 LRU、LFU、基于重要性评分的淘汰策略，重要性评分方法（结合相关性 + 新近性 + 频率）在复杂任务中比 LRU 的任务完成率高 9%。",
    "depth_hint": "需包含淘汰策略的伪代码实现、重要性评分的加权公式、不同窗口大小下的性能曲线",
    "status": "pending"
  },
  {
    "id": 402,
    "title": "多 Agent 共享记忆的一致性与冲突解决",
    "slug": "multi-agent-shared-memory-consistency",
    "tags": [
      "智能体",
      "多Agent",
      "共享记忆"
    ],
    "brief": "核心要点：多个 Agent 共享记忆池时面临并发写入冲突和信息一致性问题。借鉴分布式系统的 CRDT 和向量时钟机制，实现最终一致性。实验表明无冲突解决机制时，3 Agent 协作的任务成功率下降 23%。",
    "depth_hint": "需包含共享记忆的并发控制协议设计、冲突检测与合并的算法、多 Agent 协作实验数据",
    "status": "pending"
  },
  {
    "id": 403,
    "title": "Embedding 模型微调提升领域记忆检索精度",
    "slug": "embedding-finetuning-domain-memory-retrieval",
    "tags": [
      "智能体",
      "Embedding",
      "微调"
    ],
    "brief": "核心要点：通用 Embedding 模型在垂直领域（医疗、法律、代码）的检索精度不足，通过对比学习微调可将领域内 Recall@10 从 0.68 提升至 0.89。关键在于构造高质量的正负样本对，hard negative mining 策略比随机采样效果提升 15%。",
    "depth_hint": "需包含对比学习的损失函数（InfoNCE）、hard negative 采样策略、微调前后的检索指标对比",
    "status": "pending"
  },
  {
    "id": 404,
    "title": "记忆的元数据标注与多维度过滤检索",
    "slug": "memory-metadata-annotation-filtered-retrieval",
    "tags": [
      "智能体",
      "记忆系统",
      "元数据"
    ],
    "brief": "核心要点：纯向量相似度检索在 Agent 记忆场景中精度不足，叠加元数据过滤（时间范围、来源类型、置信度、主题标签）可将检索精确率提升 25%。Pinecone 和 Milvus 均支持向量搜索 + 标量过滤的混合查询，延迟增加不超过 15%。",
    "depth_hint": "需包含元数据 Schema 设计、过滤条件的组合查询语法示例、有无过滤的精度对比实验",
    "status": "pending"
  },
  {
    "id": 405,
    "title": "Agent 反思记忆的生成与自我修正机制",
    "slug": "agent-reflective-memory-self-correction",
    "tags": [
      "智能体",
      "反思记忆",
      "自我修正"
    ],
    "brief": "核心要点：反思记忆是 Agent 从失败经验中提取的高层规则，如'调用 API 前先验证参数格式'。Generative Agents 论文中反思机制将行为合理性评分从 2.1 提升至 3.8（5 分制）。关键是反思的触发时机、抽象层级和存储粒度。",
    "depth_hint": "需包含反思生成的 Prompt 模板、反思条目的结构化格式、反思频率与任务性能的关系曲线",
    "status": "pending"
  },
  {
    "id": 406,
    "title": "向量数据库的量化压缩与检索精度权衡",
    "slug": "vector-quantization-compression-accuracy-tradeoff",
    "tags": [
      "智能体",
      "向量数据库",
      "量化压缩"
    ],
    "brief": "核心要点：百万级记忆向量的存储成本高昂，Product Quantization（PQ）可将 1536 维向量压缩至 64 字节（压缩比 24:1），Recall@10 仅下降 3%。OPQ（Optimized PQ）通过旋转矩阵预处理进一步将精度损失控制在 1% 以内。",
    "depth_hint": "需包含 PQ 的子空间划分与码本训练过程、不同压缩比的 Recall 曲线、内存节省的量化计算",
    "status": "pending"
  },
  {
    "id": 407,
    "title": "外部知识库的增量更新与索引同步",
    "slug": "knowledge-base-incremental-update-index-sync",
    "tags": [
      "智能体",
      "知识管理",
      "增量更新"
    ],
    "brief": "核心要点：Agent 依赖的外部知识库需要持续更新，全量重建索引（百万文档约 2 小时）不可接受。增量更新策略包括：变更检测（文档哈希）、差量 Embedding、在线索引插入。Milvus 的动态索引支持实时插入，延迟 <100ms/条。",
    "depth_hint": "需包含增量更新的流水线架构、变更检测算法、增量 vs 全量重建的耗时对比数据",
    "status": "pending"
  },
  {
    "id": 408,
    "title": "记忆检索中的上下文窗口填充策略",
    "slug": "memory-retrieval-context-window-packing",
    "tags": [
      "智能体",
      "记忆系统",
      "上下文优化"
    ],
    "brief": "核心要点：检索到的记忆片段如何填入有限的上下文窗口直接影响 Agent 表现。贪心填充（按相关性排序直到满）、多样性采样（MMR 算法，λ=0.7 时效果最优）、分层摘要填充三种策略。MMR 策略在多跳推理任务中比贪心填充准确率高 11%。",
    "depth_hint": "需包含 MMR 算法的公式推导、λ 参数的消融实验、不同窗口大小下三种策略的对比",
    "status": "pending"
  },
  {
    "id": 409,
    "title": "Generative Agents 的记忆流与重要性评分",
    "slug": "generative-agents-memory-stream-importance",
    "tags": [
      "智能体",
      "记忆系统",
      "Generative Agents"
    ],
    "brief": "核心要点：Stanford Generative Agents 提出记忆流（Memory Stream）架构，每条记忆带有新近性、重要性、相关性三维评分，综合分数 score = α·recency + β·importance + γ·relevance 决定检索排序。重要性由 LLM 自评（1-10 分），新近性按指数衰减。",
    "depth_hint": "需包含三维评分的具体公式与参数选择、记忆检索的完整算法流程、评分权重的敏感性分析",
    "status": "pending"
  },
  {
    "id": 410,
    "title": "基于 SQL 的结构化 Agent 记忆系统",
    "slug": "sql-structured-agent-memory-system",
    "tags": [
      "智能体",
      "记忆系统",
      "结构化存储"
    ],
    "brief": "核心要点：并非所有 Agent 记忆都适合向量化，用户偏好、任务状态、配置参数等结构化数据用 SQL 存储更高效。SQLite 作为嵌入式方案，查询延迟 <1ms，配合 JSON 字段实现半结构化存储，比向量检索在精确匹配场景下准确率高 40%。",
    "depth_hint": "需包含记忆表的 Schema 设计、SQL 与向量检索的适用场景划分、混合查询的实现代码",
    "status": "pending"
  },
  {
    "id": 411,
    "title": "多模态记忆：图像与文本的联合向量存储",
    "slug": "multimodal-memory-image-text-joint-storage",
    "tags": [
      "智能体",
      "多模态",
      "记忆系统"
    ],
    "brief": "核心要点：视觉 Agent 需要存储和检索图像记忆，CLIP 模型将图像和文本映射到同一 512/768 维空间，实现跨模态检索。图文联合索引比分离索引在跨模态查询中 Recall@10 高 22%，但索引构建时间增加 35%。",
    "depth_hint": "需包含 CLIP Embedding 的维度与检索性能关系、跨模态检索的评测指标、联合索引的构建流程",
    "status": "pending"
  },
  {
    "id": 412,
    "title": "记忆蒸馏：从经验中提取可复用规则",
    "slug": "memory-distillation-extracting-reusable-rules",
    "tags": [
      "智能体",
      "记忆蒸馏",
      "经验学习"
    ],
    "brief": "核心要点：Agent 积累大量情景记忆后，需要蒸馏出通用规则以减少存储和检索开销。通过聚类相似经验（DBSCAN，eps=0.3）再用 LLM 归纳规则，可将 1000 条原始记忆压缩为 50 条规则，任务表现仅下降 2%，检索速度提升 20 倍。",
    "depth_hint": "需包含经验聚类的参数选择、规则提取的 Prompt 设计、压缩前后的性能对比实验",
    "status": "pending"
  },
  {
    "id": 413,
    "title": "LangChain Memory 模块的实现与局限",
    "slug": "langchain-memory-module-implementation-limitations",
    "tags": [
      "智能体",
      "LangChain",
      "记忆系统"
    ],
    "brief": "核心要点：LangChain 提供 ConversationBufferMemory、ConversationSummaryMemory、VectorStoreRetrieverMemory 等六种记忆模块。Buffer 适合短对话（<10 轮），Summary 适合长对话但丢失细节，VectorStore 检索精度依赖 Embedding 质量。实际生产中常需组合使用。",
    "depth_hint": "需包含各 Memory 类的核心代码实现、适用场景决策树、组合使用的架构示例",
    "status": "pending"
  },
  {
    "id": 414,
    "title": "记忆检索的重排序：Cross-Encoder 精排",
    "slug": "memory-retrieval-reranking-cross-encoder",
    "tags": [
      "智能体",
      "记忆系统",
      "重排序"
    ],
    "brief": "核心要点：向量近似检索（Bi-Encoder）的召回结果存在排序噪声，Cross-Encoder 重排序可将 NDCG@10 从 0.65 提升至 0.78。Cohere Rerank 和 bge-reranker-v2 在 Agent 记忆场景下延迟约 50-100ms，适合 Top-20 候选精排。",
    "depth_hint": "需包含 Bi-Encoder vs Cross-Encoder 的架构差异、重排序的延迟-精度权衡、候选集大小的消融实验",
    "status": "pending"
  },
  {
    "id": 415,
    "title": "Agent 记忆的隐私保护与选择性遗忘",
    "slug": "agent-memory-privacy-selective-forgetting",
    "tags": [
      "智能体",
      "记忆系统",
      "隐私安全"
    ],
    "brief": "核心要点：Agent 长期记忆中可能存储敏感信息，需要支持选择性遗忘（GDPR 合规）。向量删除后索引中仍可能残留近似信息，需要配合向量扰动（差分隐私 ε=1.0）或索引重建。完全遗忘的计算开销约为插入的 5-10 倍。",
    "depth_hint": "需包含差分隐私在向量空间的应用公式、选择性删除的实现方案、遗忘完整性的验证方法",
    "status": "pending"
  },
  {
    "id": 416,
    "title": "基于 Graph RAG 的多跳知识检索",
    "slug": "graph-rag-multi-hop-knowledge-retrieval",
    "tags": [
      "智能体",
      "Graph RAG",
      "知识检索"
    ],
    "brief": "核心要点：传统 RAG 单次检索难以回答需要多步推理的问题，Graph RAG 将文档构建为实体关系图，支持沿图谱边进行多跳检索。微软 Graph RAG 在全局摘要任务中比朴素 RAG 的全面性评分高 20-30%，但索引构建成本增加 5-10 倍。",
    "depth_hint": "需包含图谱构建的实体抽取流程、社区检测算法选择、多跳检索与直接检索的质量对比",
    "status": "pending"
  },
  {
    "id": 417,
    "title": "Sparse-Dense 混合检索在记忆系统中的实践",
    "slug": "sparse-dense-hybrid-retrieval-memory-system",
    "tags": [
      "智能体",
      "混合检索",
      "记忆系统"
    ],
    "brief": "核心要点：纯密集检索（Dense）对精确关键词匹配不敏感，纯稀疏检索（BM25）缺乏语义理解。混合检索 score = α·dense + (1-α)·sparse，α=0.7 时在 Agent 记忆检索中 MRR@10 最优，比单一方案高 10-15%。SPLADE 作为学习型稀疏表示可进一步提升。",
    "depth_hint": "需包含混合评分的归一化方法、α 参数搜索实验、BM25/Dense/SPLADE/Hybrid 四者的全面对比",
    "status": "pending"
  },
  {
    "id": 418,
    "title": "Agent 记忆系统的端到端评测框架",
    "slug": "agent-memory-system-evaluation-framework",
    "tags": [
      "智能体",
      "记忆系统",
      "评测"
    ],
    "brief": "核心要点：缺乏标准化评测框架是 Agent 记忆研究的瓶颈。提出四维评测体系：检索精度（Recall/MRR）、信息保持率（长对话后的事实回忆准确率）、存储效率（Token/字节比）、任务贡献度（有无记忆的任务完成率差异）。在 5 个基准任务上验证框架有效性。",
    "depth_hint": "需包含四维指标的具体定义与计算方法、基准任务设计、主流记忆方案的横评数据",
    "status": "pending"
  },
  {
    "id": 419,
    "title": "记忆感知的 Prompt 构建与动态注入",
    "slug": "memory-aware-prompt-construction-dynamic-injection",
    "tags": [
      "智能体",
      "记忆系统",
      "Prompt工程"
    ],
    "brief": "核心要点：检索到的记忆如何注入 Prompt 直接影响 LLM 利用效率。对比前置注入（System Prompt）、上下文内嵌、工具调用返回三种方式，前置注入的记忆利用率最高（85%）但占用固定窗口空间，工具调用方式最灵活但引入额外延迟（100-200ms）。",
    "depth_hint": "需包含三种注入方式的 Prompt 模板示例、记忆利用率的量化测量方法、不同注入位置的注意力分布分析",
    "status": "pending"
  },
  {
    "id": 420,
    "title": "多Agent系统的角色分工设计模式",
    "slug": "multi-agent-role-design-patterns",
    "tags": [
      "智能体",
      "多Agent",
      "架构设计"
    ],
    "brief": "核心要点：分析多Agent系统中常见的角色分工模式——专家型、流水线型、辩论型、层级型。对比各模式在代码生成任务上的准确率差异（专家型85% vs 辩论型91%），给出角色定义的Prompt模板与职责边界划分原则。",
    "depth_hint": "需包含角色定义Prompt示例、各模式的任务完成率对比表、职责冲突的仲裁逻辑",
    "status": "pending"
  },
  {
    "id": 421,
    "title": "AutoGen的会话编排与终止条件设计",
    "slug": "autogen-conversation-orchestration-termination",
    "tags": [
      "智能体",
      "AutoGen",
      "多Agent"
    ],
    "brief": "核心要点：深入AutoGen的GroupChat机制，解析speaker_selection_method的四种策略（round_robin/random/auto/manual）及其对任务收敛速度的影响。分析max_round与is_termination_msg的组合策略，避免Agent陷入无限对话循环的实践方案。",
    "depth_hint": "需包含GroupChat源码关键路径、终止条件的状态机图、不同选择策略的Token消耗对比",
    "status": "pending"
  },
  {
    "id": 422,
    "title": "CrewAI的Task依赖链与执行调度",
    "slug": "crewai-task-dependency-scheduling",
    "tags": [
      "智能体",
      "CrewAI",
      "任务调度"
    ],
    "brief": "核心要点：解析CrewAI中Task的context参数如何构建DAG依赖图，对比sequential与hierarchical两种Process模式的执行语义差异。分析任务输出的expected_output字段如何约束Agent行为，以及async_execution参数实现并行任务的底层机制。",
    "depth_hint": "需包含DAG调度伪代码、两种Process模式的执行时序图、并行任务的线程模型",
    "status": "pending"
  },
  {
    "id": 423,
    "title": "多Agent通信的消息传递协议设计",
    "slug": "multi-agent-message-passing-protocol",
    "tags": [
      "智能体",
      "通信协议",
      "系统设计"
    ],
    "brief": "核心要点：对比多Agent系统中三种通信范式——共享黑板、点对点消息、发布订阅。分析消息格式标准化（sender/receiver/content/metadata）对系统可扩展性的影响，以及消息队列在异步Agent协作中的缓冲与背压处理。",
    "depth_hint": "需包含三种通信范式的架构图、消息Schema定义、消息丢失与重试的处理代码",
    "status": "pending"
  },
  {
    "id": 424,
    "title": "AutoGen中自定义Agent的注册与能力声明",
    "slug": "autogen-custom-agent-registration-capability",
    "tags": [
      "智能体",
      "AutoGen",
      "扩展机制"
    ],
    "brief": "核心要点：解析AutoGen v0.4的ConversableAgent基类继承体系，分析register_reply方法的触发链机制。详解如何通过system_message声明Agent能力边界，以及register_function将Python函数暴露为Agent可调用工具的完整流程。",
    "depth_hint": "需包含Agent继承链类图、register_reply的优先级匹配逻辑、自定义Agent的完整代码示例",
    "status": "pending"
  },
  {
    "id": 425,
    "title": "多Agent辩论机制提升推理准确率",
    "slug": "multi-agent-debate-reasoning-accuracy",
    "tags": [
      "智能体",
      "推理增强",
      "辩论机制"
    ],
    "brief": "核心要点：分析Du et al. (2023)提出的多Agent辩论框架，通过3轮辩论在GSM8K上将准确率从78%提升至85%。解析辩论轮次、Agent数量与准确率的关系曲线，以及majority voting与weighted voting的聚合策略差异。",
    "depth_hint": "需包含辩论框架的Prompt模板、准确率-轮次关系图数据、投票聚合的数学公式",
    "status": "pending"
  },
  {
    "id": 426,
    "title": "CrewAI的Agent记忆与学习机制",
    "slug": "crewai-agent-memory-learning",
    "tags": [
      "智能体",
      "CrewAI",
      "记忆系统"
    ],
    "brief": "核心要点：解析CrewAI中short_term_memory、long_term_memory和entity_memory三层记忆架构的存储实现。分析memory=True开启后的RAG检索流程，以及Crew级别shared_memory如何在多Agent间同步任务上下文与中间结果。",
    "depth_hint": "需包含三层记忆的存储后端对比表、检索注入的Prompt拼接逻辑、记忆写入的触发条件",
    "status": "pending"
  },
  {
    "id": 427,
    "title": "层级式多Agent的管理者路由策略",
    "slug": "hierarchical-agent-manager-routing",
    "tags": [
      "智能体",
      "多Agent",
      "路由策略"
    ],
    "brief": "核心要点：分析层级式多Agent架构中Manager Agent的任务分解与路由决策机制。对比基于规则、基于语义相似度、基于LLM判断三种路由策略的延迟与准确率，以及子任务结果的汇聚与冲突检测算法。",
    "depth_hint": "需包含路由决策的Prompt模板、三种策略的延迟/准确率对比表、结果汇聚的伪代码",
    "status": "pending"
  },
  {
    "id": 428,
    "title": "多Agent系统中的死锁检测与预防",
    "slug": "multi-agent-deadlock-detection-prevention",
    "tags": [
      "智能体",
      "多Agent",
      "系统可靠性"
    ],
    "brief": "核心要点：分析多Agent协作中因循环等待（A等B的输出、B等A的输出）导致的死锁场景。提出基于超时机制、资源有序分配、死锁检测图的三种解决方案，以及在AutoGen GroupChat中实现max_stall_count的实践。",
    "depth_hint": "需包含死锁场景的时序图、等待图的环检测算法、AutoGen中的超时配置代码",
    "status": "pending"
  },
  {
    "id": 429,
    "title": "AutoGen的代码执行沙箱架构",
    "slug": "autogen-code-execution-sandbox",
    "tags": [
      "智能体",
      "AutoGen",
      "安全隔离"
    ],
    "brief": "核心要点：解析AutoGen中LocalCommandLineCodeExecutor与DockerCommandLineCodeExecutor的实现差异。分析Docker沙箱的资源限制配置（CPU/内存/网络）、文件系统挂载策略，以及代码执行结果的stdout/stderr捕获与超时终止机制。",
    "depth_hint": "需包含两种Executor的架构对比图、Docker资源限制参数表、执行超时的信号处理代码",
    "status": "pending"
  },
  {
    "id": 430,
    "title": "多Agent的共识达成算法与投票机制",
    "slug": "multi-agent-consensus-voting-mechanism",
    "tags": [
      "智能体",
      "多Agent",
      "协调机制"
    ],
    "brief": "核心要点：分析多Agent系统中达成共识的常见算法——多数投票、加权投票、Borda计数、LLM-as-Judge。对比各算法在代码审查任务上的一致率（多数投票72% vs LLM-Judge 89%），以及拜占庭容错在Agent可靠性中的启示。",
    "depth_hint": "需包含各投票算法的数学定义、一致率对比实验数据、拜占庭容错的简化模型",
    "status": "pending"
  },
  {
    "id": 431,
    "title": "CrewAI与LangGraph的多Agent编排对比",
    "slug": "crewai-vs-langgraph-orchestration",
    "tags": [
      "智能体",
      "CrewAI",
      "LangGraph"
    ],
    "brief": "核心要点：从编排范式（声明式vs图式）、状态管理（隐式vs显式State）、控制流（Process vs Edge/Condition）三个维度对比CrewAI与LangGraph。分析各自适用场景——CrewAI适合角色驱动任务、LangGraph适合复杂状态流转。",
    "depth_hint": "需包含同一任务的双框架实现代码、特性对比表、状态传递机制的架构图",
    "status": "pending"
  },
  {
    "id": 432,
    "title": "多Agent系统的可观测性与调试追踪",
    "slug": "multi-agent-observability-tracing",
    "tags": [
      "智能体",
      "多Agent",
      "可观测性"
    ],
    "brief": "核心要点：分析多Agent系统调试的核心挑战——消息流不透明、状态分散、因果链难追踪。介绍基于OpenTelemetry的分布式追踪方案，以及AutoGen的logging模块和CrewAI的verbose模式在生产环境中的局限与增强方案。",
    "depth_hint": "需包含Trace span的层级结构设计、关键日志字段定义、可视化Dashboard的数据模型",
    "status": "pending"
  },
  {
    "id": 433,
    "title": "多Agent任务分解的递归细化策略",
    "slug": "multi-agent-recursive-task-decomposition",
    "tags": [
      "智能体",
      "多Agent",
      "任务规划"
    ],
    "brief": "核心要点：分析将复杂任务递归分解为子任务的策略——深度优先分解、广度优先分解、自适应粒度控制。解析分解终止条件的判定（子任务Token数<阈值、单Agent可独立完成），以及子任务间依赖关系的自动推断机制。",
    "depth_hint": "需包含递归分解的Prompt链设计、粒度判定的启发式规则、依赖推断的示例输入输出",
    "status": "pending"
  },
  {
    "id": 434,
    "title": "AutoGen中的人机协作与Human-in-the-Loop",
    "slug": "autogen-human-in-the-loop",
    "tags": [
      "智能体",
      "AutoGen",
      "人机协作"
    ],
    "brief": "核心要点：解析AutoGen的UserProxyAgent与人类交互的三种模式——ALWAYS（每步确认）、TERMINATE（仅终止时）、NEVER（全自动）。分析human_input_mode对任务完成率与安全性的影响，以及在关键决策节点插入人类审批的中间件设计。",
    "depth_hint": "需包含三种模式的交互流程图、安全性对比数据、审批中间件的接口定义",
    "status": "pending"
  },
  {
    "id": 435,
    "title": "多Agent系统的Token预算分配与成本控制",
    "slug": "multi-agent-token-budget-cost-control",
    "tags": [
      "智能体",
      "多Agent",
      "成本优化"
    ],
    "brief": "核心要点：分析多Agent系统中Token消耗爆炸的根因——N个Agent的上下文窗口叠加、冗余信息重复传递。提出Token预算分配策略：per-agent上限、全局预算池、动态优先级分配，以及通过消息摘要压缩降低40%Token消耗的实践。",
    "depth_hint": "需包含Token消耗的数学模型、预算分配算法伪代码、摘要压缩前后的Token对比",
    "status": "pending"
  },
  {
    "id": 436,
    "title": "基于事件驱动的Agent协调架构",
    "slug": "event-driven-agent-coordination",
    "tags": [
      "智能体",
      "多Agent",
      "事件驱动"
    ],
    "brief": "核心要点：对比请求-响应式与事件驱动式Agent协调的架构差异。分析事件总线（Event Bus）在多Agent系统中的实现——事件定义、订阅注册、异步分发，以及事件溯源（Event Sourcing）在Agent状态恢复中的应用。",
    "depth_hint": "需包含事件驱动架构图、事件Schema定义、事件溯源的状态重放代码示例",
    "status": "pending"
  },
  {
    "id": 437,
    "title": "多Agent的动态组队与角色自适应",
    "slug": "multi-agent-dynamic-teaming-role-adaptation",
    "tags": [
      "智能体",
      "多Agent",
      "动态组队"
    ],
    "brief": "核心要点：分析静态角色分配的局限性，提出基于任务特征动态组建Agent团队的机制。解析Agent能力向量与任务需求向量的匹配算法（余弦相似度>0.8为匹配），以及运行时角色切换的Prompt重写策略。",
    "depth_hint": "需包含能力-需求匹配的向量定义、匹配算法代码、角色切换的Prompt模板对比",
    "status": "pending"
  },
  {
    "id": 438,
    "title": "AutoGen的状态持久化与断点恢复",
    "slug": "autogen-state-persistence-checkpoint",
    "tags": [
      "智能体",
      "AutoGen",
      "容错恢复"
    ],
    "brief": "核心要点：分析AutoGen中长时间运行的多Agent会话的状态持久化需求。解析Agent chat_messages的序列化格式、GroupChat状态的checkpoint存储，以及从断点恢复会话时的消息重放与状态一致性校验机制。",
    "depth_hint": "需包含状态序列化的数据结构、checkpoint的存储格式、恢复流程的步骤与校验逻辑",
    "status": "pending"
  },
  {
    "id": 439,
    "title": "多Agent对抗式测试与红蓝对抗",
    "slug": "multi-agent-adversarial-red-blue-testing",
    "tags": [
      "智能体",
      "多Agent",
      "安全测试"
    ],
    "brief": "核心要点：分析利用多Agent框架构建红蓝对抗系统——Red Agent生成攻击Prompt、Blue Agent执行防御检测、Judge Agent评估结果。解析对抗迭代的收敛条件，以及在Prompt注入防御中红队Agent的攻击策略演化机制。",
    "depth_hint": "需包含红蓝对抗的系统架构图、攻击策略的变异算法、防御效果的评估指标",
    "status": "pending"
  },
  {
    "id": 440,
    "title": "CrewAI的Tool委托与跨Agent工具共享",
    "slug": "crewai-tool-delegation-sharing",
    "tags": [
      "智能体",
      "CrewAI",
      "工具系统"
    ],
    "brief": "核心要点：解析CrewAI中allow_delegation参数的工作机制——Agent如何将子任务委托给团队中其他具备特定工具的Agent。分析Tool的定义规范（BaseTool继承、_run方法实现），以及工具权限隔离与共享的设计决策。",
    "depth_hint": "需包含委托请求的内部消息格式、自定义Tool的完整代码、权限矩阵设计",
    "status": "pending"
  },
  {
    "id": 441,
    "title": "多Agent系统的负载均衡与弹性伸缩",
    "slug": "multi-agent-load-balancing-scaling",
    "tags": [
      "智能体",
      "多Agent",
      "系统工程"
    ],
    "brief": "核心要点：分析多Agent系统在高并发场景下的性能瓶颈——LLM API调用延迟、上下文窗口占用、消息队列积压。提出基于Agent池的负载均衡策略（轮询/最少连接/能力路由），以及Agent实例的水平伸缩与冷启动优化。",
    "depth_hint": "需包含负载均衡算法对比表、Agent池的资源管理伪代码、伸缩触发条件的阈值设计",
    "status": "pending"
  },
  {
    "id": 442,
    "title": "多Agent的结构化输出与Schema协商",
    "slug": "multi-agent-structured-output-schema",
    "tags": [
      "智能体",
      "多Agent",
      "数据交换"
    ],
    "brief": "核心要点：分析多Agent间传递非结构化文本的信息损耗问题。提出基于JSON Schema的Agent间结构化通信协议——输出Schema声明、输入Schema校验、Schema不兼容时的自动适配。对比Pydantic BaseModel与JSON Schema在Agent输出约束中的实践。",
    "depth_hint": "需包含Schema协商的握手协议、Pydantic输出约束代码、Schema版本兼容处理逻辑",
    "status": "pending"
  },
  {
    "id": 443,
    "title": "AutoGen的多模型混合编排策略",
    "slug": "autogen-multi-model-orchestration",
    "tags": [
      "智能体",
      "AutoGen",
      "模型选择"
    ],
    "brief": "核心要点：解析AutoGen中为不同Agent配置不同LLM的策略——推理Agent用GPT-4、编码Agent用Claude、摘要Agent用GPT-3.5以降低成本。分析config_list的优先级与fallback机制，以及模型能力差异对多Agent协作质量的影响。",
    "depth_hint": "需包含config_list的配置示例、不同模型组合的成本-质量对比表、fallback链的实现逻辑",
    "status": "pending"
  },
  {
    "id": 444,
    "title": "多Agent的工作流编排DSL设计",
    "slug": "multi-agent-workflow-dsl-design",
    "tags": [
      "智能体",
      "多Agent",
      "DSL设计"
    ],
    "brief": "核心要点：分析用DSL（领域特定语言）描述多Agent工作流的设计思路——节点定义（Agent）、边定义（消息传递）、条件分支、循环结构。对比YAML声明式、Python装饰器式、图可视化三种DSL形态的表达力与易用性。",
    "depth_hint": "需包含DSL语法示例（三种形态各一）、工作流执行引擎的核心循环伪代码",
    "status": "pending"
  },
  {
    "id": 445,
    "title": "多Agent协作中的信息不对称与隐私保护",
    "slug": "multi-agent-information-asymmetry-privacy",
    "tags": [
      "智能体",
      "多Agent",
      "隐私安全"
    ],
    "brief": "核心要点：分析多Agent协作中的信息可见性控制——并非所有Agent都应看到完整对话历史。提出基于角色的访问控制（RBAC）模型：消息过滤、上下文裁剪、敏感字段脱敏，以及在医疗/金融场景中的合规性要求。",
    "depth_hint": "需包含RBAC权限矩阵设计、消息过滤的中间件代码、脱敏规则的正则表达式示例",
    "status": "pending"
  },
  {
    "id": 446,
    "title": "Autogen Studio的可视化Agent编排",
    "slug": "autogen-studio-visual-orchestration",
    "tags": [
      "智能体",
      "AutoGen",
      "低代码"
    ],
    "brief": "核心要点：解析AutoGen Studio的架构——前端React可视化画布、后端FastAPI编排引擎、WebSocket实时通信。分析其Gallery模板系统如何降低多Agent应用开发门槛，以及自定义Skill（Python函数）的注册与版本管理机制。",
    "depth_hint": "需包含Studio架构图、Gallery模板的JSON结构、Skill注册的API接口定义",
    "status": "pending"
  },
  {
    "id": 447,
    "title": "多Agent系统的错误传播与容错策略",
    "slug": "multi-agent-error-propagation-fault-tolerance",
    "tags": [
      "智能体",
      "多Agent",
      "容错设计"
    ],
    "brief": "核心要点：分析多Agent系统中单Agent错误的级联传播路径——幻觉输出→下游Agent决策偏移→系统整体失败。提出三级容错策略：Agent级重试与自我纠正、流水线级checkpoint回滚、系统级降级与人工接管，以及错误隔离的断路器模式。",
    "depth_hint": "需包含错误传播的概率模型、断路器状态机实现、三级容错的决策流程图",
    "status": "pending"
  },
  {
    "id": 448,
    "title": "Swarm框架的轻量级Agent切换机制",
    "slug": "swarm-lightweight-agent-handoff",
    "tags": [
      "智能体",
      "Swarm",
      "Agent切换"
    ],
    "brief": "核心要点：解析OpenAI Swarm框架的核心设计——Agent间通过handoff函数实现轻量级切换，无需复杂的编排层。分析其与AutoGen/CrewAI的本质差异：Swarm是客户端编排、无状态、单次对话切换。对比三者在客服路由场景中的实现复杂度。",
    "depth_hint": "需包含Swarm handoff的源码分析、三框架的架构差异对比表、客服路由的完整代码示例",
    "status": "pending"
  },
  {
    "id": 449,
    "title": "多Agent评估基准与性能度量体系",
    "slug": "multi-agent-evaluation-benchmark",
    "tags": [
      "智能体",
      "多Agent",
      "评测"
    ],
    "brief": "核心要点：分析多Agent系统评估的四个维度——任务完成率、协作效率（消息轮次）、Token成本、延迟。介绍AgentBench、BOLAA等基准测试的评测方法论，以及自建评测Pipeline的关键设计：确定性场景复现、自动化评分、A/B对比框架。",
    "depth_hint": "需包含评估指标的数学定义、AgentBench的任务类型分布、自建评测Pipeline的架构图",
    "status": "pending"
  },
  {
    "id": 450,
    "title": "GAIA基准详解：通用AI助手的评测方法论",
    "slug": "gaia-benchmark-methodology",
    "tags": [
      "智能体",
      "Agent评测",
      "GAIA"
    ],
    "brief": "核心要点：解析GAIA基准的三级难度划分（Level 1/2/3），466道题目的设计原则——无需专业知识但需多步推理与工具使用。对比GPT-4（15%）与人类（92%）的通过率差距，分析失败案例的分布规律，揭示当前Agent在多步工具链组合上的系统性短板。",
    "depth_hint": "需包含GAIA评分标准的exact match机制、三级难度的具体区分标准、各模型得分对比表",
    "status": "pending"
  },
  {
    "id": 451,
    "title": "SWE-bench：代码修复Agent的真实能力边界",
    "slug": "swe-bench-code-repair-evaluation",
    "tags": [
      "智能体",
      "Agent评测",
      "SWE-bench"
    ],
    "brief": "核心要点：SWE-bench从2294个GitHub真实issue构建评测，要求Agent定位bug并生成patch。分析SWE-agent（12.5%→18%）的架构改进，对比Devin、AutoCodeRover等方案的解题率差异。重点拆解Lite子集（300题）的通过率瓶颈：跨文件定位失败占比超过60%。",
    "depth_hint": "需包含SWE-bench数据构建流程、patch验证的测试用例匹配机制、各Agent在Lite子集的对比数据",
    "status": "pending"
  },
  {
    "id": 452,
    "title": "AgentBench：八维度Agent能力的标准化评测",
    "slug": "agentbench-eight-dimensions",
    "tags": [
      "智能体",
      "Agent评测",
      "AgentBench"
    ],
    "brief": "核心要点：AgentBench覆盖操作系统、数据库、知识图谱、数字卡牌、家居环境、网页浏览、网上购物、拼图八个交互环境。分析GPT-4在OS环境得分42.4%但在KG环境仅9.1%的能力不均衡现象，揭示开源模型与闭源模型的性能断崖（平均差距超30%）。",
    "depth_hint": "需包含八个环境的具体评测协议、各模型的雷达图对比、开源vs闭源的统计显著性检验",
    "status": "pending"
  },
  {
    "id": 453,
    "title": "Agent自我反思的ReAct与Reflexion对比",
    "slug": "react-vs-reflexion-self-reflection",
    "tags": [
      "智能体",
      "自我反思",
      "推理框架"
    ],
    "brief": "核心要点：ReAct将推理与行动交织但缺乏失败后的系统性修正，Reflexion引入语言化反馈信号实现跨episode学习。在HotPotQA上Reflexion将ReAct的34%提升至68%，在AlfWorld上从55%提升至97%。解析反思记忆的滑动窗口机制与反思质量对收敛速度的影响。",
    "depth_hint": "需包含两种框架的prompt模板对比、反思信号的具体生成格式、HotPotQA/AlfWorld的迭代收敛曲线",
    "status": "pending"
  },
  {
    "id": 454,
    "title": "Agent错误恢复的回退策略与检查点机制",
    "slug": "agent-error-recovery-checkpoint",
    "tags": [
      "智能体",
      "错误恢复",
      "容错"
    ],
    "brief": "核心要点：Agent执行链中错误可分为工具调用失败、推理偏离、环境状态不可逆三类。分析检查点保存的粒度选择——按步骤vs按子目标，状态回滚的实现方式，以及回退后重规划的策略。以SWE-agent的重试机制为例，说明最大重试次数与成功率的非线性关系。",
    "depth_hint": "需包含三类错误的判定条件代码、检查点序列化方案、重试次数-成功率的实验数据",
    "status": "pending"
  },
  {
    "id": 455,
    "title": "Agent安全边界：Prompt注入攻击与防御",
    "slug": "agent-prompt-injection-defense",
    "tags": [
      "智能体",
      "安全边界",
      "Prompt注入"
    ],
    "brief": "核心要点：Agent使用工具读取外部内容时面临间接Prompt注入风险。攻击向量包括网页嵌入指令、文件隐藏指令、API返回值污染。分析Dual-LLM架构（特权层+隔离层）、输入消毒、输出验证三种防御方案的有效性，实测表明组合防御可将攻击成功率从78%降至12%。",
    "depth_hint": "需包含三种攻击向量的具体示例、Dual-LLM架构图、防御方案的攻击成功率对比表",
    "status": "pending"
  },
  {
    "id": 456,
    "title": "GAIA Level-3题目的多工具链编排分析",
    "slug": "gaia-level3-tool-chain-analysis",
    "tags": [
      "智能体",
      "Agent评测",
      "GAIA"
    ],
    "brief": "核心要点：GAIA Level-3题目平均需要5-8步工具调用，涉及搜索、计算、文件解析、代码执行的复杂组合。分析工具调用顺序对成功率的影响，错误在链路中的传播模式。当前最优Agent在Level-3的通过率不足5%，主要瓶颈在中间结果的验证缺失和工具选择错误。",
    "depth_hint": "需包含Level-3典型题目的完整工具调用链、错误传播概率模型、工具选择准确率统计",
    "status": "pending"
  },
  {
    "id": 457,
    "title": "SWE-bench Verified：人工验证子集的评测意义",
    "slug": "swe-bench-verified-subset",
    "tags": [
      "智能体",
      "Agent评测",
      "SWE-bench"
    ],
    "brief": "核心要点：SWE-bench原始数据存在测试用例不充分、题目描述歧义等问题，Verified子集（500题）经人工筛选确保每题有明确正确答案。对比Agent在原始集与Verified子集的排名变化，分析刷榜风险与评测公平性。Claude 3.5在Verified上达49%，显著高于原始集表现。",
    "depth_hint": "需包含Verified筛选标准、原始集vs Verified的Agent排名对比表、测试用例覆盖率分析",
    "status": "pending"
  },
  {
    "id": 458,
    "title": "Agent自我纠错的内省机制设计",
    "slug": "agent-introspection-self-correction",
    "tags": [
      "智能体",
      "自我反思",
      "纠错机制"
    ],
    "brief": "核心要点：自我纠错包含错误检测、原因归因、修正生成三个阶段。分析基于输出一致性检查、置信度阈值、外部验证器三种检测方法的精度与召回率。研究表明无外部反馈的纯自纠错可能导致性能下降（正确答案被改错的概率约15-25%），需设计选择性纠错策略。",
    "depth_hint": "需包含三种检测方法的精度-召回率对比、选择性纠错的决策树、自纠错反而降低性能的实验数据",
    "status": "pending"
  },
  {
    "id": 459,
    "title": "Agent工具调用的权限控制与沙箱隔离",
    "slug": "agent-tool-permission-sandbox",
    "tags": [
      "智能体",
      "安全边界",
      "沙箱"
    ],
    "brief": "核心要点：Agent可调用文件系统、网络请求、代码执行等工具，需要细粒度权限控制。分析基于能力的访问控制模型（Capability-based）、最小权限原则的实现、Docker/gVisor/Firecracker三种沙箱方案在Agent场景的适用性与性能开销（延迟增加5-50ms）。",
    "depth_hint": "需包含权限声明的JSON Schema示例、三种沙箱的安全性-性能对比表、逃逸攻击面分析",
    "status": "pending"
  },
  {
    "id": 460,
    "title": "Reflexion的长期记忆管理与反思质量衰减",
    "slug": "reflexion-memory-management-decay",
    "tags": [
      "智能体",
      "自我反思",
      "Reflexion"
    ],
    "brief": "核心要点：Reflexion将自然语言反思存入长期记忆供后续episode使用，但随迭代次数增加，记忆条目增多导致检索噪声和上下文窗口溢出。分析反思摘要压缩、相似去重、时间衰减加权三种记忆管理策略，实验表明保留最近3-5条反思的效果优于全量保留。",
    "depth_hint": "需包含记忆检索的余弦相似度阈值设定、不同保留策略的成功率对比曲线、上下文长度与性能关系",
    "status": "pending"
  },
  {
    "id": 461,
    "title": "Agent执行链的异常分类与恢复策略选择",
    "slug": "agent-exception-taxonomy-recovery",
    "tags": [
      "智能体",
      "错误恢复",
      "异常处理"
    ],
    "brief": "核心要点：建立Agent异常的四层分类体系——环境异常（API超时/限流）、工具异常（参数错误/返回格式异常）、推理异常（幻觉/循环）、目标异常（任务不可行）。每类异常对应不同恢复策略：重试、参数修正、重规划、任务终止。分析各策略的适用条件与误判代价。",
    "depth_hint": "需包含四层异常的判定规则代码、每类异常的恢复策略决策流程图、误判率统计",
    "status": "pending"
  },
  {
    "id": 462,
    "title": "WebArena：网页交互Agent的端到端评测",
    "slug": "webarena-web-agent-evaluation",
    "tags": [
      "智能体",
      "Agent评测",
      "WebArena"
    ],
    "brief": "核心要点：WebArena部署五个真实网站（电商、论坛、CMS、GitLab、地图）构建812个任务，要求Agent完成完整的网页操作序列。GPT-4的端到端成功率仅14.4%，主要失败原因：页面状态理解错误（35%）、操作序列过长导致迷失（28%）、动态元素定位失败（22%）。",
    "depth_hint": "需包含WebArena环境搭建架构、五个网站的任务分布、失败原因的详细分类统计",
    "status": "pending"
  },
  {
    "id": 463,
    "title": "Agent安全的红队测试方法论",
    "slug": "agent-red-teaming-methodology",
    "tags": [
      "智能体",
      "安全边界",
      "红队测试"
    ],
    "brief": "核心要点：Agent红队测试需覆盖目标劫持、权限提升、信息泄露、拒绝服务四个攻击面。设计对抗性测试用例的系统方法：基于攻击树（Attack Tree）枚举路径、基于模糊测试生成边界输入、基于对抗提示探测安全护栏。分析自动化红队（用LLM生成攻击）的效率与覆盖率。",
    "depth_hint": "需包含攻击树的具体示例、自动化红队的prompt模板、护栏绕过率的实验数据",
    "status": "pending"
  },
  {
    "id": 464,
    "title": "SWE-agent的Agent-Computer Interface设计",
    "slug": "swe-agent-aci-design",
    "tags": [
      "智能体",
      "Agent评测",
      "SWE-agent"
    ],
    "brief": "核心要点：SWE-agent提出ACI（Agent-Computer Interface）概念，通过定制化命令行接口降低Agent操作复杂度。核心设计：文件查看器（带行号滚动）、搜索命令（语义级grep）、编辑命令（带lint校验的原子操作）。ACI使基础模型的SWE-bench解题率从2.3%提升至12.5%。",
    "depth_hint": "需包含ACI的完整命令列表与设计动机、各命令的使用频率统计、消融实验结果",
    "status": "pending"
  },
  {
    "id": 465,
    "title": "Agent推理循环检测与强制终止机制",
    "slug": "agent-loop-detection-termination",
    "tags": [
      "智能体",
      "错误恢复",
      "循环检测"
    ],
    "brief": "核心要点：Agent在复杂任务中易陷入重复推理循环（同一工具反复调用、相似思考链重现），浪费token且无法收敛。分析基于行为签名比对、n-gram重复率检测、状态空间覆盖度三种循环检测方法。设计渐进式干预策略：提示→重规划→子目标切换→强制终止的四级响应。",
    "depth_hint": "需包含循环检测的具体算法伪代码、n-gram阈值的调优实验、四级干预的触发条件",
    "status": "pending"
  },
  {
    "id": 466,
    "title": "Agent输出的事实性验证与幻觉检测",
    "slug": "agent-output-factuality-hallucination",
    "tags": [
      "智能体",
      "安全边界",
      "幻觉检测"
    ],
    "brief": "核心要点：Agent生成的工具调用参数和最终回答均可能包含幻觉。分析三种检测方案：基于知识库的事实核查、基于自一致性的多次采样比对（SC@k）、基于不确定性估计的token级置信度。在ToolBench上，SC@5可将幻觉工具调用率从18%降至7%，但计算成本增加5倍。",
    "depth_hint": "需包含SC@k的具体实现流程、不确定性估计的计算方法、幻觉率-计算成本的权衡曲线",
    "status": "pending"
  },
  {
    "id": 467,
    "title": "τ-bench：工具使用Agent的对话式评测",
    "slug": "tau-bench-tool-agent-evaluation",
    "tags": [
      "智能体",
      "Agent评测",
      "τ-bench"
    ],
    "brief": "核心要点：τ-bench模拟客服场景，评测Agent在多轮对话中正确使用工具（数据库查询、订单操作）的能力。引入策略遵循度和数据库一致性双重指标，GPT-4o在航空领域得分50.0%、零售领域得分73.9%。分析工具调用顺序依赖和上下文切换导致的性能衰减模式。",
    "depth_hint": "需包含τ-bench的任务构造方法、双重评分指标定义、上下文长度与得分的关系曲线",
    "status": "pending"
  },
  {
    "id": 468,
    "title": "Agent的渐进式规划与动态子目标调整",
    "slug": "agent-progressive-planning-subgoals",
    "tags": [
      "智能体",
      "自我反思",
      "规划"
    ],
    "brief": "核心要点：静态规划在长序列任务中失败率高，渐进式规划在每步执行后根据环境反馈动态调整后续子目标。分析Plan-and-Solve、ADaPT、LATS三种方案的子目标粒度与调整频率。在ALFWorld上，ADaPT的动态分解使成功率从ReAct的55%提升至85%，但规划开销增加40%。",
    "depth_hint": "需包含三种方案的规划-执行交互流程图、子目标粒度的消融实验、规划开销的token统计",
    "status": "pending"
  },
  {
    "id": 469,
    "title": "Agent对抗性环境下的鲁棒性评测",
    "slug": "agent-adversarial-robustness-eval",
    "tags": [
      "智能体",
      "Agent评测",
      "鲁棒性"
    ],
    "brief": "核心要点：评测Agent在对抗性环境中的表现：工具返回误导信息、环境状态被篡改、用户指令含陷阱。AdvBench和AgentHarm基准的设计原理与攻击类别。分析Agent在正常vs对抗环境下的性能差异，GPT-4在对抗设置下的成功率平均下降35-50%。",
    "depth_hint": "需包含对抗性环境的具体构造方法、AgentHarm的攻击分类、正常vs对抗的性能对比表",
    "status": "pending"
  },
  {
    "id": 470,
    "title": "Agent的失败归因：从日志到根因分析",
    "slug": "agent-failure-attribution-root-cause",
    "tags": [
      "智能体",
      "错误恢复",
      "根因分析"
    ],
    "brief": "核心要点：Agent失败后需要从执行日志中定位根因——是工具选择错误、参数构造错误还是推理链偏离。分析基于执行轨迹回溯、关键决策点标记、反事实分析三种归因方法。以SWE-bench失败案例为例，70%的失败可追溯至代码定位阶段的搜索策略不当。",
    "depth_hint": "需包含执行轨迹的结构化日志格式、反事实分析的具体实现、SWE-bench失败归因的统计分布",
    "status": "pending"
  },
  {
    "id": 471,
    "title": "Agent的越权行为检测与意图对齐验证",
    "slug": "agent-overreach-detection-alignment",
    "tags": [
      "智能体",
      "安全边界",
      "意图对齐"
    ],
    "brief": "核心要点：Agent可能超出用户授权范围执行操作（如被要求查找文件却尝试删除文件）。分析基于行为规范的合规检查、基于意图推理的预操作审核、基于操作影响评估的后验验证三层防护。实现细粒度的操作白名单和基于LLM的意图-操作一致性判定器。",
    "depth_hint": "需包含操作白名单的配置示例、意图-操作一致性判定的prompt设计、越权检测的误报率分析",
    "status": "pending"
  },
  {
    "id": 472,
    "title": "LATS：蒙特卡洛树搜索驱动的Agent决策",
    "slug": "lats-monte-carlo-tree-search-agent",
    "tags": [
      "智能体",
      "自我反思",
      "LATS"
    ],
    "brief": "核心要点：LATS将LLM Agent的决策建模为树搜索问题，通过UCT选择、LLM扩展、环境模拟评估、反向传播四步实现系统性探索。在HotPotQA上LATS比ReAct提升12%、比Reflexion提升6%。分析搜索宽度、深度、模拟次数对性能与计算成本的影响。",
    "depth_hint": "需包含LATS的UCT公式与参数设定、搜索树的可视化示例、宽度-深度-性能的三维消融实验",
    "status": "pending"
  },
  {
    "id": 473,
    "title": "Agent评测中的数据污染与防泄漏机制",
    "slug": "agent-eval-data-contamination",
    "tags": [
      "智能体",
      "Agent评测",
      "数据污染"
    ],
    "brief": "核心要点：LLM训练数据可能包含评测基准的答案，导致评测分数虚高。分析SWE-bench、GAIA等基准的数据污染风险评估方法：n-gram重叠检测、membership inference攻击、canary token注入。讨论动态评测集（持续更新题目）和私有评测集的设计策略。",
    "depth_hint": "需包含数据污染检测的具体算法、各基准的污染风险等级评估、动态评测集的更新策略",
    "status": "pending"
  },
  {
    "id": 474,
    "title": "Agent的多步推理中间结果验证",
    "slug": "agent-intermediate-result-verification",
    "tags": [
      "智能体",
      "错误恢复",
      "结果验证"
    ],
    "brief": "核心要点：Agent多步推理中的中间结果错误会级联放大，需在每步插入验证点。分析三种验证方案：基于规则的格式与范围检查、基于LLM的语义合理性判定、基于工具的交叉验证。在数学推理任务上，中间步验证可将最终准确率从45%提升至67%，但每步增加约200ms延迟。",
    "depth_hint": "需包含三种验证方案的实现代码片段、验证点插入粒度的消融实验、延迟-准确率权衡数据",
    "status": "pending"
  },
  {
    "id": 475,
    "title": "Agent安全的多层防御架构设计",
    "slug": "agent-defense-in-depth-architecture",
    "tags": [
      "智能体",
      "安全边界",
      "防御架构"
    ],
    "brief": "核心要点：单点安全措施易被绕过，需构建输入过滤→意图分析→权限控制→执行隔离→输出审查的五层纵深防御。分析每层的具体实现技术与性能开销，总延迟增加约100-300ms。以真实攻击链为例演示单层失效时其他层的兜底能力，组合防御使攻击成功率降至3%以下。",
    "depth_hint": "需包含五层防御的具体技术选型、各层的延迟开销测量、多层组合的攻击成功率矩阵",
    "status": "pending"
  },
  {
    "id": 476,
    "title": "OSWorld：桌面操作Agent的跨平台评测",
    "slug": "osworld-desktop-agent-evaluation",
    "tags": [
      "智能体",
      "Agent评测",
      "OSWorld"
    ],
    "brief": "核心要点：OSWorld在真实操作系统中评测Agent执行计算机任务的能力，覆盖Ubuntu/Windows/macOS的369个任务。Agent需通过截图理解界面并生成鼠标/键盘操作。GPT-4V的成功率仅12.24%（截图模式），人类基线为72.36%。分析视觉理解与操作精度的主要瓶颈。",
    "depth_hint": "需包含OSWorld的环境配置架构、三个平台的任务分布、视觉grounding错误的分类统计",
    "status": "pending"
  },
  {
    "id": 477,
    "title": "Agent自我评估的校准度与过度自信问题",
    "slug": "agent-self-assessment-calibration",
    "tags": [
      "智能体",
      "自我反思",
      "置信度校准"
    ],
    "brief": "核心要点：Agent对自身输出的置信度估计往往过度自信——声称90%确定时实际准确率仅60-70%。分析校准度的量化方法（ECE期望校准误差）、过度自信的成因（训练目标偏差、RLHF对冲hedging的惩罚）。温度缩放、Verbalized校准、采样一致性三种校准改进方案的效果对比。",
    "depth_hint": "需包含ECE的计算公式与可靠性图、三种校准方案的ECE改进数据、校准前后的决策质量对比",
    "status": "pending"
  },
  {
    "id": 478,
    "title": "Agent的优雅降级与用户透明度设计",
    "slug": "agent-graceful-degradation-transparency",
    "tags": [
      "智能体",
      "错误恢复",
      "用户体验"
    ],
    "brief": "核心要点：Agent无法完成任务时应优雅降级而非静默失败或幻觉输出。设计三级降级策略：完整完成→部分完成并说明→明确拒绝并给出原因。分析不确定性传达的UI模式、部分结果的质量标注、人工接管的无缝切换。用户研究表明透明沟通可将满意度从32%提升至78%。",
    "depth_hint": "需包含三级降级的触发条件与输出模板、不确定性可视化的设计方案、用户满意度的A/B测试数据",
    "status": "pending"
  },
  {
    "id": 479,
    "title": "Agent评测的成本效率指标：性能与Token消耗的帕累托前沿",
    "slug": "agent-eval-cost-efficiency-pareto",
    "tags": [
      "智能体",
      "Agent评测",
      "成本效率"
    ],
    "brief": "核心要点：Agent评测不应只看成功率，还需考虑Token消耗、API调用次数、延迟。分析SWE-bench上各Agent的帕累托前沿：部分Agent以10倍token消耗仅换来5%的成功率提升。提出标准化的成本效率指标（成功率/千token），建立不同预算约束下的最优Agent选择框架。",
    "depth_hint": "需包含帕累托前沿的散点图数据、成本效率指标的形式化定义、预算约束下的Agent推荐矩阵",
    "status": "pending"
  },
  {
    "id": 480,
    "title": "FlashAttention的IO复杂度分析",
    "slug": "flashattention-io-complexity",
    "tags": [
      "底层原理",
      "FlashAttention",
      "IO感知"
    ],
    "brief": "核心要点：标准Attention的HBM读写量为Θ(Nd+N²)，FlashAttention通过分块计算将其降至Θ(N²d²M⁻¹)，其中M为SRAM大小。详解tiling策略如何将N×N注意力矩阵的显存访问从O(N²)压缩到O(N²/B_c)，B_c为块大小，受限于SRAM容量",
    "depth_hint": "需推导标准Attention与FlashAttention的FLOPs相同但IO次数差异，给出A100上SRAM=20MB时的具体分块参数",
    "status": "pending"
  },
  {
    "id": 481,
    "title": "FlashAttention的在线Softmax算法",
    "slug": "flashattention-online-softmax",
    "tags": [
      "底层原理",
      "FlashAttention",
      "数值计算"
    ],
    "brief": "核心要点：标准Softmax需两次遍历（求max和求sum），FlashAttention采用Milakov & Gimelshein的在线Softmax技巧，单次遍历完成。通过维护running max m_i和running sum l_i，在分块处理时动态修正已计算的部分结果，保证数值稳定性与精确Softmax等价",
    "depth_hint": "需给出在线Softmax的递推公式推导，展示分块间修正因子e^(m_old - m_new)的作用",
    "status": "pending"
  },
  {
    "id": 482,
    "title": "FlashAttention-3的异步与低精度优化",
    "slug": "flashattention-3-async-fp8",
    "tags": [
      "底层原理",
      "FlashAttention",
      "Hopper架构"
    ],
    "brief": "核心要点：FlashAttention-3针对H100 Hopper架构，利用wgmma异步Tensor Core指令和TMA硬件实现计算与数据传输重叠。引入FP8量化支持（E4M3/E5M2混合），在损失<0.01 perplexity的条件下将吞吐提升至1.2 PFLOPs/s，接近H100理论峰值的75%",
    "depth_hint": "需说明Hopper的异步wgmma与Ampere的同步mma的流水线差异，给出FP8 vs BF16的精度-速度权衡数据",
    "status": "pending"
  },
  {
    "id": 483,
    "title": "FlashAttention的因果Mask实现",
    "slug": "flashattention-causal-mask",
    "tags": [
      "底层原理",
      "FlashAttention",
      "Transformer"
    ],
    "brief": "核心要点：因果Mask在FlashAttention中不通过显式存储三角矩阵实现，而是在分块计算时跳过全部被Mask的block，仅对部分被Mask的边界block做行级mask。这使因果Attention的FLOPs降为非因果的约50%，而标准实现即使用mask仍需完整O(N²)计算",
    "depth_hint": "需画出block网格中因果mask的跳过模式，计算实际节省的block比例与序列长度的关系",
    "status": "pending"
  },
  {
    "id": 484,
    "title": "GPU显存层次与Attention的带宽瓶颈",
    "slug": "gpu-memory-hierarchy-attention-bottleneck",
    "tags": [
      "底层原理",
      "GPU架构",
      "IO感知"
    ],
    "brief": "核心要点：A100的HBM带宽2TB/s，SRAM带宽19TB/s，但SRAM仅20MB。标准Attention的算术强度（FLOPs/byte）仅为O(1)到O(d)，远低于GPU的ops:byte比（约200:1），导致计算单元空转等待数据。FlashAttention通过提升算术强度到O(N·d/M)使计算变为compute-bound",
    "depth_hint": "需用roofline模型分析标准Attention与FlashAttention在A100上的位置，给出具体算术强度数值",
    "status": "pending"
  },
  {
    "id": 485,
    "title": "Longformer的滑动窗口注意力机制",
    "slug": "longformer-sliding-window-attention",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "Longformer"
    ],
    "brief": "核心要点：Longformer将全局O(N²)注意力替换为滑动窗口局部注意力O(N·w)，窗口大小w=512时可处理4096+ tokens。结合扩张滑动窗口（dilation=d时感受野扩大d倍）和全局token（对特定位置如[CLS]保留全局注意力），在保持长距离依赖的同时实现线性复杂度",
    "depth_hint": "需推导滑动窗口+扩张的等效感受野计算，对比不同层使用不同窗口大小的层级策略",
    "status": "pending"
  },
  {
    "id": 486,
    "title": "BigBird的随机稀疏注意力理论基础",
    "slug": "bigbird-random-sparse-attention",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "BigBird"
    ],
    "brief": "核心要点：BigBird的注意力由三部分组成：随机注意力（每个token随机关注r个token）、窗口注意力（局部w个邻居）、全局注意力（g个全局token）。论文证明当r·w≥Ω(N)时，该稀疏图是O(1)直径的expander graph，保证信息O(1)跳可达任意位置，是完整Transformer的通用近似器",
    "depth_hint": "需说明expander graph性质如何保证稀疏注意力的表达能力，给出r、w、g的典型取值",
    "status": "pending"
  },
  {
    "id": 487,
    "title": "Longformer与BigBird的稀疏模式对比",
    "slug": "longformer-bigbird-sparse-pattern-comparison",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "模型对比"
    ],
    "brief": "核心要点：Longformer使用确定性稀疏模式（滑动窗口+扩张+全局token），BigBird额外引入随机连接并提供理论保证。实验显示两者在长文档任务上性能接近（±0.5 F1），但BigBird的随机连接在训练时增加实现复杂度，Longformer的全局token策略更易于工程部署",
    "depth_hint": "需用表格对比两者在TriviaQA、WikiHop等长文档基准上的具体指标差异",
    "status": "pending"
  },
  {
    "id": 488,
    "title": "稀疏注意力的CUDA kernel实现挑战",
    "slug": "sparse-attention-cuda-kernel-challenges",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "CUDA"
    ],
    "brief": "核心要点：稀疏注意力的理论复杂度为O(N·k)但实际加速受限于GPU的不规则访存。block-sparse实现（如OpenAI的block_size=64策略）通过将稀疏模式对齐到Tensor Core的block粒度，避免warp divergence。实测在N<2048时稀疏kernel反而慢于dense FlashAttention",
    "depth_hint": "需分析block-sparse矩阵乘法的coalesced memory access条件，给出不同序列长度下的break-even点",
    "status": "pending"
  },
  {
    "id": 489,
    "title": "Performer的随机特征映射近似Softmax",
    "slug": "performer-random-feature-softmax-approximation",
    "tags": [
      "底层原理",
      "线性注意力",
      "Performer"
    ],
    "brief": "核心要点：Performer用随机特征映射φ(x)将softmax核近似为φ(Q)·φ(K)ᵀ，其中φ使用FAVOR+算法：φ(x)=exp(ωᵀx - ||x||²/2)/√m，ω采样自N(0,I)。这允许利用矩阵乘法结合律将Attention从O(N²d)降至O(Nmd)，m为特征维度（通常m=O(d·log d)）",
    "depth_hint": "需推导FAVOR+的无偏性证明和方差界，说明正随机特征(positive random features)为何优于三角函数特征",
    "status": "pending"
  },
  {
    "id": 490,
    "title": "线性注意力的因果约束处理",
    "slug": "linear-attention-causal-constraint",
    "tags": [
      "底层原理",
      "线性注意力",
      "因果推理"
    ],
    "brief": "核心要点：线性注意力通过结合律实现O(Nd²)复杂度：O_i = Σ_{j≤i} φ(Q_i)φ(K_j)ᵀV_j = φ(Q_i)·S_i，其中S_i = S_{i-1} + φ(K_i)⊗V_i为累积状态矩阵（d×d）。因果约束自然适配递推形式，使线性注意力等价于RNN，推理时O(1)复杂度per token",
    "depth_hint": "需写出从标准Attention到线性Attention再到RNN形式的完整推导链",
    "status": "pending"
  },
  {
    "id": 491,
    "title": "Mamba的选择性状态空间机制",
    "slug": "mamba-selective-state-space",
    "tags": [
      "底层原理",
      "Mamba",
      "状态空间模型"
    ],
    "brief": "核心要点：Mamba（S6）的核心创新是让SSM参数Δ、B、C依赖于输入x，即Δ=softplus(Linear(x))。选择性机制使模型能根据内容动态决定遗忘/记忆，Δ大时遗忘历史聚焦当前输入，Δ小时保持长期记忆。这打破了线性时不变假设，代价是无法使用FFT卷积加速",
    "depth_hint": "需推导Δ对离散化矩阵A̅=exp(ΔA)的影响，说明选择性如何实现内容感知的记忆门控",
    "status": "pending"
  },
  {
    "id": 492,
    "title": "Mamba的硬件感知扫描算法",
    "slug": "mamba-hardware-aware-scan",
    "tags": [
      "底层原理",
      "Mamba",
      "GPU优化"
    ],
    "brief": "核心要点：Mamba的选择性SSM无法用FFT加速，改用parallel scan算法实现O(N)工作量和O(log N)并行深度。关键优化：1)将Δ、B、C的投影和离散化融合为单个kernel；2)状态矩阵d_state×d_model不存入HBM，全程在SRAM中计算；3)反向传播时重计算避免存储中间状态",
    "depth_hint": "需说明parallel scan的work-depth分析，对比naive sequential scan与parallel scan在GPU上的实际吞吐差异",
    "status": "pending"
  },
  {
    "id": 493,
    "title": "S4模型的HiPPO矩阵初始化",
    "slug": "s4-hippo-matrix-initialization",
    "tags": [
      "底层原理",
      "状态空间模型",
      "HiPPO"
    ],
    "brief": "核心要点：S4的核心是状态矩阵A的初始化策略HiPPO（High-order Polynomial Projection Operators）。HiPPO-LegS矩阵A_{nk}=-(2n+1)^{1/2}(2k+1)^{1/2}（n>k时），使隐状态最优地压缩历史信息为Legendre多项式系数，理论上实现O(N)记忆长度无衰减",
    "depth_hint": "需推导HiPPO-LegS矩阵的在线逼近性质，说明为何Legendre基函数是最优选择",
    "status": "pending"
  },
  {
    "id": 494,
    "title": "状态空间模型的连续-离散对偶性",
    "slug": "ssm-continuous-discrete-duality",
    "tags": [
      "底层原理",
      "状态空间模型",
      "数学基础"
    ],
    "brief": "核心要点：SSM的连续形式dx/dt=Ax+Bu通过零阶保持(ZOH)离散化得到x_k=A̅x_{k-1}+B̅u_k，其中A̅=exp(ΔA), B̅=(exp(ΔA)-I)A⁻¹B。离散SSM等价于卷积y=K̄*u（K̄_i=C̅A̅ⁱB̅），训练时用FFT做O(N log N)卷积，推理时用递推做O(1)/step",
    "depth_hint": "需完整推导ZOH离散化过程，对比双线性变换(Bilinear/Tustin)的差异与各自优劣",
    "status": "pending"
  },
  {
    "id": 495,
    "title": "Mamba-2的结构化状态空间对偶性",
    "slug": "mamba-2-ssd-duality",
    "tags": [
      "底层原理",
      "Mamba",
      "SSD"
    ],
    "brief": "核心要点：Mamba-2揭示了SSM与注意力的结构对偶：当SSM的状态转移矩阵A为标量乘以单位矩阵时，SSM的输入-输出映射等价于一个半可分(semiseparable)矩阵乘法，形式上与因果线性注意力一致。SSD算法利用此对偶，在chunk内用矩阵乘法、chunk间用递推，实现8倍于Mamba-1的速度",
    "depth_hint": "需推导半可分矩阵的分块乘法公式，说明chunk_size的选择如何平衡计算效率与并行度",
    "status": "pending"
  },
  {
    "id": 496,
    "title": "GLA：门控线性注意力的衰减机制",
    "slug": "gated-linear-attention-decay",
    "tags": [
      "底层原理",
      "线性注意力",
      "GLA"
    ],
    "brief": "核心要点：GLA（Gated Linear Attention）在线性注意力的递推S_t = G_t⊙S_{t-1} + K_tᵀV_t中引入逐元素门控矩阵G_t∈(0,1)^{d×d}，替代标量遗忘门。二维门控允许选择性遗忘状态矩阵的不同维度，比Mamba的标量Δ控制更精细，同时保持O(Nd²)的线性复杂度",
    "depth_hint": "需对比GLA、RetNet、Mamba三者的遗忘机制差异，给出在MQAR任务上的recall准确率对比",
    "status": "pending"
  },
  {
    "id": 497,
    "title": "RetNet的多尺度指数衰减与chunk递推",
    "slug": "retnet-multiscale-exponential-decay",
    "tags": [
      "底层原理",
      "线性注意力",
      "RetNet"
    ],
    "brief": "核心要点：RetNet将线性注意力的递推引入指数衰减因子γ∈(0,1)：S_n = γ·S_{n-1} + K_nᵀV_n，不同注意力头使用不同γ值（多尺度）实现短/长程信息的分层捕获。支持三种等价计算模式：并行O(N²)训练、chunk-wise O(C²)混合、递推O(1)推理",
    "depth_hint": "需推导三种模式的等价性，说明多尺度γ的设置策略（如γ=1-2^{-k}）",
    "status": "pending"
  },
  {
    "id": 498,
    "title": "线性注意力的表达能力理论界限",
    "slug": "linear-attention-expressiveness-bounds",
    "tags": [
      "底层原理",
      "线性注意力",
      "理论分析"
    ],
    "brief": "核心要点：线性注意力用有限维特征映射近似softmax核，存在本质精度限制。softmax注意力可区分的token对，线性注意力在特征维度m<d²时无法区分（MQAR任务上m需≥O(Nd)才能完美recall）。状态矩阵S∈R^{d×d}的容量上界为d²，限制了可存储的KV对数量",
    "depth_hint": "需给出MQAR（Multi-Query Associative Recall）任务的形式定义，推导状态容量与recall性能的理论关系",
    "status": "pending"
  },
  {
    "id": 499,
    "title": "FlashAttention的多头注意力并行策略",
    "slug": "flashattention-multi-head-parallelism",
    "tags": [
      "底层原理",
      "FlashAttention",
      "并行计算"
    ],
    "brief": "核心要点：FlashAttention在batch×head维度做外层并行，序列维度做内层分块。当batch×head数不足以填满GPU的108个SM时（如长序列+少头），利用率下降。FlashAttention-2引入序列维度的并行分割，将thread block数从batch×head增加到batch×head×⌈N/B_r⌉，显著提升长序列场景的SM占用率",
    "depth_hint": "需计算不同batch/head/seq配置下的SM利用率，说明为什么在Q维度而非K维度做并行更优",
    "status": "pending"
  },
  {
    "id": 500,
    "title": "PagedAttention与KV Cache分页管理",
    "slug": "paged-attention-kv-cache",
    "tags": [
      "底层原理",
      "推理优化",
      "显存管理"
    ],
    "brief": "核心要点：LLM推理的KV Cache按序列连续分配导致严重内存碎片（浪费60-80%显存）。PagedAttention借鉴OS虚拟内存，将KV Cache分为固定大小的block（如16 tokens），通过block table实现非连续存储。碎片率从60%降至<4%，batch size可提升2-4倍，直接转化为吞吐提升",
    "depth_hint": "需画出PagedAttention的block table映射示意，说明copy-on-write如何支持beam search的KV共享",
    "status": "pending"
  },
  {
    "id": 501,
    "title": "Multi-Query与Grouped-Query Attention的KV压缩",
    "slug": "mqa-gqa-kv-compression",
    "tags": [
      "底层原理",
      "注意力变体",
      "推理优化"
    ],
    "brief": "核心要点：MQA让所有head共享一组KV（KV Cache缩小h倍），GQA将h个head分为g组共享KV（缩小h/g倍）。GQA在g=8时性能接近MHA（差<0.5% on LM eval），但推理速度接近MQA。LLaMA-2 70B使用GQA g=8，KV Cache从每层256MB降至32MB",
    "depth_hint": "需量化不同g值对模型质量和推理延迟的影响曲线，给出从MHA到GQA的权重转换方法（均值池化）",
    "status": "pending"
  },
  {
    "id": 502,
    "title": "Sliding Window Attention在Mistral中的实现",
    "slug": "mistral-sliding-window-attention",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "Mistral"
    ],
    "brief": "核心要点：Mistral-7B使用固定窗口W=4096的滑动窗口注意力，但通过L层堆叠获得L×W=32×4096=131K的理论感受野。Rolling Buffer KV Cache固定大小为W，位置i的KV存储在cache[i mod W]，实现O(W)固定显存。配合Flash实现，预填充阶段支持超过W的序列长度",
    "depth_hint": "需推导L层堆叠后的感受野计算，说明Rolling Buffer的缓存替换策略与位置编码的兼容性",
    "status": "pending"
  },
  {
    "id": 503,
    "title": "稀疏注意力的动态模式学习",
    "slug": "learnable-sparse-attention-patterns",
    "tags": [
      "底层原理",
      "稀疏注意力",
      "自适应"
    ],
    "brief": "核心要点：固定稀疏模式（如Longformer）无法适应不同输入的注意力分布。动态稀疏方法通过轻量级评分网络预测每个token应关注的top-k位置：先用低秩投影计算近似注意力分数O(Nr)（r<<N），再对top-k位置做精确注意力。Reformer用LSH哈希实现O(N log N)的近似top-k",
    "depth_hint": "需对比LSH Attention（Reformer）、Routing Transformer、Sinkhorn排序三种动态路由策略",
    "status": "pending"
  },
  {
    "id": 504,
    "title": "Linear Transformer与核函数选择",
    "slug": "linear-transformer-kernel-functions",
    "tags": [
      "底层原理",
      "线性注意力",
      "核方法"
    ],
    "brief": "核心要点：Linear Transformer将Attention解释为核函数k(Q,K)=φ(Q)ᵀφ(K)，不同φ对应不同核。ELU+1核φ(x)=elu(x)+1简单但近似质量差；随机傅里叶特征φ实现高斯核但需高维m；cosFormer用cos(πi/2N)加权实现位置感知的线性注意力，无需显式核近似",
    "depth_hint": "需推导不同核函数的近似误差界，实验对比各核在WikiText-103上的perplexity差异",
    "status": "pending"
  },
  {
    "id": 505,
    "title": "RWKV的线性复杂度语言模型设计",
    "slug": "rwkv-linear-complexity-language-model",
    "tags": [
      "底层原理",
      "线性注意力",
      "RWKV"
    ],
    "brief": "核心要点：RWKV结合Transformer的并行训练和RNN的O(1)推理。WKV算子定义为wkv_t = Σ_{i=1}^{t-1} e^{-(t-1-i)w+k_i}·v_i / Σ e^{...}，其中w为通道级衰减率。该求和可递推计算：分子a_t=e^{-w}·a_{t-1}+e^{k_t}·v_t，实现线性复杂度。RWKV-6引入data-dependent衰减提升表达力",
    "depth_hint": "需推导WKV算子的递推公式，说明数值稳定的log-space实现技巧",
    "status": "pending"
  },
  {
    "id": 506,
    "title": "注意力机制的统一视角：从二次到线性",
    "slug": "attention-unified-view-quadratic-to-linear",
    "tags": [
      "底层原理",
      "注意力机制",
      "综述"
    ],
    "brief": "核心要点：所有注意力变体可统一为Y=f(Q,K)·V的框架：标准Attention中f=softmax(QKᵀ/√d)是O(N²)；稀疏注意力中f保留top-k或固定模式；线性注意力中f=φ(Q)φ(K)ᵀ利用结合律变为O(Nd²)；SSM中f等价于半可分矩阵（Mamba-2证明）。关键权衡轴：表达力 vs 效率 vs 实现复杂度",
    "depth_hint": "需构建统一形式化框架，用表格对比8+种变体在复杂度、训练速度、推理速度、长程性能上的数据",
    "status": "pending"
  },
  {
    "id": 507,
    "title": "MoE路由机制的数学形式化",
    "slug": "moe-routing-formalization",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：将MoE路由抽象为Top-K稀疏门控函数G(x)=TopK(softmax(Wx))，分析门控网络的梯度流特性，推导专家选择概率分布与模型容量的关系，解释为何softmax温度τ影响专家利用率",
    "depth_hint": "需推导Top-K门控的梯度近似公式，给出不同K值下的专家激活率实验数据",
    "status": "pending"
  },
  {
    "id": 508,
    "title": "Switch Transformer的简化路由设计",
    "slug": "switch-transformer-routing",
    "tags": [
      "底层原理",
      "MoE",
      "Switch Transformer"
    ],
    "brief": "核心要点：Switch Transformer将Top-2简化为Top-1路由，单专家选择使计算量降低近50%。分析其capacity factor C的设定（C=1.0~1.5），溢出token的丢弃策略，以及为何Top-1在大规模预训练中反而优于Top-2",
    "depth_hint": "需对比Top-1与Top-2在相同FLOPs下的loss曲线，给出capacity factor的敏感性实验",
    "status": "pending"
  },
  {
    "id": 509,
    "title": "GShard的随机Top-2路由策略",
    "slug": "gshard-random-top2-routing",
    "tags": [
      "底层原理",
      "MoE",
      "GShard"
    ],
    "brief": "核心要点：GShard对第二专家采用概率采样而非确定性选择，路由公式为Top-1确定+第二专家按softmax概率采样。分析这种随机性如何同时提升探索性和负载均衡，以及与纯Top-2的效果差异",
    "depth_hint": "需给出GShard路由的伪代码实现，对比确定性Top-2的专家负载方差数据",
    "status": "pending"
  },
  {
    "id": 510,
    "title": "MoE的容量因子与Token溢出处理",
    "slug": "moe-capacity-factor-overflow",
    "tags": [
      "底层原理",
      "MoE",
      "系统设计"
    ],
    "brief": "核心要点：容量因子C定义每个专家的缓冲区大小为C·(N/E)，其中N为token数、E为专家数。C过小导致token丢弃影响质量，C过大浪费显存。分析溢出token的三种处理策略：丢弃、随机重路由、辅助专家兜底",
    "depth_hint": "需给出不同C值下的token丢弃率与模型perplexity关系表，分析显存占用公式",
    "status": "pending"
  },
  {
    "id": 511,
    "title": "Hash Layer：无参数路由的MoE",
    "slug": "hash-layer-moe-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Hash Layer使用固定哈希函数将token映射到专家，完全消除可学习路由的训练不稳定性。分析随机哈希、基于token-id哈希、基于位置哈希的不同策略，以及令人意外的结论：简单哈希路由在某些设置下接近learned routing性能",
    "depth_hint": "需对比Hash Layer与learned routing在不同模型规模下的perplexity差异数据",
    "status": "pending"
  },
  {
    "id": 512,
    "title": "MoE的专家并行与通信开销分析",
    "slug": "moe-expert-parallelism-communication",
    "tags": [
      "底层原理",
      "MoE",
      "分布式训练"
    ],
    "brief": "核心要点：专家并行将不同专家放在不同设备，需要All-to-All通信在路由前后交换token。分析All-to-All的通信量O(B·S·d/P)、与数据并行的正交性、以及专家并行度与模型并行度的组合策略",
    "depth_hint": "需给出All-to-All通信量的理论公式，分析不同并行度配置下的通信/计算比",
    "status": "pending"
  },
  {
    "id": 513,
    "title": "Switch Transformer的混合精度训练策略",
    "slug": "switch-transformer-mixed-precision",
    "tags": [
      "底层原理",
      "MoE",
      "训练优化"
    ],
    "brief": "核心要点：Switch Transformer发现MoE路由的softmax和Top-K操作对精度敏感，需在float32下执行，而专家FFN可用bfloat16。分析选择性精度策略如何将训练不稳定性降低80%，以及router z-loss的数值稳定化作用",
    "depth_hint": "需给出router z-loss的公式L_z=Σlog²(Σexp(x_i))，对比有无z-loss的训练loss曲线",
    "status": "pending"
  },
  {
    "id": 514,
    "title": "Mixtral的Sliding Window与MoE协同设计",
    "slug": "mixtral-sliding-window-moe-synergy",
    "tags": [
      "底层原理",
      "MoE",
      "Mixtral"
    ],
    "brief": "核心要点：Mixtral结合4096窗口的滑动窗口注意力与MoE FFN，分析两者的互补性：SWA降低Attention的计算复杂度，MoE降低FFN的激活参数量。推导组合后的总FLOPs公式与等效dense模型的对比",
    "depth_hint": "需给出Mixtral单层前向传播的FLOPs分解，对比标准Transformer的计算量",
    "status": "pending"
  },
  {
    "id": 515,
    "title": "MoE路由中的噪声注入机制",
    "slug": "moe-noisy-gating",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Noisy Top-K Gating在路由logits上添加可调噪声H(x)=Wx+softplus(W_noise·x)·ε，ε~N(0,1)。分析噪声如何促进专家探索、防止路由坍缩，以及噪声幅度随训练进程的自适应调节机制",
    "depth_hint": "需推导噪声注入对路由梯度方差的影响，给出noise coefficient的消融实验数据",
    "status": "pending"
  },
  {
    "id": 516,
    "title": "MoE模型的推理效率优化",
    "slug": "moe-inference-optimization",
    "tags": [
      "底层原理",
      "MoE",
      "推理优化"
    ],
    "brief": "核心要点：MoE推理面临专家参数加载的内存瓶颈，batch内不同token路由到不同专家导致计算不规则。分析专家预加载预测、专家缓存策略、以及动态batch重组三种优化方案的延迟与吞吐量提升",
    "depth_hint": "需给出专家缓存命中率与推理延迟的关系曲线，分析不同batch size的影响",
    "status": "pending"
  },
  {
    "id": 517,
    "title": "ST-MoE的路由稳定性与Z-Loss",
    "slug": "st-moe-router-z-loss",
    "tags": [
      "底层原理",
      "MoE",
      "训练稳定性"
    ],
    "brief": "核心要点：ST-MoE（Stable and Transferable MoE）系统研究了MoE训练不稳定的根因：路由logits的数值爆炸。提出Router Z-Loss惩罚logits的对数指数和，将训练不稳定发生率从~30%降至<1%，且不损害模型质量",
    "depth_hint": "需给出Z-Loss系数的最优区间实验，分析logits范数与训练崩溃的相关性",
    "status": "pending"
  },
  {
    "id": 518,
    "title": "MoE与Dense模型的Scaling Law对比",
    "slug": "moe-vs-dense-scaling-laws",
    "tags": [
      "底层原理",
      "MoE",
      "Scaling Law"
    ],
    "brief": "核心要点：MoE在相同计算预算下通常优于dense模型，但优势随规模增长的趋势存在争议。分析Unified Scaling Law的结论：MoE的有效参数利用率约为E^0.3（E为专家数），以及专家数存在收益递减拐点",
    "depth_hint": "需给出MoE与dense模型在不同FLOPs预算下的loss对比曲线，分析最优专家数公式",
    "status": "pending"
  },
  {
    "id": 519,
    "title": "Mixtral的专家权重合并与蒸馏",
    "slug": "mixtral-expert-merging-distillation",
    "tags": [
      "底层原理",
      "MoE",
      "模型压缩"
    ],
    "brief": "核心要点：将Mixtral的8个专家合并或蒸馏为更少专家以降低部署成本。分析基于路由统计的专家聚类合并、基于任务相关性的专家剪枝、以及将MoE蒸馏为dense模型的知识迁移策略",
    "depth_hint": "需给出专家合并前后的参数量与性能对照表，分析不同合并策略的质量损失",
    "status": "pending"
  },
  {
    "id": 520,
    "title": "MoE在Attention层的应用：Multi-Head MoE",
    "slug": "moe-attention-multi-head-moe",
    "tags": [
      "底层原理",
      "MoE",
      "注意力机制"
    ],
    "brief": "核心要点：传统MoE仅用于FFN层，Multi-Head MoE将路由机制扩展到注意力头选择。每个token只激活部分注意力头，分析这种设计如何在不损失多头多样性的前提下减少Attention层的计算量",
    "depth_hint": "需对比FFN-only MoE与Full MoE（FFN+Attention）在计算量和性能上的差异",
    "status": "pending"
  },
  {
    "id": 521,
    "title": "V-MoE：视觉Transformer中的MoE设计",
    "slug": "vmoe-vision-transformer-moe",
    "tags": [
      "底层原理",
      "MoE",
      "视觉模型"
    ],
    "brief": "核心要点：V-MoE将MoE引入ViT，发现视觉任务中路由表现出空间局部性：相邻patch倾向于选择相同专家。分析视觉MoE与语言MoE在路由模式上的本质差异，以及Priority Routing的batch级负载均衡策略",
    "depth_hint": "需给出V-MoE路由的空间可视化图，对比语言模型中的路由分布模式",
    "status": "pending"
  },
  {
    "id": 522,
    "title": "MoE的梯度估计与直通估计器",
    "slug": "moe-gradient-estimation-ste",
    "tags": [
      "底层原理",
      "MoE",
      "优化理论"
    ],
    "brief": "核心要点：Top-K操作的离散性导致梯度不连续，分析三种梯度估计方案：直通估计器（STE）直接传递梯度、Gumbel-Softmax的连续松弛、以及REINFORCE策略梯度。推导各方案的梯度偏差与方差特性",
    "depth_hint": "需推导Gumbel-Softmax在温度τ→0时趋近argmax的数学证明，给出梯度方差对比",
    "status": "pending"
  },
  {
    "id": 523,
    "title": "Sparse Upcycling：从Dense到MoE的转换",
    "slug": "sparse-upcycling-dense-to-moe",
    "tags": [
      "底层原理",
      "MoE",
      "训练策略"
    ],
    "brief": "核心要点：Sparse Upcycling将预训练好的dense模型转换为MoE模型继续训练，避免从零训练MoE的成本。分析FFN层复制初始化策略、路由网络的warm-up设计、以及转换后的训练dynamics变化",
    "depth_hint": "需给出Upcycling vs 从零训练MoE在相同总FLOPs下的性能对比曲线",
    "status": "pending"
  },
  {
    "id": 524,
    "title": "MoE的Token级与序列级路由对比",
    "slug": "moe-token-vs-sequence-routing",
    "tags": [
      "底层原理",
      "MoE",
      "路由机制"
    ],
    "brief": "核心要点：Token级路由（每个token独立选专家）是主流但引入计算不规则性；序列级路由（整个序列选一组专家）更高效但丧失细粒度。分析sentence-level routing和task-level routing的设计，以及两者的适用场景",
    "depth_hint": "需对比token级与序列级路由在不同任务（翻译、摘要、QA）上的性能差异",
    "status": "pending"
  },
  {
    "id": 525,
    "title": "DeepSeekMoE的共享专家机制",
    "slug": "deepseek-moe-shared-experts",
    "tags": [
      "底层原理",
      "MoE",
      "DeepSeek"
    ],
    "brief": "核心要点：DeepSeekMoE设计1-2个始终激活的共享专家，捕获所有token的通用特征，其余路由专家负责专业化特征。分析共享专家如何缓解专家冗余、提升参数效率，以及共享比例对性能的影响",
    "depth_hint": "需给出共享专家数量的消融实验数据，分析共享vs路由专家的表示差异",
    "status": "pending"
  },
  {
    "id": 526,
    "title": "MoE的专家容量与内存墙问题",
    "slug": "moe-expert-capacity-memory-wall",
    "tags": [
      "底层原理",
      "MoE",
      "系统优化"
    ],
    "brief": "核心要点：MoE总参数量大但激活参数少，核心瓶颈从计算转移到内存带宽。分析专家参数的HBM占用、推理时的内存带宽利用率（通常<30%）、以及专家offloading到CPU/NVMe的分层存储策略",
    "depth_hint": "需给出Mixtral 8x7B的内存占用分解表，分析不同offloading策略的延迟影响",
    "status": "pending"
  },
  {
    "id": 527,
    "title": "Mixture-of-Depths：动态计算的层级跳过",
    "slug": "mixture-of-depths-layer-skipping",
    "tags": [
      "底层原理",
      "MoE",
      "动态计算"
    ],
    "brief": "核心要点：Mixture-of-Depths将MoE的路由思想扩展到层级：每个token由路由器决定是否跳过当前Transformer层。分析与MoE的设计对偶性、容量因子在层跳过中的复用、以及FLOPs节省与性能损失的Pareto曲线",
    "depth_hint": "需给出不同跳过比例下的FLOPs节省率与模型质量关系，分析各层被跳过的频率分布",
    "status": "pending"
  },
  {
    "id": 528,
    "title": "Tensor Parallel的列并行与行并行切分",
    "slug": "tensor-parallel-column-row-split",
    "tags": [
      "底层原理",
      "模型并行",
      "分布式训练"
    ],
    "brief": "核心要点：Megatron-LM中MLP层的列并行（Column Parallel）将权重按列切分到多GPU，前向无需通信；行并行（Row Parallel）按行切分，前向需AllReduce。两者组合使MLP层仅需一次AllReduce，通信量为O(b·s·h/t)，其中t为并行度。",
    "depth_hint": "推导列并行与行并行的矩阵分块公式，给出通信量计算，对比naive切分的通信开销",
    "status": "pending"
  },
  {
    "id": 529,
    "title": "自注意力层的Tensor Parallel实现",
    "slug": "tensor-parallel-self-attention",
    "tags": [
      "底层原理",
      "模型并行",
      "注意力机制"
    ],
    "brief": "核心要点：多头注意力天然适合Tensor Parallel——将注意力头均匀分配到不同GPU，每个GPU独立计算QKV投影和注意力，仅在输出投影后做一次AllReduce。对于h=96头、t=8的配置，每GPU处理12个头，通信量与隐藏维度成正比。",
    "depth_hint": "推导QKV矩阵的切分方式，分析头数不整除并行度时的处理策略，给出通信拓扑图",
    "status": "pending"
  },
  {
    "id": 530,
    "title": "流水线并行的微批次调度策略",
    "slug": "pipeline-parallel-microbatch-scheduling",
    "tags": [
      "底层原理",
      "流水线并行",
      "分布式训练"
    ],
    "brief": "核心要点：朴素流水线并行的bubble率为(p-1)/m（p为流水线级数，m为微批次数）。GPipe采用同步调度，将mini-batch切分为m个micro-batch串行执行；当m≥4p时bubble率降至25%以下。1F1B调度进一步降低峰值内存至O(1/p)。",
    "depth_hint": "画出GPipe和1F1B的时序调度图，推导bubble率公式，给出不同m/p比值下的效率对比",
    "status": "pending"
  },
  {
    "id": 531,
    "title": "GPipe的梯度累积与同步机制",
    "slug": "gpipe-gradient-accumulation-sync",
    "tags": [
      "底层原理",
      "流水线并行",
      "GPipe"
    ],
    "brief": "核心要点：GPipe将模型按层切分到K个加速器，mini-batch切分为M个micro-batch。前向阶段顺序执行所有micro-batch，反向阶段逆序执行，梯度在所有micro-batch上累积后同步更新。re-materialization策略以计算换内存，峰值激活内存降至O(N/K)。",
    "depth_hint": "推导梯度累积的数学等价性，分析re-materialization的计算开销比，给出实际训练吞吐量数据",
    "status": "pending"
  },
  {
    "id": 532,
    "title": "PipeDream的1F1B异步调度",
    "slug": "pipedream-1f1b-async-scheduling",
    "tags": [
      "底层原理",
      "流水线并行",
      "PipeDream"
    ],
    "brief": "核心要点：PipeDream的1F1B（one-forward-one-backward）调度在稳态阶段交替执行前向和反向传播，将内存占用从O(m)降至O(p)。权重版本管理通过weight stashing保持梯度一致性，每个stage最多保存p份权重副本。PipeDream-2BW进一步将权重副本减少到2份。",
    "depth_hint": "绘制1F1B调度时序图，推导内存占用公式，分析weight stashing对收敛性的影响",
    "status": "pending"
  },
  {
    "id": 533,
    "title": "Megatron-LM的混合并行架构",
    "slug": "megatron-lm-hybrid-parallelism",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "混合并行"
    ],
    "brief": "核心要点：Megatron-LM结合数据并行（DP）、张量并行（TP）和流水线并行（PP），三维并行度d×t×p=总GPU数。TP限制在单机内（NVLink高带宽），PP跨机（容忍较高延迟），DP在TP和PP之上扩展。530B模型使用TP=8、PP=35、DP=若干的配置。",
    "depth_hint": "给出三维并行的通信量分析，推导最优并行度配置的约束方程，附实际训练配置表",
    "status": "pending"
  },
  {
    "id": 534,
    "title": "ZeRO Stage 1：优化器状态分片",
    "slug": "zero-stage1-optimizer-state-partition",
    "tags": [
      "底层原理",
      "ZeRO",
      "内存优化"
    ],
    "brief": "核心要点：混合精度训练中，Adam优化器状态（fp32参数副本、一阶动量、二阶动量）占模型参数内存的12倍（12Φ字节）。ZeRO-1将优化器状态均匀分片到N个GPU，每GPU仅存12Φ/N字节，内存降至4Φ+12Φ/N。通信量与标准数据并行相同。",
    "depth_hint": "推导混合精度训练的完整内存占用公式，分析ZeRO-1的AllReduce通信模式",
    "status": "pending"
  },
  {
    "id": 535,
    "title": "ZeRO-Offload：GPU-CPU混合训练",
    "slug": "zero-offload-gpu-cpu-hybrid",
    "tags": [
      "底层原理",
      "ZeRO",
      "异构计算"
    ],
    "brief": "核心要点：ZeRO-Offload将优化器状态和梯度计算卸载到CPU内存和CPU计算。数据流图分析表明，将fp32参数更新放在CPU上仅增加O(M)的CPU-GPU通信（M为参数量），而CPU Adam更新可与GPU计算重叠。单V100可训练10B参数模型。",
    "depth_hint": "画出GPU-CPU数据流图，分析PCIe带宽瓶颈，给出不同模型规模下的吞吐量对比",
    "status": "pending"
  },
  {
    "id": 536,
    "title": "ZeRO-Infinity：NVMe存储扩展训练",
    "slug": "zero-infinity-nvme-offload",
    "tags": [
      "底层原理",
      "ZeRO",
      "存储层级"
    ],
    "brief": "核心要点：ZeRO-Infinity将参数、梯度和优化器状态卸载到NVMe SSD，利用infinity offload engine实现GPU-CPU-NVMe三级存储的高效数据搬运。通过预取和分块传输隐藏I/O延迟，单DGX-2节点（16×V100）可训练超过1万亿参数的模型。",
    "depth_hint": "分析NVMe带宽利用率，推导预取窗口大小的计算公式，给出实测训练吞吐量",
    "status": "pending"
  },
  {
    "id": 537,
    "title": "Megatron-LM的序列并行优化",
    "slug": "megatron-sequence-parallelism",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "序列并行"
    ],
    "brief": "核心要点：Megatron-LM的序列并行将LayerNorm和Dropout沿序列维度切分到不同GPU，解决TP中这些操作的激活冗余。通过将AllReduce拆分为Reduce-Scatter+AllGather，并在LayerNorm/Dropout处切换到序列并行，总激活内存降至原来的1/t。",
    "depth_hint": "画出TP与SP切换的数据流图，推导激活内存节省量，分析额外通信开销",
    "status": "pending"
  },
  {
    "id": 538,
    "title": "虚拟流水线并行的交错调度",
    "slug": "virtual-pipeline-interleaved-schedule",
    "tags": [
      "底层原理",
      "流水线并行",
      "Megatron-LM"
    ],
    "brief": "核心要点：Megatron-LM的交错调度将每个GPU分配多个非连续的模型层块（虚拟stage），使bubble率从(p-1)/m降至(p-1)/(v·m)（v为虚拟stage数）。代价是通信量增加v倍，但在高带宽互联下，bubble减少带来的收益大于通信增加的开销。",
    "depth_hint": "推导交错调度的bubble率公式，绘制v=2和v=4的调度时序图，给出通信-计算权衡分析",
    "status": "pending"
  },
  {
    "id": 539,
    "title": "Tensor Parallel的通信原语分析",
    "slug": "tensor-parallel-communication-primitives",
    "tags": [
      "底层原理",
      "模型并行",
      "集合通信"
    ],
    "brief": "核心要点：TP中核心通信原语为AllReduce（前向聚合）和AllReduce（反向聚合），可分解为Reduce-Scatter+AllGather。在Ring AllReduce中，通信量为2(t-1)/t×M（M为张量大小）。NVLink提供600GB/s带宽，使TP在8GPU内保持90%以上的计算效率。",
    "depth_hint": "推导Ring AllReduce的通信步骤和带宽利用率，对比Tree AllReduce，给出不同拓扑下的延迟模型",
    "status": "pending"
  },
  {
    "id": 540,
    "title": "流水线并行的负载均衡策略",
    "slug": "pipeline-parallel-load-balancing",
    "tags": [
      "底层原理",
      "流水线并行",
      "性能优化"
    ],
    "brief": "核心要点：Transformer各层计算量大致相同，但Embedding层和LM Head层的计算量和内存占用不同。Megatron-LM将Embedding与第一个Transformer块共置，LM Head与最后一个块共置。不均衡的stage划分使最慢stage决定整体吞吐，需通过profiling调整层分配。",
    "depth_hint": "分析各层的FLOPs和内存占用差异，给出自动化层划分算法的伪代码",
    "status": "pending"
  },
  {
    "id": 541,
    "title": "ZeRO与模型并行的对比与互补",
    "slug": "zero-vs-model-parallelism-comparison",
    "tags": [
      "底层原理",
      "ZeRO",
      "模型并行"
    ],
    "brief": "核心要点：ZeRO-3在数学上等价于模型并行但保持数据并行的通信模式。ZeRO适合跨机扩展（仅需AllGather/Reduce-Scatter），TP适合机内高带宽互联。两者可组合：机内TP+跨机ZeRO-DP，兼顾通信效率和内存扩展。",
    "depth_hint": "推导ZeRO-3与TP的通信量对比表，分析不同网络拓扑下的最优选择",
    "status": "pending"
  },
  {
    "id": 542,
    "title": "Megatron-LM的分布式Embedding实现",
    "slug": "megatron-distributed-embedding",
    "tags": [
      "底层原理",
      "Megatron-LM",
      "模型并行"
    ],
    "brief": "核心要点：词表Embedding在TP中按词表维度切分，每GPU存储V/t行的Embedding矩阵。前向通过AllReduce收集完整输出，反向仅更新本地分片。当词表V=256000、隐藏维度h=12288时，Embedding参数约3.1B，TP=8使每GPU仅需存储390M参数。",
    "depth_hint": "推导Embedding层的TP切分和通信公式，分析与输出层权重绑定时的处理方式",
    "status": "pending"
  },
  {
    "id": 543,
    "title": "激活重计算在并行训练中的应用",
    "slug": "activation-recomputation-parallel-training",
    "tags": [
      "底层原理",
      "内存优化",
      "分布式训练"
    ],
    "brief": "核心要点：激活重计算（activation checkpointing/recomputation）在反向传播时重新计算激活值而非存储。完全重计算增加约33%计算开销但将激活内存从O(L·s·h)降至O(L·s·h/L)=O(s·h)。选择性重计算仅重算QKV计算等占内存大但计算快的部分。",
    "depth_hint": "推导完全重计算和选择性重计算的内存-计算权衡公式，给出Megatron-LM的实现策略",
    "status": "pending"
  },
  {
    "id": 544,
    "title": "3D并行的通信拓扑与带宽分配",
    "slug": "3d-parallelism-communication-topology",
    "tags": [
      "底层原理",
      "混合并行",
      "通信优化"
    ],
    "brief": "核心要点：3D并行（TP×PP×DP）的GPU分组形成三维网格。最优映射：TP组映射到NVLink连接的机内GPU（600GB/s），PP组映射到机间InfiniBand（200Gb/s），DP组利用剩余带宽。错误的映射可使训练吞吐量下降40%以上。",
    "depth_hint": "给出3D映射的约束优化问题形式化，分析DGX SuperPOD拓扑下的最优配置",
    "status": "pending"
  },
  {
    "id": 545,
    "title": "ZeRO++的量化通信优化",
    "slug": "zero-plus-plus-quantized-communication",
    "tags": [
      "底层原理",
      "ZeRO",
      "通信压缩"
    ],
    "brief": "核心要点：ZeRO++引入三项优化：qwZ（权重量化AllGather，INT8/INT4），qgZ（梯度量化Reduce-Scatter，INT8），hpZ（二级分层分片，机内保留完整参数副本减少跨机通信）。三项优化组合可将跨机通信量减少4倍，400Gb/s网络下吞吐提升最高2.16倍。",
    "depth_hint": "分析各优化的量化误差界，推导通信量减少的精确比例，给出训练精度影响的实验数据",
    "status": "pending"
  },
  {
    "id": 546,
    "title": "FSDP：PyTorch原生的ZeRO实现",
    "slug": "fsdp-pytorch-zero-implementation",
    "tags": [
      "底层原理",
      "ZeRO",
      "PyTorch"
    ],
    "brief": "核心要点：FullyShardedDataParallel（FSDP）是PyTorch对ZeRO-3的原生实现。基于FlatParameter将参数展平为一维张量后分片，前向/反向时通过AllGather重建、Reduce-Scatter聚合。FSDP2重构为per-parameter分片，支持更灵活的混合分片策略。",
    "depth_hint": "对比FSDP与DeepSpeed ZeRO-3的实现差异，分析FlatParameter的内存碎片问题",
    "status": "pending"
  },
  {
    "id": 547,
    "title": "Expert Parallel与MoE分布式训练",
    "slug": "expert-parallelism-moe-distributed",
    "tags": [
      "底层原理",
      "模型并行",
      "MoE"
    ],
    "brief": "核心要点：专家并行将不同Expert分配到不同GPU，Token通过All-to-All通信路由到目标Expert所在GPU。通信量为O(B·s·h·k/E)（k为top-k，E为Expert数）。All-to-All的通信模式不规则，难以与计算重叠，是MoE训练的主要瓶颈。",
    "depth_hint": "分析All-to-All通信的带宽和延迟模型，推导Expert并行与TP/PP的组合策略",
    "status": "pending"
  },
  {
    "id": 548,
    "title": "梯度累积与微批次大小的数学等价性",
    "slug": "gradient-accumulation-microbatch-equivalence",
    "tags": [
      "底层原理",
      "分布式训练",
      "优化算法"
    ],
    "brief": "核心要点：梯度累积K步等价于K倍batch size的单步更新（SGD下严格等价，Adam下近似等价）。流水线并行中，micro-batch数m与gradient accumulation步数相关联。当m过小，bubble比例增大；m过大，收敛速度下降。最优m满足bubble率<10%且有效batch size在线性缩放范围内。",
    "depth_hint": "推导SGD和Adam下梯度累积的等价性条件，分析学习率线性缩放法则的适用范围",
    "status": "pending"
  },
  {
    "id": 549,
    "title": "Tensor Parallel中的随机性同步",
    "slug": "tensor-parallel-randomness-synchronization",
    "tags": [
      "底层原理",
      "模型并行",
      "数值一致性"
    ],
    "brief": "核心要点：TP要求部分操作（如Dropout）跨GPU保持相同随机种子（保证AllReduce结果正确），另一部分操作需要不同种子（保证正则化多样性）。Megatron-LM维护两个CUDA RNG状态：model-parallel随机种子和data-parallel随机种子，在前向/反向传播中精确切换。",
    "depth_hint": "分析Dropout在TP中的正确性条件，给出RNG状态管理的代码逻辑，讨论确定性训练的挑战",
    "status": "pending"
  },
  {
    "id": 550,
    "title": "Context Parallel的长序列分布式训练",
    "slug": "context-parallel-long-sequence-training",
    "tags": [
      "底层原理",
      "序列并行",
      "长序列"
    ],
    "brief": "核心要点：Context Parallel（CP）将输入序列沿序列维度切分到多GPU，注意力计算通过Ring Attention实现：KV块在GPU间循环传递，每步计算局部注意力并累积结果。CP使序列长度可线性扩展，128K长度的序列在CP=8时每GPU仅处理16K。",
    "depth_hint": "分析Ring Attention的通信-计算重叠条件，推导CP与TP/PP的组合约束",
    "status": "pending"
  },
  {
    "id": 551,
    "title": "分布式训练的内存分析框架",
    "slug": "distributed-training-memory-analysis",
    "tags": [
      "底层原理",
      "内存优化",
      "分布式训练"
    ],
    "brief": "核心要点：模型训练内存由四部分组成：参数（2Φ fp16）、梯度（2Φ fp16）、优化器状态（12Φ Adam fp32）和激活值（与batch×seq×hidden成正比）。对于175B模型，参数+梯度+优化器=2.8TB，需ZeRO-3+TP+PP组合才能训练。系统化的内存建模指导并行策略选择。",
    "depth_hint": "给出完整的内存占用计算公式，建立不同并行策略下的内存模型，附多个模型规模的计算实例",
    "status": "pending"
  },
  {
    "id": 552,
    "title": "异步流水线并行的权重一致性问题",
    "slug": "async-pipeline-weight-consistency",
    "tags": [
      "底层原理",
      "流水线并行",
      "收敛性"
    ],
    "brief": "核心要点：异步流水线中，前向和反向使用不同版本的权重（weight staleness），导致梯度计算不一致。PipeDream通过weight stashing保证前向-反向使用相同权重版本，但需存储多份权重副本。PipeDream-Flush采用周期性同步消除staleness，以增加bubble为代价保证收敛性。",
    "depth_hint": "形式化定义weight staleness，推导其对收敛速率的影响界，对比同步和异步方案的实际训练损失曲线",
    "status": "pending"
  },
  {
    "id": 553,
    "title": "SP-TP-PP通信重叠的实现技术",
    "slug": "communication-computation-overlap-parallel",
    "tags": [
      "底层原理",
      "通信优化",
      "性能优化"
    ],
    "brief": "核心要点：通信-计算重叠是提升并行效率的关键。TP的AllReduce可拆分为Reduce-Scatter+AllGather并与后续计算重叠；PP的点对点通信可与不相关层的计算重叠；ZeRO的AllGather可在参数使用前提前发起。CUDA Stream和异步通信原语是实现重叠的基础。",
    "depth_hint": "给出通信-计算重叠的CUDA Stream调度伪代码，分析重叠效率的理论上限",
    "status": "pending"
  },
  {
    "id": 554,
    "title": "大模型分布式Checkpoint的保存与恢复",
    "slug": "distributed-checkpoint-save-restore",
    "tags": [
      "底层原理",
      "分布式训练",
      "工程实践"
    ],
    "brief": "核心要点：分布式训练的Checkpoint需保存各并行维度的分片状态。TP分片的参数需记录切分维度和偏移，PP需保存各stage的层映射，ZeRO需保存优化器状态的分片信息。Megatron-LM支持在不同并行度间转换Checkpoint，需处理参数的重新切分和合并。",
    "depth_hint": "分析并行度变化时的Checkpoint resharding算法，给出TP=8转TP=4的参数合并流程",
    "status": "pending"
  },
  {
    "id": 555,
    "title": "数据并行的梯度AllReduce带宽瓶颈",
    "slug": "data-parallel-allreduce-bandwidth-bottleneck",
    "tags": [
      "底层原理",
      "数据并行",
      "通信优化"
    ],
    "brief": "核心要点：数据并行中梯度AllReduce的通信量为2(N-1)/N×M（M为参数量，N为GPU数），对于175B模型约350GB（fp16）。Ring AllReduce的时间复杂度为2(N-1)/N×M/B（B为带宽），在200Gbps InfiniBand下需14秒，必须通过梯度分桶和计算重叠来隐藏。",
    "depth_hint": "推导Ring AllReduce的通信延迟公式，分析梯度分桶（bucketing）对延迟的影响",
    "status": "pending"
  },
  {
    "id": 556,
    "title": "Chinchilla定律的推导与最优分配",
    "slug": "chinchilla-scaling-law-derivation",
    "tags": [
      "底层原理",
      "扩展定律",
      "训练效率"
    ],
    "brief": "核心要点：从Kaplan等人的幂律拟合L(N,D)=E+A/N^α+B/D^β出发，推导Chinchilla的计算最优分配N∝C^0.5、D∝C^0.5，解释为何参数量与数据量应等比扩展，对比Kaplan原始结论N∝C^0.73的偏差来源",
    "depth_hint": "需推导损失函数对N和D的偏导等式，复现IsoFLOP曲线拟合过程",
    "status": "pending"
  },
  {
    "id": 557,
    "title": "扩展定律中的不可约损失项分析",
    "slug": "irreducible-loss-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "损失函数"
    ],
    "brief": "核心要点：幂律公式L(N)=E+A/N^α中的常数项E代表数据分布的熵下界，其数值约1.69 nats（OpenAI原始拟合）。分析E的物理含义、不同数据集上E的变化、以及E对超大规模外推预测的影响",
    "depth_hint": "需对比不同数据集拟合出的E值差异，讨论E的估计误差如何影响最优模型规模预测",
    "status": "pending"
  },
  {
    "id": 558,
    "title": "扩展定律的幂律指数：α与β的实验估计",
    "slug": "scaling-exponents-alpha-beta",
    "tags": [
      "底层原理",
      "扩展定律",
      "实验方法"
    ],
    "brief": "核心要点：Kaplan给出α≈0.076、β≈0.095，Chinchilla拟合为α≈0.34、β≈0.28，两组指数差异巨大。深入分析拟合方法差异（固定D vs联合拟合）、学习率调度的影响、以及小模型拟合向大模型外推的可靠性边界",
    "depth_hint": "需复现两种拟合方法的数学设定差异，展示不同指数下的最优N/D比值变化",
    "status": "pending"
  },
  {
    "id": 559,
    "title": "计算最优之外：过训练的扩展定律",
    "slug": "overtrained-scaling-law",
    "tags": [
      "底层原理",
      "扩展定律",
      "推理成本"
    ],
    "brief": "核心要点：LLaMA系列故意偏离Chinchilla最优点，用更多数据训练更小模型以降低推理成本。推导训练-推理联合成本函数C_total=C_train+n·C_infer，分析过训练倍率（D/D_opt）对最终损失的影响曲线",
    "depth_hint": "需建立包含推理次数n的总成本模型，推导不同部署规模下的最优过训练比",
    "status": "pending"
  },
  {
    "id": 560,
    "title": "涌现能力的度量依赖性解释",
    "slug": "emergent-abilities-metric-mirage",
    "tags": [
      "底层原理",
      "涌现能力",
      "评测方法"
    ],
    "brief": "核心要点：Schaeffer等人提出涌现可能是度量选择的假象——非线性度量（如精确匹配）制造了阶跃式跳变，换用连续度量（如token级Brier分数）后能力曲线变平滑。深入分析度量函数的凸凹性如何扭曲扩展曲线的形态",
    "depth_hint": "需推导精确匹配与Brier分数在概率阈值附近的数学行为差异，复现关键实验图表",
    "status": "pending"
  },
  {
    "id": 561,
    "title": "涌现能力的相变类比与临界指数",
    "slug": "emergence-phase-transition-analogy",
    "tags": [
      "底层原理",
      "涌现能力",
      "统计物理"
    ],
    "brief": "核心要点：将涌现能力类比为统计物理中的相变：模型规模对应温度，任务准确率对应序参量。分析这一类比的数学基础——损失函数的平滑下降如何通过非线性解码产生序参量的不连续跳变，讨论临界指数的可预测性",
    "depth_hint": "需建立简化的相变数学模型，展示连续量到离散输出的阈值映射机制",
    "status": "pending"
  },
  {
    "id": 562,
    "title": "思维链如何降低涌现的临界规模",
    "slug": "chain-of-thought-emergence-threshold",
    "tags": [
      "底层原理",
      "涌现能力",
      "思维链"
    ],
    "brief": "核心要点：Wei等人实验显示，思维链提示将多步推理任务的涌现阈值从540B参数降至~62B。分析CoT的作用机制：将复杂推理分解为多个子问题，每个子问题的临界规模远低于整体直接求解。量化CoT对有效计算深度的放大效应",
    "depth_hint": "需对比同一任务在direct与CoT设定下的扩展曲线，分析计算步骤分解的复杂度收益",
    "status": "pending"
  },
  {
    "id": 563,
    "title": "涌现能力的可预测性：扩展定律外推方法",
    "slug": "predicting-emergence-extrapolation",
    "tags": [
      "底层原理",
      "涌现能力",
      "扩展定律"
    ],
    "brief": "核心要点：Ganguli等人提出用小模型的连续度量扩展曲线外推预测大模型的涌现点。技术路线：拟合token级损失的幂律→建立损失与任务度量的映射函数→预测准确率跃升的临界损失值和对应模型规模",
    "depth_hint": "需展示两阶段预测框架的数学形式化，讨论外推置信区间的估计方法",
    "status": "pending"
  },
  {
    "id": 564,
    "title": "上下文学习的贝叶斯推断解释",
    "slug": "in-context-learning-bayesian-inference",
    "tags": [
      "底层原理",
      "上下文学习",
      "贝叶斯推断"
    ],
    "brief": "核心要点：Xie等人证明ICL隐式执行贝叶斯推断——给定示例序列，Transformer在隐空间中推断潜在概念变量θ，实现P(y|x,示例)≈∫P(y|x,θ)P(θ|示例)dθ。分析这一理论的HMM构造证明及其对示例数量、顺序敏感性的解释力",
    "depth_hint": "需推导HMM框架下的后验更新公式，展示隐含概念推断的计算过程",
    "status": "pending"
  },
  {
    "id": 565,
    "title": "Transformer的上下文学习与梯度下降的等价性",
    "slug": "icl-gradient-descent-equivalence",
    "tags": [
      "底层原理",
      "上下文学习",
      "优化理论"
    ],
    "brief": "核心要点：Akyürek和Von Oswald分别证明线性注意力Transformer的前向传播等价于对示例数据执行一步梯度下降。具体构造：将QKV矩阵参数化使得注意力输出恢复出(X^TX)^{-1}X^Ty的最小二乘解，揭示ICL是隐式优化过程",
    "depth_hint": "需推导线性注意力到岭回归解的矩阵构造，展示多层对应多步梯度下降",
    "status": "pending"
  },
  {
    "id": 566,
    "title": "上下文学习中的任务向量与功能压缩",
    "slug": "icl-task-vectors-function-compression",
    "tags": [
      "底层原理",
      "上下文学习",
      "表示学习"
    ],
    "brief": "核心要点：Todd等人发现ICL示例在Transformer中间层被压缩为一个任务向量（task vector），该向量足以替代全部示例触发相同行为。通过因果追踪定位任务向量的形成层（通常在中间层），分析其与示例数量的信息饱和关系",
    "depth_hint": "需展示因果干预实验设计，分析任务向量提取后零样本激活效果的定量数据",
    "status": "pending"
  },
  {
    "id": 567,
    "title": "上下文学习的示例选择与排序效应",
    "slug": "icl-example-selection-ordering",
    "tags": [
      "底层原理",
      "上下文学习",
      "提示工程"
    ],
    "brief": "核心要点：ICL性能对示例选择和排序高度敏感——Lu等人实验显示不同排列的准确率方差可达30%+。分析排序效应的注意力机制解释（近因偏差与首因偏差）、基于困惑度的最优排序算法、以及校准方法（contextual calibration）的数学原理",
    "depth_hint": "需展示排列方差的实验数据，推导困惑度排序与上下文校准的具体计算步骤",
    "status": "pending"
  },
  {
    "id": 568,
    "title": "上下文学习的规模涌现：从模式匹配到任务学习",
    "slug": "icl-emergence-pattern-to-task-learning",
    "tags": [
      "底层原理",
      "上下文学习",
      "涌现能力"
    ],
    "brief": "核心要点：小模型的ICL主要依赖输入-标签的表面统计共现（如正面词→正面标签），而大模型能真正学习输入到标签的映射函数——即使标签被随机翻转仍能遵循示例模式。分析这一从PMI检索到函数学习的相变机制及其与模型规模的关系",
    "depth_hint": "需对比翻转标签实验中不同规模模型的表现差异，分析注意力模式的质变",
    "status": "pending"
  },
  {
    "id": 569,
    "title": "SimCSE对比学习的梯度分析与表示坍缩",
    "slug": "simcse-contrastive-gradient-representation-collapse",
    "tags": [
      "底层原理",
      "对比学习",
      "表示学习"
    ],
    "brief": "核心要点：从InfoNCE损失的梯度公式出发，分析SimCSE中正负样本梯度的推拉机制。推导温度系数τ对梯度幅值的缩放效应，解释τ→0时的硬负样本主导现象。剖析表示坍缩（collapse）的成因——所有向量收敛到同一方向，以及SimCSE通过Dropout噪声构造正样本对如何缓解均匀性-对齐性（uniformity-alignment）的权衡。",
    "depth_hint": "需推导InfoNCE梯度的解析形式，给出alignment和uniformity的量化指标公式，附不同τ值下STS-B性能的实验数据",
    "status": "pending"
  },
  {
    "id": 570,
    "title": "知识蒸馏的损失函数设计与软标签信息量分析",
    "slug": "knowledge-distillation-loss-design-soft-label-information",
    "tags": [
      "底层原理",
      "知识蒸馏",
      "模型压缩"
    ],
    "brief": "核心要点：从Hinton经典蒸馏框架出发，推导软标签KL散度损失在高温极限T→∞下退化为MSE的数学过程。分析教师模型logits中暗知识（dark knowledge）的信息论含义——类间相似度结构编码在非最大概率项中。对比硬标签交叉熵与软标签KL散度的梯度差异，解释蒸馏温度T²缩放因子的来源及其对梯度量级的归一化作用。",
    "depth_hint": "需完整推导KL散度在高温下的Taylor展开，给出T²系数的严格证明，附不同温度T下CIFAR-100蒸馏精度的消融实验",
    "status": "pending"
  }
]